import os
import logging
from datasets import load_dataset, Dataset, concatenate_datasets
from typing import Optional, Tuple, Dict, List, Union
import json

logger = logging.getLogger(__name__)

class DataLoader:
    """ç»Ÿä¸€çš„æ•°æ®åŠ è½½å™¨ï¼Œä¸è®­ç»ƒç±»å‹æ— å…³ï¼Œåªè´Ÿè´£æ•°æ®åŠ è½½"""
    
    # é»˜è®¤HF subseté…ç½®
    DEFAULT_HF_SUBSET = "pair-class"
    
    def __init__(self, hf_subset: Optional[str] = None, 
                 train_sample_size: int = 0,
                 eval_sample_size: int = 0,
                 test_sample_size: int = 0):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            hf_subset: HuggingFaceæ•°æ®é›†çš„å­é…ç½®åç§°ï¼Œæ”¯æŒï¼š
                     - å•ä¸ªé…ç½®: 'pair-score'
                     - å¤šä¸ªé…ç½®: 'pair-score,pair-class,custom-config'
                     - å¦‚æœHFæ•°æ®é›†æ•°é‡è¶…è¿‡subseté…ç½®æ•°é‡ï¼Œè¶…å‡ºéƒ¨åˆ†ä½¿ç”¨é»˜è®¤çš„pair-class
            train_sample_size: è®­ç»ƒé›†æ ·æœ¬æ•°é‡é™åˆ¶ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶
            eval_sample_size: éªŒè¯é›†æ ·æœ¬æ•°é‡é™åˆ¶ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶
            test_sample_size: æµ‹è¯•é›†æ ·æœ¬æ•°é‡é™åˆ¶ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶
        """
        # è§£æHF_subsetä¸ºåˆ—è¡¨
        if hf_subset and isinstance(hf_subset, str):
            if "," in hf_subset:
                # åˆ†å‰²subseté…ç½®ï¼Œæ”¯æŒç©ºé…ç½®å’Œæ˜¾å¼çš„none/null
                parts = [s.strip() for s in hf_subset.split(",")]
                self.hf_subsets = []
                for part in parts:
                    if part == "" or part.lower() in ["none", "null"]:
                        # ç©ºå­—ç¬¦ä¸²ã€noneã€nulléƒ½è¡¨ç¤ºæ— é…ç½®
                        self.hf_subsets.append(None)
                    else:
                        self.hf_subsets.append(part)
            else:
                if hf_subset.lower() in ["none", "null"]:
                    self.hf_subsets = [None]
                else:
                    self.hf_subsets = [hf_subset]
        elif isinstance(hf_subset, list):
            self.hf_subsets = hf_subset
        else:
            self.hf_subsets = []
        
        # ä¿æŒå‘åå…¼å®¹
        self.hf_subset = hf_subset
        
        # å­˜å‚¨æ ·æœ¬å¤§å°é™åˆ¶å‚æ•°ï¼Œè¿›è¡Œå‚æ•°éªŒè¯
        self.train_sample_size = max(0, train_sample_size)
        self.eval_sample_size = max(0, eval_sample_size) 
        self.test_sample_size = max(0, test_sample_size)
        
        # å¦‚æœå‚æ•°æœ‰è´Ÿæ•°ï¼Œè®°å½•è­¦å‘Š
        if train_sample_size < 0:
            logger.warning(f"train_sample_sizeä¸èƒ½ä¸ºè´Ÿæ•°ï¼Œå·²é‡ç½®ä¸º0: {train_sample_size}")
        if eval_sample_size < 0:
            logger.warning(f"eval_sample_sizeä¸èƒ½ä¸ºè´Ÿæ•°ï¼Œå·²é‡ç½®ä¸º0: {eval_sample_size}")
        if test_sample_size < 0:
            logger.warning(f"test_sample_sizeä¸èƒ½ä¸ºè´Ÿæ•°ï¼Œå·²é‡ç½®ä¸º0: {test_sample_size}")
            
        logger.info(f"åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨ï¼ŒHF_subset: {hf_subset}, è§£æåçš„subsetåˆ—è¡¨: {self.hf_subsets}")
        logger.info(f"æ ·æœ¬æ•°é‡é™åˆ¶: train={self.train_sample_size}, eval={self.eval_sample_size}, test={self.test_sample_size}")
        
    def load_data(self, split_type: str = "train", dataset_path: str = None) -> Union[Optional[Dataset], Dict[str, Dataset]]:
        """
        æ ¹æ®ä¼ å…¥å‚æ•°åŠ è½½æ•°æ®é›†ï¼Œæ”¯æŒå•ä¸ªæ•°æ®é›†æˆ–å¤šä¸ªæ•°æ®é›†
        
        Args:
            split_type: æ•°æ®é›†åˆ†å‰²ç±»å‹ ('train', 'dev', 'test')
            dataset_path: æ•°æ®é›†è·¯å¾„ï¼Œå¿…é¡»æä¾›
            
        Returns:
            Datasetã€Dict[str, Dataset]æˆ–None
            - å•ä¸ªæ•°æ®é›†è¿”å›Dataset
            - å¤šä¸ªæ•°æ®é›†è¿”å›Dict[str, Dataset]ï¼Œé”®ä¸ºæ•°æ®é›†åç§°
        """
        if not dataset_path:
            logger.error("æœªæä¾›æ•°æ®é›†è·¯å¾„")
            return None
        
        return self._load_from_unified_path(dataset_path, split_type)
    
    def _is_huggingface_dataset_names(self, dataset_path: str) -> bool:
        """
        åˆ¤æ–­å•ä¸ªè·¯å¾„æ˜¯å¦ä¸ºHugging Faceæ•°æ®é›†åç§°
        
        Args:
            dataset_path: å•ä¸ªæ•°æ®é›†è·¯å¾„å­—ç¬¦ä¸²ï¼ˆä¸åŒ…å«é€—å·ï¼‰
            
        Returns:
            Trueè¡¨ç¤ºæ˜¯HuggingFaceæ•°æ®é›†åç§°ï¼ŒFalseè¡¨ç¤ºæ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„
        """
        path = dataset_path.strip()
        
        # æ³¨æ„ï¼šä¸èƒ½å› ä¸ºæŒ‡å®šäº†HF_subsetå°±å¼ºåˆ¶åˆ¤æ–­æ‰€æœ‰è·¯å¾„ä¸ºHFæ•°æ®é›†
        # HF_subsetåªå½±å“å·²è¯†åˆ«çš„HFæ•°æ®é›†çš„é…ç½®ï¼Œä¸å½±å“æ•°æ®é›†ç±»å‹è¯†åˆ«
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡ä»¶æ‰©å±•åï¼ˆæœ¬åœ°æ–‡ä»¶çš„ç‰¹å¾ï¼‰
        if path.endswith(('.json', '.jsonl', '.txt', '.csv')):
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä»¥ ./ æˆ– ../ å¼€å¤´ï¼ˆæ˜ç¡®çš„ç›¸å¯¹è·¯å¾„ï¼‰
        if path.startswith('./') or path.startswith('../'):
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸º HuggingFace æ ‡å‡†æ ¼å¼ org/repo
        if '/' in path:
            parts = path.split('/')
            if len(parts) == 2 and parts[0] and parts[1]:
                # ç¬¦åˆ org/repo æ ¼å¼ï¼Œå¾ˆå¯èƒ½æ˜¯ HuggingFace æ•°æ®é›†
                return True
            elif len(parts) > 2:
                # å¤šçº§è·¯å¾„ï¼Œå¯èƒ½æ˜¯æœ¬åœ°è·¯å¾„
                return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ¬åœ°è·¯å¾„åˆ†éš”ç¬¦ï¼ˆWindowsï¼‰
        if os.path.sep in path and os.path.sep != '/':
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç»å¯¹è·¯å¾„
        if os.path.isabs(path):
            return False
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä½œä¸ºæœ¬åœ°æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹
        if os.path.exists(path):
            return False
        
        # å¦‚æœä¸ç¬¦åˆæœ¬åœ°æ–‡ä»¶ç‰¹å¾ï¼Œå‡å®šä¸ºHuggingFaceæ•°æ®é›†åç§°
        return True

    def _load_from_unified_path(self, dataset_path: str, split_type: str) -> Union[Optional[Dataset], Dict[str, Dataset]]:
        """
        ç»Ÿä¸€çš„æ•°æ®åŠ è½½æ–¹æ³•ï¼ŒæŒ‰ä¼˜å…ˆçº§é¡ºåºå°è¯•åŠ è½½
        
        Args:
            dataset_path: æ•°æ®é›†è·¯å¾„ï¼Œå¯èƒ½æ˜¯ï¼š
                1. Hugging Faceæ•°æ®é›†åç§°ï¼ˆå¦‚ï¼šsentence-transformers/all-nliï¼‰
                2. é€—å·åˆ†éš”çš„å¤šä¸ªHubæ•°æ®é›†åç§°
                3. æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆ.json/.jsonlï¼‰
                4. é€—å·åˆ†éš”çš„å¤šä¸ªæ–‡ä»¶è·¯å¾„ï¼ˆä»…ç”¨äºè®­ç»ƒé›†ï¼‰
                5. æœ¬åœ°æ–‡ä»¶å¤¹è·¯å¾„
            split_type: æ•°æ®é›†åˆ†å‰²ç±»å‹
            
        Returns:
            Datasetã€Dict[str, Dataset]æˆ–None
        """
        # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯æœ¬åœ°æ–‡ä»¶ï¼ˆéæ··åˆï¼‰
        if not "," in dataset_path and not self._is_huggingface_dataset_names(dataset_path) and os.path.isfile(dataset_path):
            # å•ä¸ªæœ¬åœ°æ–‡ä»¶ï¼Œä»…æ”¯æŒè®­ç»ƒé›†
            if split_type != "train":
                logger.info(f"å•ä¸ªæœ¬åœ°æ–‡ä»¶ä»…æ”¯æŒè®­ç»ƒé›†ï¼Œ{split_type}åˆ†å‰²è¿”å›None")
                return None
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ··åˆè·¯å¾„ï¼ˆåŒ…å«å¤šç§ç±»å‹ï¼‰
        if "," in dataset_path:
            # æ··åˆè·¯å¾„ï¼šå¯èƒ½åŒ…å«HuggingFaceæ•°æ®é›†ã€æœ¬åœ°æ–‡ä»¶å’Œæœ¬åœ°æ–‡ä»¶å¤¹
            return self._load_from_mixed_sources(dataset_path, split_type)
        
        # å•ä¸ªè·¯å¾„çš„å¤„ç†é€»è¾‘ï¼šæ ¹æ®è·¯å¾„ç±»å‹ç›´æ¥è°ƒç”¨å¯¹åº”æ–¹æ³•
        if self._is_huggingface_dataset_names(dataset_path):
            # HuggingFaceæ•°æ®é›†
            logger.info(f"è¯†åˆ«ä¸ºHuggingFaceæ•°æ®é›†: {dataset_path}")
            try:
                result = self._load_from_hub_unified(dataset_path, split_type)
                if result is not None:
                    return result
            except Exception as e:
                logger.error(f"HuggingFaceæ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        else:
            # æœ¬åœ°è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–æ–‡ä»¶å¤¹ï¼‰
            logger.info(f"è¯†åˆ«ä¸ºæœ¬åœ°è·¯å¾„: {dataset_path}")
            try:
                result = self._load_from_local_unified(dataset_path, split_type)
                if result is not None:
                    return result
            except Exception as e:
                logger.error(f"æœ¬åœ°è·¯å¾„åŠ è½½å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§3: ä½¿ç”¨é»˜è®¤æ•°æ®é›†ï¼ˆå¯¹æ‰€æœ‰åˆ†å‰²ç±»å‹éƒ½å°è¯•ï¼‰ - æš‚æ—¶ç¦ç”¨
        logger.error(f"æ‰€æœ‰åŠ è½½æ–¹å¼å¤±è´¥ï¼Œä¸è‡ªåŠ¨ä½¿ç”¨é»˜è®¤æ•°æ®é›†: split={split_type}")
        # default_result = self._load_default_dataset(split_type)
        # if default_result is not None:
        #     logger.info(f"æˆåŠŸä½¿ç”¨é»˜è®¤æ•°æ®é›†: {split_type}")
        #     return default_result
        
        logger.warning(f"æ— æ³•åŠ è½½æ•°æ®é›†: {dataset_path}, split={split_type}")
        return None
    
    def _load_from_local_unified(self, path: str, split_type: str) -> Union[Optional[Dataset], Dict[str, Dataset]]:
        """ä»å•ä¸ªæœ¬åœ°è·¯å¾„åŠ è½½æ•°æ®ï¼ˆä¸å¤„ç†é€—å·åˆ†éš”çš„å¤šè·¯å¾„ï¼‰"""
        if os.path.isdir(path):
            # å•ä¸ªæ–‡ä»¶å¤¹è·¯å¾„
            return self._load_from_directory(path, split_type)
        elif os.path.isfile(path):
            # å•ä¸ªæ–‡ä»¶è·¯å¾„
            return self._load_from_json_single(path, split_type)
        else:
            logger.error(f"è·¯å¾„ä¸å­˜åœ¨: {path}")
            if split_type == "train":
                raise ValueError(f"è®­ç»ƒæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {path}")
            return None
    
    def _load_from_hub_unified(self, dataset_name: str, split_type: str) -> Union[Optional[Dataset], Dict[str, Dataset]]:
        """ä»HubåŠ è½½å•ä¸ªæ•°æ®é›†ï¼ˆä¸å¤„ç†é€—å·åˆ†éš”çš„å¤šæ•°æ®é›†ï¼‰"""
        # å¯¹äºå•ä¸ªHFæ•°æ®é›†ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªsubseté…ç½®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if self.hf_subsets:
            subset_to_use = self.hf_subsets[0]
            logger.info(f"å•ä¸ªHFæ•°æ®é›†ä½¿ç”¨ç¬¬ä¸€ä¸ªsubseté…ç½®: {subset_to_use}")
            
            # å¦‚æœç”¨æˆ·ä¸ºå•ä¸ªæ•°æ®é›†æŒ‡å®šäº†å¤šä¸ªsubsetï¼Œç»™å‡ºè­¦å‘Š
            if len(self.hf_subsets) > 1:
                unused_subsets = self.hf_subsets[1:]
                logger.warning(f"å•ä¸ªHFæ•°æ®é›† '{dataset_name}' æŒ‡å®šäº†å¤šä¸ªsubseté…ç½®ï¼Œåªä½¿ç”¨ç¬¬ä¸€ä¸ª: {subset_to_use}ï¼Œå¿½ç•¥: {unused_subsets}")
                logger.warning(f"å¦‚æœè¦ä½¿ç”¨å¤šä¸ªsubsetï¼Œè¯·æä¾›å¤šä¸ªæ•°æ®é›†è·¯å¾„ï¼Œç”¨é€—å·åˆ†éš”")
            
            # åˆ›å»ºä¸´æ—¶loaderå®ä¾‹æ¥ä½¿ç”¨ç‰¹å®šçš„subset
            temp_loader = DataLoader(hf_subset=subset_to_use)
            return temp_loader._load_from_hub_single(dataset_name, split_type)
        else:
            # æ²¡æœ‰æŒ‡å®šsubsetï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            return self._load_from_hub_single(dataset_name, split_type)
    
    def _load_from_hub_single(self, dataset_name: str, split_type: str) -> Optional[Dataset]:
        """ä»Hugging Face HubåŠ è½½å•ä¸ªæ•°æ®é›†ï¼Œæ”¯æŒæœ¬åœ°ç¼“å­˜æ¨¡å¼å’Œåˆ†å‰²åç§°è‡ªåŠ¨å›é€€"""
        logger.info(f"ä»HubåŠ è½½æ•°æ®é›†: {dataset_name}, split: {split_type}, HF_subset: {self.hf_subset}")
        
        # ç¡®å®šè¦ä½¿ç”¨çš„subseté…ç½®
        if self.hf_subsets:
            # å¦‚æœæœ‰å¤šä¸ªsubsetï¼Œåªä½¿ç”¨ç¬¬ä¸€ä¸ªï¼ˆé€‚ç”¨äºå•ä¸ªHFæ•°æ®é›†çš„åœºæ™¯ï¼‰
            actual_subset = self.hf_subsets[0]
            # æ·»åŠ æ™ºèƒ½å›é€€ï¼šæŒ‡å®šé…ç½® -> å¸¸è§é…ç½® -> æ— é…ç½®
            config_alternatives = [actual_subset, "pair-class", "pair-score", None]
            logger.info(f"ğŸ¯ ä½¿ç”¨HF_subseté…ç½®: {actual_subset} (æ¥è‡ªåˆ—è¡¨: {self.hf_subsets})ï¼Œå¸¦æ™ºèƒ½å›é€€")
        elif self.hf_subset is not None and not "," in str(self.hf_subset):
            # å‘åå…¼å®¹ï¼šå•ä¸ªsubsetå­—ç¬¦ä¸²
            config_alternatives = [self.hf_subset, "pair-class", "pair-score", None]
            logger.info(f"ğŸ¯ ä½¿ç”¨æŒ‡å®šçš„HF_subseté…ç½®: {self.hf_subset}ï¼Œå¸¦æ™ºèƒ½å›é€€")
        elif self.hf_subset is None:
            # æ˜ç¡®æŒ‡å®šäº†Noneï¼Œä¼˜å…ˆæ— é…ç½®ä½†ä¿ç•™å›é€€
            config_alternatives = [None, "pair-class", "pair-score"]
            logger.info("ğŸ¯ ä½¿ç”¨æ˜ç¡®æŒ‡å®šçš„æ— é…ç½®ï¼Œå¸¦æœ‰å›é€€æœºåˆ¶")
        else:
            # æ²¡æœ‰æœ‰æ•ˆçš„subseté…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼å’Œæ— é…ç½®å›é€€
            config_alternatives = ["pair-class", "pair-score", None]
            logger.info("ğŸ”„ ä½¿ç”¨é…ç½®å›é€€ç­–ç•¥: pair-class -> pair-score -> æ— é…ç½®")
        
        # å®šä¹‰åˆ†å‰²åç§°çš„å›é€€é¡ºåº
        split_alternatives = [split_type]
        if split_type == "eval":
            split_alternatives = ["eval", "dev", "validation"]  # ä¼˜å…ˆå°è¯•åŸåï¼Œç„¶åå›é€€
        elif split_type == "dev":
            split_alternatives = ["dev", "eval", "validation"]  # ä¼˜å…ˆå°è¯•åŸåï¼Œç„¶åå›é€€
        elif split_type == "validation":
            split_alternatives = ["validation", "eval", "dev"]  # ä¼˜å…ˆå°è¯•åŸåï¼Œç„¶åå›é€€
        
        # å°è¯•æ¯ä¸ªåˆ†å‰²åç§°å’Œé…ç½®ç»„åˆ
        for split_attempt, actual_split in enumerate(split_alternatives):
            if split_attempt > 0:
                logger.info(f"å›é€€å°è¯•åˆ†å‰²åç§°: {split_type} -> {actual_split}")
            
            for config_attempt, config in enumerate(config_alternatives):
                if config_attempt > 0:
                    config_desc = "æ— é…ç½®" if config is None else config
                    logger.info(f"å›é€€å°è¯•é…ç½®: {config_desc}")
                
                try:
                    # ä¼˜å…ˆå°è¯•ModelScopeåŠ è½½
                    try:
                        from modelscope.msdatasets import MsDataset
                        logger.info(f"ä¼˜å…ˆå°è¯•ä»ModelScopeåŠ è½½æ•°æ®é›†: {dataset_name}")
                        
                        if config is None:
                            ms_dataset = MsDataset.load(dataset_name, split=actual_split)
                        else:
                            ms_dataset = MsDataset.load(dataset_name, subset_name=config, split=actual_split)
                        
                        # è½¬æ¢ä¸ºHuggingFace Datasetæ ¼å¼
                        from datasets import Dataset
                        dataset = Dataset.from_dict(ms_dataset.to_dict())
                        
                        if split_attempt > 0:
                            logger.info(f"âœ… ModelScopeæˆåŠŸä½¿ç”¨å›é€€åˆ†å‰²åç§°: {actual_split}")
                        if config_attempt > 0:
                            config_desc = "æ— é…ç½®" if config is None else config
                            logger.info(f"âœ… ModelScopeæˆåŠŸä½¿ç”¨å›é€€é…ç½®: {config_desc}")
                        logger.info(f"âœ… æˆåŠŸä»ModelScopeåŠ è½½æ•°æ®é›†: {dataset_name}")
                        return self._apply_sample_size(dataset, split_type)
                        
                    except ImportError:
                        logger.info("ModelScopeæœªå®‰è£…ï¼Œå›é€€åˆ°HuggingFace")
                        # å›é€€åˆ°HuggingFaceåŠ è½½
                        if config is None:
                            dataset = load_dataset(dataset_name, split=actual_split)
                        else:
                            dataset = load_dataset(dataset_name, config, split=actual_split)
                        if split_attempt > 0:
                            logger.info(f"âœ… HuggingFaceæˆåŠŸä½¿ç”¨å›é€€åˆ†å‰²åç§°: {actual_split}")
                        if config_attempt > 0:
                            config_desc = "æ— é…ç½®" if config is None else config
                            logger.info(f"âœ… HuggingFaceæˆåŠŸä½¿ç”¨å›é€€é…ç½®: {config_desc}")
                        logger.info(f"âœ… æˆåŠŸä»HuggingFaceåŠ è½½æ•°æ®é›†: {dataset_name}")
                        return self._apply_sample_size(dataset, split_type)
                    
                    except Exception as ms_error:
                        logger.warning(f"ModelScopeåŠ è½½å¤±è´¥: {ms_error}ï¼Œå›é€€åˆ°HuggingFace")
                        # ModelScopeå¤±è´¥ï¼Œå›é€€åˆ°HuggingFace
                        if config is None:
                            dataset = load_dataset(dataset_name, split=actual_split)
                        else:
                            dataset = load_dataset(dataset_name, config, split=actual_split)
                        if split_attempt > 0:
                            logger.info(f"âœ… HuggingFaceæˆåŠŸä½¿ç”¨å›é€€åˆ†å‰²åç§°: {actual_split}")
                        if config_attempt > 0:
                            config_desc = "æ— é…ç½®" if config is None else config
                            logger.info(f"âœ… HuggingFaceæˆåŠŸä½¿ç”¨å›é€€é…ç½®: {config_desc}")
                        logger.info(f"âœ… æˆåŠŸä»HuggingFaceåŠ è½½æ•°æ®é›†: {dataset_name}")
                        return self._apply_sample_size(dataset, split_type)
                except Exception as e:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯é…ç½®ä¸å­˜åœ¨çš„é—®é¢˜
                    if ("Unknown config" in str(e) or "BuilderConfig" in str(e) or "Config name is missing" in str(e)) and config_attempt < len(config_alternatives) - 1:
                        # æå–å¯ç”¨é…ç½®ä¿¡æ¯
                        if "Available:" in str(e):
                            available_configs = str(e).split("Available:")[1].strip()
                            logger.info(f"é…ç½® '{config}' ä¸å­˜åœ¨ï¼Œå¯ç”¨é…ç½®: {available_configs}ï¼Œå°è¯•ä¸‹ä¸€ä¸ªé…ç½®...")
                        elif "Config name is missing" in str(e):
                            logger.info(f"æ•°æ®é›†è¦æ±‚æŒ‡å®šé…ç½®ï¼ˆå½“å‰ä¸ºNoneï¼‰ï¼Œå°è¯•ä¸‹ä¸€ä¸ªé…ç½®...")
                        else:
                            logger.info(f"é…ç½® '{config}' ä¸å­˜åœ¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ªé…ç½®...")
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†å‰²åç§°é—®é¢˜
                    if "Unknown split" in str(e) and split_attempt < len(split_alternatives) - 1:
                        logger.info(f"åˆ†å‰² '{actual_split}' ä¸å­˜åœ¨ï¼Œå°è¯•ä¸‹ä¸€ä¸ªåˆ†å‰²...")
                        break  # è·³å‡ºå†…å±‚é…ç½®å¾ªç¯ï¼Œå°è¯•ä¸‹ä¸€ä¸ªåˆ†å‰²
                    
                    # æœ€åå°è¯•æœ¬åœ°ç¼“å­˜ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶ï¼‰
                    if split_attempt == 0 and config_attempt == 0:
                        logger.warning(f"æ•°æ®é›†åŠ è½½å¤±è´¥ï¼Œå°è¯•æœ¬åœ°ç¼“å­˜: {e}")
                        try:
                            logger.info(f"å°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ•°æ®é›†: {dataset_name}")
                            # å°è¯•ä½¿ç”¨local_files_onlyå‚æ•°
                            try:
                                if config is None:
                                    dataset = load_dataset(dataset_name, split=actual_split, local_files_only=True)
                                else:
                                    dataset = load_dataset(dataset_name, config, split=actual_split, local_files_only=True)
                            except TypeError as type_error:
                                # å¦‚æœlocal_files_onlyå‚æ•°ä¸æ”¯æŒï¼Œåˆ™ä¸ä½¿ç”¨è¯¥å‚æ•°
                                if "unexpected keyword argument" in str(type_error) and "local_files_only" in str(type_error):
                                    logger.info(f"æ•°æ®é›† {dataset_name} ä¸æ”¯æŒlocal_files_onlyå‚æ•°ï¼Œå°è¯•ä¸ä½¿ç”¨è¯¥å‚æ•°")
                                    if config is None:
                                        dataset = load_dataset(dataset_name, split=actual_split)
                                    else:
                                        dataset = load_dataset(dataset_name, config, split=actual_split)
                                else:
                                    raise type_error
                            logger.info(f"âœ… æˆåŠŸä»æœ¬åœ°ç¼“å­˜åŠ è½½æ•°æ®é›†: {dataset_name}")
                            return self._apply_sample_size(dataset, split_type)
                        except Exception as cache_error:
                            logger.error(f"æœ¬åœ°ç¼“å­˜ä¹Ÿå¤±è´¥: {cache_error}")
                    
                    # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œè®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
                    if split_attempt == len(split_alternatives) - 1 and config_attempt == len(config_alternatives) - 1:
                        error_str = str(e).lower()
                        if any(keyword in error_str for keyword in ["connection", "network", "timeout", "huggingface.co"]):
                            logger.error(f"âŒ ç½‘ç»œè¿æ¥é—®é¢˜å¯¼è‡´æ•°æ®é›†åŠ è½½å¤±è´¥: {dataset_name}")
                            logger.error("ğŸ’¡ å»ºè®®: 1) æ£€æŸ¥ç½‘ç»œè¿æ¥ 2) ä½¿ç”¨æœ¬åœ°æ•°æ®é›†æ–‡ä»¶ 3) é…ç½®ä»£ç†")
                        else:
                            logger.error(f"æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
                            if "Unknown split" in str(e):
                                logger.error(f"ğŸ’¡ å°è¯•çš„æ‰€æœ‰åˆ†å‰²åç§°éƒ½ä¸å­˜åœ¨: {split_alternatives}")
                            if "Unknown config" in str(e):
                                logger.error(f"ğŸ’¡ å°è¯•çš„æ‰€æœ‰é…ç½®éƒ½ä¸å­˜åœ¨: {config_alternatives}")
        
        return None
    
    def _load_from_local_multiple(self, split_type: str) -> Union[Dataset, Dict[str, Dataset]]:
        """ä»æœ¬åœ°æ–‡ä»¶/æ–‡ä»¶å¤¹åŠ è½½å¤šä¸ªæ•°æ®é›†"""
        path = os.getenv(f"{split_type.upper()}_DATASET")
        
        if os.path.isdir(path):
            # æ–‡ä»¶å¤¹ï¼šåŠ è½½å…¶ä¸­æ‰€æœ‰JSONæ–‡ä»¶
            return self._load_from_directory(path, split_type)
        elif "," in path:
            # é€—å·åˆ†éš”çš„å¤šä¸ªæ–‡ä»¶è·¯å¾„
            return self._load_from_multiple_files(path, split_type)
        else:
            # å•ä¸ªæ–‡ä»¶ï¼Œä¿æŒå‘åå…¼å®¹
            return self._load_from_json_single(path, split_type)
    
    def _load_from_directory(self, dir_path: str, split_type: str) -> Union[Dataset, Dict[str, Dataset]]:
        """ä»æ–‡ä»¶å¤¹ä¸­åŠ è½½æ•°æ®é›†ï¼Œæ™ºèƒ½è¯†åˆ«æ ‡å‡†å‘½åçš„æ–‡ä»¶"""
        logger.info(f"ä»æ–‡ä»¶å¤¹åŠ è½½æ•°æ®é›†: {dir_path}")
        
        # æ ‡å‡†æ–‡ä»¶åæ˜ å°„
        standard_files = {
            'train': ['train_data.jsonl', 'train_data.json', 'train.jsonl', 'train.json'],
            'eval': ['eval_data.jsonl', 'eval_data.json', 'eval.jsonl', 'eval.json', 'val_data.jsonl', 'val.jsonl','val.json','val_data.json', 'dev_data.jsonl', 'dev_data.json', 'dev.jsonl', 'dev.json'],
            'dev': ['eval_data.jsonl', 'eval_data.json', 'eval.jsonl', 'eval.json', 'val_data.jsonl', 'val.jsonl','val.json','val_data.json', 'dev_data.jsonl', 'dev_data.json', 'dev.jsonl', 'dev.json'],
            'test': ['test_data.jsonl', 'test_data.json', 'test.jsonl', 'test.json']
        }
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡å‡†å‘½åçš„æ–‡ä»¶
        found_files = {}
        for split, possible_names in standard_files.items():
            for name in possible_names:
                file_path = os.path.join(dir_path, name)
                if os.path.exists(file_path):
                    found_files[split] = file_path
                    break
        
        if found_files:
            # å¦‚æœæ‰¾åˆ°æ ‡å‡†å‘½åæ–‡ä»¶ï¼ŒåªåŠ è½½å¯¹åº”splitçš„æ•°æ®
            if split_type in found_files:
                file_path = found_files[split_type]
                logger.info(f"åŠ è½½æ ‡å‡†æ•°æ®æ–‡ä»¶: {file_path}")
                try:
                    dataset = load_dataset("json", data_files=file_path, split="train")
                    return self._apply_sample_size(dataset, split_type)
                except Exception as e:
                    logger.error(f"åŠ è½½æ ‡å‡†æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
                    if split_type == "train":
                        raise ValueError(f"è®­ç»ƒæ•°æ®åŠ è½½å¤±è´¥: {e}")
                    return None
            else:
                logger.info(f"æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰{split_type}åˆ†å‰²çš„æ ‡å‡†æ•°æ®æ–‡ä»¶")
                return None
        
        else:
            # å¦‚æœæ²¡æœ‰æ ‡å‡†å‘½åæ–‡ä»¶ï¼Œè¿”å›None
            logger.warning(f"æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°{split_type}åˆ†å‰²çš„æ ‡å‡†å‘½åæ–‡ä»¶")
            if split_type == "train":
                raise ValueError(f"æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®çš„æ ‡å‡†å‘½åæ–‡ä»¶")
            return None
    
    
    def _extract_dataset_base_name(self, path: str) -> str:
        """
        æå–æ•°æ®é›†åŸºç¡€åç§°ï¼ˆä¸å«åˆ†å‰²åç¼€ï¼‰
        
        Args:
            path: æ•°æ®é›†è·¯å¾„
            
        Returns:
            æ ‡å‡†åŒ–çš„æ•°æ®é›†åŸºç¡€åç§°
        """
        if self._is_huggingface_dataset_names(path):
            # HuggingFaceæ•°æ®é›†ï¼šä¿æŒåŸå§‹åç§°ä¸å˜
            return path
        elif os.path.isdir(path):
            # æ–‡ä»¶å¤¹ï¼šä½¿ç”¨è·¯å¾„æœ€åéƒ¨åˆ†
            return os.path.basename(path.rstrip(os.path.sep))
        elif os.path.isfile(path):
            # æ–‡ä»¶ï¼šä½¿ç”¨ä¸å¸¦åç¼€çš„æ–‡ä»¶å
            filename = os.path.basename(path)
            # ç§»é™¤å¸¸è§çš„æ•°æ®æ–‡ä»¶åç¼€
            for ext in ['.json', '.jsonl', '.txt', '.csv']:
                if filename.endswith(ext):
                    filename = filename[:-len(ext)]
                    break
            return filename
        else:
            # æœªçŸ¥ç±»å‹ï¼Œç›´æ¥è¿”å›è·¯å¾„
            return path

    def _load_from_mixed_sources(self, mixed_paths: str, split_type: str) -> Union[Dataset, Dict[str, Dataset]]:
        """ä»æ··åˆæ•°æ®æºåŠ è½½æ•°æ®é›†ï¼Œæ”¯æŒHF_subsetæŒ‰HFæ•°æ®é›†é¡ºåºåˆ†é…"""
        path_list = [path.strip() for path in mixed_paths.split(",")]
        datasets = {}
        
        # ç¬¬ä¸€æ­¥ï¼šè¯†åˆ«æ‰€æœ‰HFæ•°æ®é›†å¹¶è®°å½•ç´¢å¼•
        hf_datasets_info = []  # [(åŸå§‹ç´¢å¼•, è·¯å¾„)]
        for i, path in enumerate(path_list):
            if self._is_huggingface_dataset_names(path):
                hf_datasets_info.append((i, path))
        
        # è®°å½•subsetåˆ†é…æƒ…å†µ
        if self.hf_subsets:
            logger.info(f"HFæ•°æ®é›†æ•°é‡: {len(hf_datasets_info)}, subseté…ç½®æ•°é‡: {len(self.hf_subsets)}")
            if len(hf_datasets_info) > len(self.hf_subsets):
                logger.info(f"HFæ•°æ®é›†å¤šäºsubseté…ç½®ï¼Œ{len(hf_datasets_info) - len(self.hf_subsets)}ä¸ªæ•°æ®é›†å°†ä½¿ç”¨é»˜è®¤subset: {self.DEFAULT_HF_SUBSET}")
            elif len(self.hf_subsets) > len(hf_datasets_info):
                unused_subsets = self.hf_subsets[len(hf_datasets_info):]
                logger.info(f"subseté…ç½®å¤šäºHFæ•°æ®é›†ï¼Œä»¥ä¸‹subsetå°†è¢«å¿½ç•¥: {unused_subsets}")
        
        # ç¬¬äºŒæ­¥ï¼šæŒ‰é¡ºåºå¤„ç†æ‰€æœ‰æ•°æ®é›†
        hf_dataset_counter = 0  # HFæ•°æ®é›†è®¡æ•°å™¨
        
        for original_index, path in enumerate(path_list):
            try:
                dataset = None
                dataset_base_name = self._extract_dataset_base_name(path)
                
                # åˆ¤æ–­è·¯å¾„ç±»å‹å¹¶åŠ è½½
                if self._is_huggingface_dataset_names(path):
                    # HFæ•°æ®é›†ï¼šè·å–å¯¹åº”çš„subset
                    subset_for_this_dataset = self._get_hf_subset_for_dataset(hf_dataset_counter)
                    
                    logger.info(f"ä»HuggingFaceåŠ è½½æ•°æ®é›†: {path} (HFæ•°æ®é›†#{hf_dataset_counter}, subset: {subset_for_this_dataset})")
                    
                    # ä¸´æ—¶åˆ›å»ºä¸“é—¨çš„DataLoaderå®ä¾‹æ¥å¤„ç†è¿™ä¸ªç‰¹å®šçš„subsetï¼Œä¿æŒsample_sizeé…ç½®
                    temp_loader = DataLoader(
                        hf_subset=subset_for_this_dataset,
                        train_sample_size=self.train_sample_size,
                        eval_sample_size=self.eval_sample_size,
                        test_sample_size=self.test_sample_size
                    )
                    dataset = temp_loader._load_from_hub_single(path, split_type)
                    
                    hf_dataset_counter += 1
                    
                elif os.path.isdir(path):
                    # æœ¬åœ°æ–‡ä»¶å¤¹ï¼šä¸éœ€è¦subset
                    logger.info(f"ä»æ–‡ä»¶å¤¹åŠ è½½æ•°æ®é›†: {path}")
                    dataset = self._load_from_directory(path, split_type)
                    
                elif os.path.isfile(path):
                    # æœ¬åœ°æ–‡ä»¶ï¼šä¸éœ€è¦subsetï¼Œä»…è®­ç»ƒé›†æ”¯æŒ
                    if split_type != "train":
                        logger.info(f"æœ¬åœ°æ–‡ä»¶ä»…æ”¯æŒè®­ç»ƒé›†ï¼Œè·³è¿‡éªŒè¯/æµ‹è¯•é›†åŠ è½½: {path}")
                        continue
                    logger.info(f"ä»æ–‡ä»¶åŠ è½½æ•°æ®é›†: {path}")
                    dataset = load_dataset("json", data_files=path, split="train")
                    dataset = self._apply_sample_size(dataset, split_type)
                else:
                    logger.warning(f"è·¯å¾„ä¸å­˜åœ¨æˆ–æ— æ³•è¯†åˆ«ï¼Œè·³è¿‡: {path}")
                    continue
                
                # æ·»åŠ æˆåŠŸåŠ è½½çš„æ•°æ®é›†
                if dataset is not None:
                    datasets[dataset_base_name] = dataset
                    logger.info(f"æˆåŠŸåŠ è½½æ•°æ®é›†: {dataset_base_name} (æ¥æº: {path})")
                
            except Exception as e:
                logger.error(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {path}, é”™è¯¯: {e}")
                continue
        
        if not datasets and split_type == "train":
            raise ValueError(f"æ‰€æœ‰æ··åˆæ•°æ®æºåŠ è½½å¤±è´¥")
        
        # æ··åˆæ•°æ®æºæ€»æ˜¯è¿”å›Dictæ ¼å¼ï¼ˆå³ä½¿åªæœ‰1ä¸ªæˆåŠŸï¼‰ï¼Œä¿æŒä¸€è‡´æ€§
        return datasets if datasets else None
    
    def _load_from_json_single(self, json_path: str, split_type: str) -> Optional[Dataset]:
        """ä»å•ä¸ªJSONæ–‡ä»¶åŠ è½½æ•°æ®é›†"""
        logger.info(f"ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®é›†: {json_path}")
        
        try:
            dataset = load_dataset("json", data_files=json_path, split="train")
            return self._apply_sample_size(dataset, split_type)
        except Exception as e:
            logger.error(f"ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
            if split_type == "train":
                raise ValueError(f"è®­ç»ƒæ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    
    def _load_default_dataset(self, split_type: str) -> Optional[Dataset]:
        """åŠ è½½é»˜è®¤ç¤ºä¾‹æ•°æ®é›†ï¼Œè‡ªåŠ¨æ˜ å°„åˆ†å‰²åç§°"""
        logger.info(f"ä½¿ç”¨é»˜è®¤æ•°æ®é›†: sentence-transformers/all-nli")
        try:
            # æ˜ å°„åˆ†å‰²åç§°ï¼šsentence-transformers/all-nli ä½¿ç”¨ dev è€Œä¸æ˜¯ eval
            actual_split = "dev" if split_type == "eval" else split_type
            
            # å°è¯•ä¸åŒçš„é…ç½®
            for config in ["pair-class", "pair-score"]:
                try:
                    dataset = load_dataset("sentence-transformers/all-nli", config, split=actual_split)
                    if split_type != actual_split:
                        logger.info(f"æˆåŠŸåŠ è½½é»˜è®¤æ•°æ®é›† {split_type} åˆ†å‰²ï¼ˆå®é™…ä½¿ç”¨ {actual_split}ï¼‰")
                    if config == "pair-class":
                        logger.info(f"âœ… æˆåŠŸä½¿ç”¨pair-classé…ç½®åŠ è½½é»˜è®¤æ•°æ®é›†")
                    return self._apply_sample_size(dataset, split_type)
                except Exception as config_error:
                    if config == "pair-class":
                        logger.warning(f"é»˜è®¤æ•°æ®é›†é…ç½® {config} å¤±è´¥: {config_error}")
                    continue
            
            logger.warning(f"é»˜è®¤æ•°æ®é›†çš„æ‰€æœ‰é…ç½®éƒ½å¤±è´¥")
            return None
        except Exception as e:
            logger.warning(f"é»˜è®¤æ•°æ®é›†çš„{split_type}åˆ†å‰²ä¸å­˜åœ¨: {e}")
            return None
    
    def _apply_sample_size(self, dataset: Union[Dataset, Dict[str, Dataset]], split_type: str = "train") -> Union[Dataset, Dict[str, Dataset]]:
        """
        åº”ç”¨æ ·æœ¬æ•°é‡é™åˆ¶
        
        Args:
            dataset: æ•°æ®é›†ï¼Œå¯ä»¥æ˜¯å•ä¸ªDatasetæˆ–Dict[str, Dataset]
            split_type: åˆ†å‰²ç±»å‹ï¼Œç”¨äºç¡®å®šä½¿ç”¨å“ªä¸ªsample_sizeå‚æ•°
            
        Returns:
            é™åˆ¶æ ·æœ¬æ•°é‡åçš„æ•°æ®é›†
        """
        # æ ¹æ®split_typeé€‰æ‹©å¯¹åº”çš„sample_size
        size_map = {
            "train": self.train_sample_size,
            "eval": self.eval_sample_size,
            "dev": self.eval_sample_size,  # devç­‰åŒäºeval
            "test": self.test_sample_size
        }
        sample_size = size_map.get(split_type, 0)
        
        if sample_size <= 0:
            return dataset
            
        # å¤„ç†å¤šæ•°æ®é›†åœºæ™¯
        if isinstance(dataset, dict):
            result = {}
            for name, ds in dataset.items():
                original_size = len(ds)
                if original_size > sample_size:
                    result[name] = ds.select(range(sample_size))
                    logger.info(f"æ•°æ®é›† {name}({split_type}): {original_size} â†’ {sample_size}")
                else:
                    result[name] = ds
                    logger.info(f"æ•°æ®é›† {name}({split_type}): {original_size} (æ— éœ€é™åˆ¶)")
            return result
        else:
            # å•æ•°æ®é›†åœºæ™¯
            original_size = len(dataset)
            if original_size > sample_size:
                result = dataset.select(range(sample_size))
                logger.info(f"æ•°æ®é›†({split_type}): {original_size} â†’ {sample_size}")
                return result
            else:
                logger.info(f"æ•°æ®é›†({split_type}): {original_size} (æ— éœ€é™åˆ¶)")
                return dataset
    
    def load_all_splits(self, dataset_path: str = None) -> Tuple[Union[Dataset, Dict[str, Dataset]], 
                                      Optional[Union[Dataset, Dict[str, Dataset]]], 
                                      Optional[Union[Dataset, Dict[str, Dataset]]]]:
        """
        åŠ è½½æ‰€æœ‰æ•°æ®åˆ†å‰²ï¼Œæ”¯æŒå•ä¸ªæ•°æ®é›†æˆ–å¤šä¸ªæ•°æ®é›†
        
        Args:
            dataset_path: æ•°æ®é›†è·¯å¾„ï¼Œå¿…é¡»æä¾›
        
        Returns:
            (train_dataset, eval_dataset, test_dataset)
            æ¯ä¸ªå¯èƒ½æ˜¯Datasetã€Dict[str, Dataset]æˆ–None
        """
        train_data = self.load_data("train", dataset_path)
        # åŠ è½½éªŒè¯é›†ï¼šè®©åº•å±‚çš„ _load_from_hub_single è‡ªåŠ¨å¤„ç†åˆ†å‰²åç§°å›é€€
        # ï¼ˆ"eval" -> "dev" -> "validation"ï¼‰
        eval_data = self.load_data("eval", dataset_path)
        test_data = self.load_data("test", dataset_path)
        
        return train_data, eval_data, test_data
    
    def is_multi_dataset(self, data) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ•°æ®é›†"""
        return isinstance(data, dict)
    
    def get_dataset_names(self, data) -> List[str]:
        """è·å–æ•°æ®é›†åç§°åˆ—è¡¨"""
        if self.is_multi_dataset(data):
            return list(data.keys())
        else:
            return ["single_dataset"]
    
    def _get_hf_subset_for_dataset(self, hf_dataset_index: int) -> Optional[str]:
        """
        æ ¹æ®HFæ•°æ®é›†çš„ç´¢å¼•è·å–å¯¹åº”çš„subset
        
        Args:
            hf_dataset_index: è¯¥æ•°æ®é›†åœ¨æ‰€æœ‰HFæ•°æ®é›†ä¸­çš„ç´¢å¼•ï¼ˆ0-basedï¼‰
            
        Returns:
            å¯¹åº”çš„subseté…ç½®ï¼Œè¶…å‡ºéƒ¨åˆ†è¿”å›Noneï¼ˆå°è¯•æ— é…ç½®åŠ è½½ï¼‰
        """
        if not self.hf_subsets or len(self.hf_subsets) == 0:
            # æ²¡æœ‰æŒ‡å®šä»»ä½•subsetï¼Œä½¿ç”¨é»˜è®¤å€¼
            return self.DEFAULT_HF_SUBSET
        
        # æ·»åŠ è¾¹ç•Œæ£€æŸ¥
        if hf_dataset_index < 0:
            logger.warning(f"æ•°æ®é›†ç´¢å¼•ä¸ºè´Ÿæ•°: {hf_dataset_index}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self.DEFAULT_HF_SUBSET
            
        if hf_dataset_index < len(self.hf_subsets):
            # æœ‰å¯¹åº”çš„subseté…ç½®
            return self.hf_subsets[hf_dataset_index]
        else:
            # HFæ•°æ®é›†æ•°é‡å¤§äºsubsetæ•°é‡ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            logger.info(f"æ•°æ®é›†ç´¢å¼• {hf_dataset_index} è¶…å‡ºé…ç½®æ•°é‡ {len(self.hf_subsets)}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {self.DEFAULT_HF_SUBSET}")
            return self.DEFAULT_HF_SUBSET
    
    def get_target_column(self, data: Union[Dataset, Dict[str, Dataset]]) -> str:
        """
        è·å–ç›®æ ‡åˆ—åï¼Œæ”¯æŒå•ä¸ªæ•°æ®é›†æˆ–å¤šä¸ªæ•°æ®é›†
        
        Args:
            data: æ•°æ®é›†æˆ–æ•°æ®é›†å­—å…¸
            
        Returns:
            ç›®æ ‡åˆ—å ('score' æˆ– 'label')
        """
        if self.is_multi_dataset(data):
            # å¤šæ•°æ®é›†ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªæ•°æ®é›†ç¡®å®šç›®æ ‡åˆ—
            first_dataset = next(iter(data.values()))
            return self._get_single_target_column(first_dataset)
        else:
            return self._get_single_target_column(data)
    
    def _get_single_target_column(self, dataset: Dataset) -> str:
        """è·å–å•ä¸ªæ•°æ®é›†çš„ç›®æ ‡åˆ—åï¼šä¸‰åˆ—æ ¼å¼ç›´æ¥ä½¿ç”¨ç¬¬ä¸‰åˆ—"""
        column_names = dataset.column_names
        
        # ä¸‰åˆ—æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨ç¬¬ä¸‰åˆ—ä½œä¸ºç›®æ ‡åˆ—
        if len(column_names) == 3:
            target_col = column_names[2]
            logger.info(f"ğŸ¯ ä½¿ç”¨ç¬¬ä¸‰åˆ—ä½œä¸ºç›®æ ‡åˆ—: '{target_col}'")
            return target_col
        
        # å…¼å®¹æ—§æ ¼å¼ï¼šä¼˜å…ˆä½¿ç”¨æ ‡å‡†åˆ—å
        if "score" in column_names:
            return "score"
        elif "label" in column_names:
            return "label"
        
        # å…¶ä»–æƒ…å†µæŠ¥é”™
        raise ValueError(f"æ— æ³•ç¡®å®šç›®æ ‡åˆ—ã€‚æ•°æ®é›†åˆ—å: {column_names}ï¼Œè¯·ä½¿ç”¨3åˆ—æ ¼å¼æˆ–åŒ…å«'score'/'label'åˆ—")
    
    def _standardize_dataset_columns(self, dataset: Dataset, target_column: str) -> Dataset:
        """
        æ ‡å‡†åŒ–æ•°æ®é›†åˆ—åä»¥ç¬¦åˆsentence-transformersè¦æ±‚
        
        æ ¹æ®å®˜æ–¹æ–‡æ¡£è¦æ±‚ï¼š
        1. æ ‡ç­¾åˆ—å¿…é¡»å‘½åä¸º "label" æˆ– "score"
        2. å…¶ä»–åˆ—åç§°æ— å…³ç´§è¦ï¼Œåªæœ‰é¡ºåºé‡è¦
        
        Args:
            dataset: è¾“å…¥æ•°æ®é›†
            target_column: ç›®æ ‡åˆ—åï¼ˆå½“å‰ä½¿ç”¨çš„æ ‡ç­¾åˆ—åï¼‰
            
        Returns:
            æ ‡å‡†åŒ–åçš„æ•°æ®é›†
        """
        column_names = dataset.column_names
        
        # å¦‚æœç›®æ ‡åˆ—å·²ç»æ˜¯æ ‡å‡†åç§°ï¼Œåˆ™ä¸éœ€è¦é‡å‘½å
        if target_column in ["label", "score"]:
            logger.info(f"ç›®æ ‡åˆ— '{target_column}' å·²ç»æ˜¯æ ‡å‡†åç§°ï¼Œæ— éœ€é‡å‘½å")
            return dataset
        
        # æ£€æŸ¥ç›®æ ‡åˆ—çš„æ•°æ®ç±»å‹æ¥ç¡®å®šé‡å‘½åç›®æ ‡
        sample_value = dataset[target_column][0]
        if isinstance(sample_value, int) or (isinstance(sample_value, float) and sample_value.is_integer()):
            # æ•´æ•°ç±»å‹ â†’ é‡å‘½åä¸º "label"
            new_name = "label"
            logger.info(f"å°†ç›®æ ‡åˆ— '{target_column}' é‡å‘½åä¸º 'label'ï¼ˆæ£€æµ‹åˆ°æ•´æ•°ç±»å‹ï¼‰")
        else:
            # æµ®ç‚¹æ•°ç±»å‹ â†’ é‡å‘½åä¸º "score" 
            new_name = "score"
            logger.info(f"å°†ç›®æ ‡åˆ— '{target_column}' é‡å‘½åä¸º 'score'ï¼ˆæ£€æµ‹åˆ°æµ®ç‚¹ç±»å‹ï¼‰")
        
        # åˆ›å»ºåˆ—åæ˜ å°„
        rename_mapping = {target_column: new_name}
        
        # æ‰§è¡Œé‡å‘½å
        standardized_dataset = dataset.rename_columns(rename_mapping)
        logger.info(f"æ•°æ®é›†åˆ—åæ ‡å‡†åŒ–å®Œæˆ: {column_names} â†’ {standardized_dataset.column_names}")
        
        return standardized_dataset
    
    def standardize_dataset_columns(self, data: Union[Dataset, Dict[str, Dataset]], target_column: str) -> Union[Dataset, Dict[str, Dataset]]:
        """
        å¯¹å•ä¸ªæˆ–å¤šä¸ªæ•°æ®é›†è¿›è¡Œåˆ—åæ ‡å‡†åŒ–
        
        Args:
            data: æ•°æ®é›†æˆ–æ•°æ®é›†å­—å…¸
            target_column: ç›®æ ‡åˆ—å
            
        Returns:
            æ ‡å‡†åŒ–åçš„æ•°æ®é›†
        """
        if self.is_multi_dataset(data):
            # å¤šæ•°æ®é›†ï¼šå¯¹æ¯ä¸ªæ•°æ®é›†è¿›è¡Œæ ‡å‡†åŒ–
            standardized_datasets = {}
            for name, dataset in data.items():
                if dataset is not None:
                    standardized_datasets[name] = self._standardize_dataset_columns(dataset, target_column)
                    logger.info(f"æ•°æ®é›† '{name}' åˆ—åæ ‡å‡†åŒ–å®Œæˆ")
                else:
                    standardized_datasets[name] = dataset
            return standardized_datasets
        else:
            # å•æ•°æ®é›†
            return self._standardize_dataset_columns(data, target_column)
    
    def validate_dataset(self, data: Union[Dataset, Dict[str, Dataset]]) -> bool:
        """
        éªŒè¯æ•°æ®é›†æ ¼å¼ï¼Œæ”¯æŒå•ä¸ªæ•°æ®é›†æˆ–å¤šä¸ªæ•°æ®é›†
        
        Args:
            data: æ•°æ®é›†æˆ–æ•°æ®é›†å­—å…¸
            
        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        if self.is_multi_dataset(data):
            # å¤šæ•°æ®é›†ï¼šéªŒè¯æ‰€æœ‰æ•°æ®é›†
            for name, dataset in data.items():
                try:
                    self._validate_single_dataset(dataset)
                    logger.info(f"æ•°æ®é›† {name} éªŒè¯é€šè¿‡")
                except ValueError as e:
                    raise ValueError(f"æ•°æ®é›† {name} éªŒè¯å¤±è´¥: {e}")
        else:
            self._validate_single_dataset(data)
        
        return True
    
    def _validate_single_dataset(self, dataset: Dataset) -> bool:
        """éªŒè¯å•ä¸ªæ•°æ®é›†æ ¼å¼ï¼šå¿…é¡»ä¸ºä¸‰åˆ—ï¼ˆæ–‡æœ¬1ï¼Œæ–‡æœ¬2ï¼Œæ ‡ç­¾ï¼‰"""
        column_names = dataset.column_names
        
        # æ£€æŸ¥åˆ—æ•°é‡ï¼šå¿…é¡»ä¸º3åˆ—
        if len(column_names) != 3:
            raise ValueError(f"æ•°æ®é›†å¿…é¡»ä¸º3åˆ—æ ¼å¼ï¼ˆæ–‡æœ¬1ï¼Œæ–‡æœ¬2ï¼Œæ ‡ç­¾ï¼‰ï¼Œå½“å‰æœ‰{len(column_names)}åˆ—: {column_names}")
        
        # éªŒè¯å‰ä¸¤åˆ—æ˜¯å¦ä¸ºæ–‡æœ¬ç±»å‹
        try:
            sample = dataset[0] if len(dataset) > 0 else None
            if sample:
                col1, col2, col3 = column_names[0], column_names[1], column_names[2]
                
                # æ£€æŸ¥å‰ä¸¤åˆ—æ˜¯æ–‡æœ¬
                if not isinstance(sample[col1], str):
                    raise ValueError(f"ç¬¬1åˆ— '{col1}' å¿…é¡»æ˜¯æ–‡æœ¬ç±»å‹ï¼Œå½“å‰ç±»å‹: {type(sample[col1])}")
                if not isinstance(sample[col2], str):
                    raise ValueError(f"ç¬¬2åˆ— '{col2}' å¿…é¡»æ˜¯æ–‡æœ¬ç±»å‹ï¼Œå½“å‰ç±»å‹: {type(sample[col2])}")
                
                # æ£€æŸ¥ç¬¬ä¸‰åˆ—æ˜¯æ•°å€¼ç±»å‹ï¼ˆint, floatï¼‰æˆ–å¯è½¬æ¢ä¸ºæ•°å€¼
                label_value = sample[col3]
                if not self._is_valid_label(label_value):
                    raise ValueError(f"ç¬¬3åˆ— '{col3}' å¿…é¡»æ˜¯æ•°å€¼ç±»å‹ï¼ˆint/floatï¼‰æˆ–å¯è½¬æ¢ä¸ºæ•°å€¼ï¼Œå½“å‰å€¼: {label_value} ({type(label_value).__name__})")
                
                label_type = "float" if isinstance(label_value, float) or self._is_float_like(label_value) else "int"
                logger.info(f"âœ… æ•°æ®é›†éªŒè¯é€šè¿‡: æ–‡æœ¬1='{col1}', æ–‡æœ¬2='{col2}', æ ‡ç­¾='{col3}' ({label_type})")
                
        except (IndexError, KeyError) as e:
            raise ValueError(f"æ— æ³•éªŒè¯æ•°æ®é›†æ ¼å¼: {e}")
        
        return True
    
    def _is_valid_label(self, value) -> bool:
        """æ£€æŸ¥å€¼æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ ‡ç­¾ï¼ˆæ•°å€¼ç±»å‹æˆ–å¯è½¬æ¢ä¸ºæ•°å€¼ï¼‰"""
        # ç›´æ¥æ˜¯æ•°å€¼ç±»å‹
        if isinstance(value, (int, float)):
            return True
        
        # å­—ç¬¦ä¸²æ•°å€¼
        try:
            float(str(value))
            return True
        except (ValueError, TypeError):
            return False
    
    def _is_float_like(self, value) -> bool:
        """æ£€æŸ¥å€¼æ˜¯å¦ä¸ºæµ®ç‚¹æ•°ç±»å‹"""
        try:
            if isinstance(value, float):
                return True
            if isinstance(value, str) and ('.' in value or 'e' in value.lower()):
                float(value)
                return True
            return False
        except (ValueError, TypeError):
            return False

# ä¿æŒå‘åå…¼å®¹çš„å‡½æ•°
def load_training_data(split_type):
    loader = DataLoader()
    return loader.load_data(split_type)