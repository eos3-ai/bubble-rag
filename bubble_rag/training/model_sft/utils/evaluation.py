"""ç»Ÿä¸€è¯„ä¼°æ¨¡å—

æ”¯æŒembeddingå’Œrerankerä¸¤ç§æ¨¡å‹ç±»å‹çš„è¯„ä¼°ï¼Œ
æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„è¯„ä¼°å™¨ã€‚
"""
import logging
import os
from typing import Optional, Dict, Any
from datasets import Dataset
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, BinaryClassificationEvaluator, SimilarityFunction, SequentialEvaluator
from sentence_transformers.cross_encoder.evaluation import CrossEncoderCorrelationEvaluator, CrossEncoderClassificationEvaluator

logger = logging.getLogger(__name__)

class UnifiedEvaluator:
    """
    ç»Ÿä¸€çš„è¯„ä¼°å™¨å·¥å‚ç±»
    
    æ ¹æ®æ¨¡å‹ç±»å‹ï¼ˆembeddingæˆ–rerankerï¼‰è‡ªåŠ¨åˆ›å»ºç›¸åº”çš„è¯„ä¼°å™¨ï¼Œ
    ç®€åŒ–äº†ä¸åŒæ¨¡å‹ç±»å‹çš„è¯„ä¼°æµç¨‹ã€‚
    
    Attributes:
        model_type (str): æ¨¡å‹ç±»å‹ï¼Œ'embedding' æˆ– 'reranker'
    """
    
    def __init__(self, model_type: Optional[str] = None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            model_type: 'embedding' æˆ– 'reranker'ï¼Œå¦‚æœä¸ºNoneåˆ™ä»TRAIN_TYPEç¯å¢ƒå˜é‡è·å–
        """
        if model_type is None:
            model_type = os.getenv("TRAIN_TYPE", "embedding").lower()
            
        if model_type not in ['embedding', 'reranker']:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}. åªæ”¯æŒ 'embedding' æˆ– 'reranker'")
        self.model_type = model_type
        logger.info(f"åˆå§‹åŒ–ç»Ÿä¸€è¯„ä¼°å™¨ï¼Œæ¨¡å‹ç±»å‹: {self.model_type}")
    
    def create_evaluator(self, dataset: Dataset, target_column: str, name: str):
        """
        æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºç›¸åº”çš„è¯„ä¼°å™¨
        
        Args:
            dataset: è¯„ä¼°æ•°æ®é›†
            target_column: ç›®æ ‡åˆ—å ('score' æˆ– 'label')
            name: è¯„ä¼°å™¨åç§°
            
        Returns:
            è¯„ä¼°å™¨å®ä¾‹
        """
        # åº”ç”¨ä¸è®­ç»ƒæ•°æ®é›†ç›¸åŒçš„åˆ—è¿‡æ»¤é€»è¾‘
        filtered_dataset = self._filter_dataset_columns(dataset, target_column, name)
        
        if self.model_type == "embedding":
            return self._create_embedding_evaluator(filtered_dataset, target_column, name)
        else:  # reranker
            return self._create_reranker_evaluator(filtered_dataset, target_column, name)
    
    def _create_embedding_evaluator(self, dataset: Dataset, target_column: str, name: str):
        """
        åˆ›å»ºembeddingæ¨¡å‹è¯„ä¼°å™¨
        
        æ ¹æ®æ ‡ç­¾ç±»å‹è‡ªåŠ¨é€‰æ‹©è¯„ä¼°å™¨ï¼š
        - scoreå­—æ®µï¼ˆæµ®ç‚¹æ•°ï¼‰â†’ EmbeddingSimilarityEvaluatorï¼ˆå›å½’ä»»åŠ¡ï¼‰
        - labelå­—æ®µï¼ˆæ•´æ•°ï¼‰â†’ BinaryClassificationEvaluatorï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
        
        Args:
            dataset: åŒ…å«å¥å­å¯¹å’Œç›®æ ‡å€¼çš„æ•°æ®é›†
            target_column: ç›®æ ‡åˆ—å ('score' æˆ– 'label')
            name: è¯„ä¼°å™¨åç§°
            
        Returns:
            å¯¹åº”çš„è¯„ä¼°å™¨å®ä¾‹
        """
        # è·å–å¥å­å¯¹æ•°æ®ï¼ˆä½¿ç”¨ä¸rerankerç›¸åŒçš„çµæ´»å¤„ç†ï¼‰
        if hasattr(dataset, 'column_names'):
            column_names = dataset.column_names
        elif isinstance(dataset, dict):
            # å¤„ç†å­—å…¸ç±»å‹çš„æ•°æ®é›†
            column_names = list(dataset.keys()) if dataset else []
        else:
            logger.warning(f"æ— æ³•è·å–æ•°æ®é›†åˆ—åï¼Œç±»å‹: {type(dataset)}")
            column_names = []

        logger.info(f"æ•°æ®é›†åˆ—å: {column_names}")
        
        # æå–å¥å­å¯¹
        sentences1, sentences2 = self._get_sentence_columns(dataset, column_names, target_column)
        
        # æ£€æŸ¥æ•°æ®ç±»å‹å’Œæ ‡ç­¾å€¼æ¥å†³å®šè¯„ä¼°å™¨ç±»å‹
        labels = list(dataset[target_column])
        unique_labels = set(labels)
        
        # åªæœ‰å½“æ‰€æœ‰æ ‡ç­¾éƒ½æ˜¯ 0 æˆ– 1 æ—¶æ‰ä½¿ç”¨ BinaryClassificationEvaluator
        is_binary_classification = (
            len(unique_labels) <= 2 and 
            all(label in [0, 1] for label in unique_labels)
        )
        
        logger.info(f"æ•°æ®é›†æ ‡ç­¾åˆ†æ - åˆ—å: {target_column}, å”¯ä¸€å€¼: {sorted(unique_labels)}, æ ·æœ¬: {labels[:5]}")
        
        is_classification = is_binary_classification
        
        if is_classification:
            logger.info(f"ä½¿ç”¨BinaryClassificationEvaluatorï¼ˆæ£€æµ‹åˆ°åˆ†ç±»ä»»åŠ¡ï¼Œåˆ—å: {target_column}ï¼‰")
            return BinaryClassificationEvaluator(
                sentences1=sentences1,
                sentences2=sentences2,
                labels=list(dataset[target_column]),
                name=name,
            )
        else:
            logger.info(f"ä½¿ç”¨EmbeddingSimilarityEvaluatorï¼ˆæ£€æµ‹åˆ°å›å½’ä»»åŠ¡ï¼Œåˆ—å: {target_column}ï¼‰")
            return EmbeddingSimilarityEvaluator(
                sentences1=sentences1,
                sentences2=sentences2,
                scores=list(dataset[target_column]),
                main_similarity=SimilarityFunction.COSINE,
                name=name,
            )
    
    def _create_reranker_evaluator(self, dataset: Dataset, target_column: str, name: str):
        """
        åˆ›å»ºrerankeræ¨¡å‹è¯„ä¼°å™¨
        
        æ ¹æ®æ ‡ç­¾ç±»å‹è‡ªåŠ¨é€‰æ‹©è¯„ä¼°å™¨ï¼š
        - labelå­—æ®µï¼ˆæ•´æ•°ï¼‰â†’ CrossEncoderClassificationEvaluatorï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
        - scoreå­—æ®µï¼ˆæµ®ç‚¹æ•°ï¼‰â†’ CrossEncoderCorrelationEvaluatorï¼ˆå›å½’ä»»åŠ¡ï¼‰
        
        Args:
            dataset: åŒ…å«å¥å­å¯¹å’Œç›®æ ‡å€¼çš„æ•°æ®é›†
            target_column: ç›®æ ‡åˆ—å ('score' æˆ– 'label')
            name: è¯„ä¼°å™¨åç§°
            
        Returns:
            å¯¹åº”çš„CrossEncoderè¯„ä¼°å™¨å®ä¾‹
        """
        # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºNone
        if dataset is None:
            logger.error("æ•°æ®é›†ä¸ºNoneï¼Œæ— æ³•åˆ›å»ºè¯„ä¼°å™¨")
            return None
        
        # è‡ªåŠ¨è¯†åˆ«å¥å­å¯¹åˆ—å
        if hasattr(dataset, 'column_names'):
            column_names = dataset.column_names
        elif isinstance(dataset, dict):
            # å¤„ç†å­—å…¸ç±»å‹çš„æ•°æ®é›†
            column_names = list(dataset.keys()) if dataset else []
        else:
            logger.warning(f"æ— æ³•è·å–æ•°æ®é›†åˆ—åï¼Œç±»å‹: {type(dataset)}")
            column_names = []

        logger.info(f"æ•°æ®é›†åˆ—å: {column_names}")
        
        # è·å–å¥å­å¯¹æ•°æ®
        sentence_pairs = self._get_sentence_pairs(dataset, column_names, target_column)
        
        # æ ¹æ®æ ‡ç­¾ç±»å‹é€‰æ‹©è¯„ä¼°å™¨
        labels = list(dataset[target_column])
        unique_labels = set(labels)
        
        # åªæœ‰å½“æ‰€æœ‰æ ‡ç­¾éƒ½æ˜¯ 0 æˆ– 1 æ—¶æ‰ä½¿ç”¨åˆ†ç±»è¯„ä¼°å™¨
        is_classification = (
            len(unique_labels) <= 2 and 
            all(label in [0, 1] for label in unique_labels)
        )
        
        logger.info(f"æ•°æ®é›†æ ‡ç­¾åˆ†æ - åˆ—å: {target_column}, å”¯ä¸€å€¼: {sorted(unique_labels)}, æ ·æœ¬: {labels[:5]}")
        
        if is_classification:
            logger.info(f"ä½¿ç”¨CrossEncoderClassificationEvaluatorï¼ˆæ£€æµ‹åˆ°åˆ†ç±»ä»»åŠ¡ï¼Œåˆ—å: {target_column}ï¼‰")
            return CrossEncoderClassificationEvaluator(
                sentence_pairs=sentence_pairs,
                labels=dataset[target_column],
                name=name,
            )
        else:
            logger.info(f"ä½¿ç”¨CrossEncoderCorrelationEvaluatorï¼ˆæ£€æµ‹åˆ°å›å½’ä»»åŠ¡ï¼Œåˆ—å: {target_column}ï¼‰")
            return CrossEncoderCorrelationEvaluator(
                sentence_pairs=sentence_pairs,
                scores=dataset[target_column],
                name=name,
            )
    
    def _get_sentence_pairs(self, dataset: Dataset, column_names: list, target_column: str):
        """æå–å¥å­å¯¹æ•°æ®"""
        # å°è¯•ä¸åŒçš„å¥å­å¯¹åˆ—åç»„åˆ
        if "sentence1" in column_names and "sentence2" in column_names:
            sentence_pairs = list(zip(dataset["sentence1"], dataset["sentence2"]))
            logger.info("ä½¿ç”¨ sentence1/sentence2 åˆ—")
        elif "query" in column_names and "passage" in column_names:
            sentence_pairs = list(zip(dataset["query"], dataset["passage"]))
            logger.info("ä½¿ç”¨ query/passage åˆ—")
        elif "anchor" in column_names and "positive" in column_names:
            sentence_pairs = list(zip(dataset["anchor"], dataset["positive"]))
            logger.info("ä½¿ç”¨ anchor/positive åˆ—")
        elif "text1" in column_names and "text2" in column_names:
            sentence_pairs = list(zip(dataset["text1"], dataset["text2"]))
            logger.info("ä½¿ç”¨ text1/text2 åˆ—")
        else:
            # å¦‚æœæ‰¾ä¸åˆ°æ ‡å‡†çš„å¥å­å¯¹åˆ—ï¼Œå°è¯•ä½¿ç”¨å‰ä¸¤ä¸ªæ–‡æœ¬åˆ—
            text_columns = [col for col in column_names if col != target_column and isinstance(dataset[col][0], str)]
            if len(text_columns) >= 2:
                sentence_pairs = list(zip(dataset[text_columns[0]], dataset[text_columns[1]]))
                logger.info(f"ä½¿ç”¨ {text_columns[0]}/{text_columns[1]} åˆ—")
            else:
                raise ValueError(f"æ— æ³•åœ¨æ•°æ®é›†ä¸­æ‰¾åˆ°å¥å­å¯¹åˆ—ã€‚å¯ç”¨åˆ—: {column_names}")
        
        return sentence_pairs
    
    def _get_sentence_columns(self, dataset: Dataset, column_names: list, target_column: str):
        """æå–å¥å­åˆ—æ•°æ®ï¼ˆç”¨äºembeddingè¯„ä¼°å™¨ï¼‰"""
        # å°è¯•ä¸åŒçš„å¥å­å¯¹åˆ—åç»„åˆ
        if "sentence1" in column_names and "sentence2" in column_names:
            sentences1 = list(dataset["sentence1"])
            sentences2 = list(dataset["sentence2"])
            logger.info("ä½¿ç”¨ sentence1/sentence2 åˆ—")
        elif "query" in column_names and "passage" in column_names:
            sentences1 = list(dataset["query"])
            sentences2 = list(dataset["passage"])
            logger.info("ä½¿ç”¨ query/passage åˆ—")
        elif "anchor" in column_names and "positive" in column_names:
            sentences1 = list(dataset["anchor"])
            sentences2 = list(dataset["positive"])
            logger.info("ä½¿ç”¨ anchor/positive åˆ—")
        elif "text1" in column_names and "text2" in column_names:
            sentences1 = list(dataset["text1"])
            sentences2 = list(dataset["text2"])
            logger.info("ä½¿ç”¨ text1/text2 åˆ—")
        else:
            # å¦‚æœæ‰¾ä¸åˆ°æ ‡å‡†çš„å¥å­å¯¹åˆ—ï¼Œå°è¯•ä½¿ç”¨å‰ä¸¤ä¸ªæ–‡æœ¬åˆ—
            text_columns = [col for col in column_names if col != target_column and isinstance(dataset[col][0], str)]
            if len(text_columns) >= 2:
                sentences1 = list(dataset[text_columns[0]])
                sentences2 = list(dataset[text_columns[1]])
                logger.info(f"ä½¿ç”¨ {text_columns[0]}/{text_columns[1]} åˆ—")
            else:
                raise ValueError(f"æ— æ³•åœ¨æ•°æ®é›†ä¸­æ‰¾åˆ°å¥å­å¯¹åˆ—ã€‚å¯ç”¨åˆ—: {column_names}")
        
        return sentences1, sentences2
    
    def _filter_dataset_columns(self, dataset: Dataset, target_column: str, name: str) -> Dataset:
        """
        è¿‡æ»¤æ•°æ®é›†åˆ—ï¼Œç¡®ä¿åªæœ‰2ä¸ªè¾“å…¥åˆ— + 1ä¸ªç›®æ ‡åˆ—
        
        è¿™ä¸è®­ç»ƒæ•°æ®é›†çš„åˆ—è¿‡æ»¤é€»è¾‘ä¿æŒä¸€è‡´ï¼Œç¡®ä¿CrossEncoderè¯„ä¼°å™¨
        èƒ½å¤Ÿæ­£ç¡®å¤„ç†æ•°æ®é›†æ ¼å¼ã€‚
        
        Args:
            dataset: åŸå§‹æ•°æ®é›†
            target_column: ç›®æ ‡åˆ—å
            name: æ•°æ®é›†åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            è¿‡æ»¤åçš„æ•°æ®é›†
        """
        if hasattr(dataset, 'column_names'):
            column_names = dataset.column_names
        elif isinstance(dataset, dict):
            column_names = list(dataset.keys()) if dataset else []
        else:
            logger.warning(f"æ— æ³•è·å–æ•°æ®é›†åˆ—åï¼Œç±»å‹: {type(dataset)}ï¼Œè¿”å›åŸæ•°æ®é›†")
            return dataset

        if len(column_names) >= 3:
            # ç¡®ä¿åªæœ‰3åˆ—ï¼š2ä¸ªè¾“å…¥åˆ— + 1ä¸ªç›®æ ‡åˆ—
            input_columns = [col for col in column_names if col != target_column][:2]
            columns_to_keep = input_columns + [target_column]
            filtered_dataset = dataset.select_columns(columns_to_keep)
            logger.info(f"è¯„ä¼°æ•°æ®é›† '{name}' åˆ—è¿‡æ»¤: {column_names} â†’ {columns_to_keep}")
            return filtered_dataset
        else:
            logger.info(f"è¯„ä¼°æ•°æ®é›† '{name}' æ— éœ€åˆ—è¿‡æ»¤ï¼ˆå·²æ˜¯ {len(column_names)} åˆ—ï¼‰")
            return dataset
    
    def evaluate_model(self, model, evaluator) -> Dict[str, float]:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½
        
        Args:
            model: å¾…è¯„ä¼°çš„æ¨¡å‹
            evaluator: è¯„ä¼°å™¨
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        try:
            results = evaluator(model)
            logger.info(f"è¯„ä¼°å®Œæˆï¼Œç»“æœ: {results}")
            return results
        except Exception as e:
            logger.error(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise
    
    def create_evaluators_from_datasets(self, 
                                      eval_dataset: Optional[Dataset],
                                      test_dataset: Optional[Dataset],
                                      target_column: str,
                                      run_name: str) -> Dict[str, Any]:
        """
        ä»æ•°æ®é›†åˆ›å»ºæ‰€æœ‰å¿…è¦çš„è¯„ä¼°å™¨
        
        Args:
            eval_dataset: éªŒè¯æ•°æ®é›†
            test_dataset: æµ‹è¯•æ•°æ®é›†  
            target_column: ç›®æ ‡åˆ—å
            run_name: è¿è¡Œåç§°
            
        Returns:
            åŒ…å«æ‰€æœ‰è¯„ä¼°å™¨çš„å­—å…¸
        """
        evaluators = {}
        
        # åˆ›å»ºéªŒè¯è¯„ä¼°å™¨
        if eval_dataset is not None and not (isinstance(eval_dataset, dict) and len(eval_dataset) == 0):
            if isinstance(eval_dataset, dict):
                # å¤šæ•°æ®é›†ï¼šä½¿ç”¨ create_multi_evaluator
                eval_evaluator = self.create_multi_evaluator(
                    eval_dataset, target_column, f"{run_name}-eval"
                )
                logger.info(f"åˆ›å»ºå¤šæ•°æ®é›†éªŒè¯è¯„ä¼°å™¨æˆåŠŸ: {run_name}-eval")
            else:
                # å•æ•°æ®é›†ï¼šä½¿ç”¨ create_evaluator
                eval_evaluator = self.create_evaluator(
                    eval_dataset, target_column, f"{run_name}-eval"
                )
                logger.info(f"åˆ›å»ºå•æ•°æ®é›†éªŒè¯è¯„ä¼°å™¨æˆåŠŸ: {run_name}-eval")
            
            # éªŒè¯ç”¨é€”çš„è¯„ä¼°å™¨ç»Ÿä¸€ä½¿ç”¨ 'dev' é”®å
            evaluators['dev'] = eval_evaluator
        else:
            logger.info("è·³è¿‡éªŒè¯è¯„ä¼°å™¨åˆ›å»ºï¼ˆæ²¡æœ‰éªŒè¯æ•°æ®é›†ï¼‰")
        
        # åˆ›å»ºæµ‹è¯•è¯„ä¼°å™¨
        if test_dataset is not None and not (isinstance(test_dataset, dict) and len(test_dataset) == 0):
            if isinstance(test_dataset, dict):
                # å¤šæ•°æ®é›†ï¼šä½¿ç”¨ create_multi_evaluator
                evaluators['test'] = self.create_multi_evaluator(
                    test_dataset, target_column, f"{run_name}-test"
                )
                logger.info(f"åˆ›å»ºå¤šæ•°æ®é›†æµ‹è¯•è¯„ä¼°å™¨æˆåŠŸ: {run_name}-test")
            else:
                # å•æ•°æ®é›†ï¼šä½¿ç”¨ create_evaluator
                evaluators['test'] = self.create_evaluator(
                    test_dataset, target_column, f"{run_name}-test"
                )
                logger.info(f"åˆ›å»ºå•æ•°æ®é›†æµ‹è¯•è¯„ä¼°å™¨æˆåŠŸ: {run_name}-test")
        else:
            logger.info("è·³è¿‡æµ‹è¯•è¯„ä¼°å™¨åˆ›å»ºï¼ˆæ²¡æœ‰æµ‹è¯•æ•°æ®é›†ï¼‰")
        
        return evaluators
    
    def create_multi_evaluator(self,
                              eval_datasets: Dict[str, Dataset],
                              target_column: str,
                              run_name: str,
                              data_source_mapping: Optional[Dict[str, str]] = None) -> Optional[Any]:
        """
        ä¸ºå¤šä¸ªæ•°æ®é›†åˆ›å»º SequentialEvaluator

        Args:
            eval_datasets: å¤šä¸ªéªŒè¯æ•°æ®é›†çš„å­—å…¸
            target_column: ç›®æ ‡åˆ—å
            run_name: è¿è¡Œåç§°
            data_source_mapping: dataset_nameåˆ°source_idçš„æ˜ å°„ï¼ˆå¯é€‰ï¼‰

        Returns:
            SequentialEvaluator å®ä¾‹ï¼Œå¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®é›†åˆ™è¿”å› None
        """
        if not eval_datasets:
            logger.info("æ²¡æœ‰éªŒè¯æ•°æ®é›†ï¼Œè·³è¿‡å¤šè¯„ä¼°å™¨åˆ›å»º")
            return None
        
        evaluators = []
        for dataset_name, dataset in eval_datasets.items():
            if dataset is None:
                logger.warning(f"æ•°æ®é›† {dataset_name} ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            # ä¸ºæ¯ä¸ªæ•°æ®é›†å•ç‹¬ç¡®å®šç›®æ ‡åˆ—å
            dataset_target_column = self._get_dataset_target_column(dataset)
            logger.info(f"æ•°æ®é›† {dataset_name} ä½¿ç”¨ç›®æ ‡åˆ—: {dataset_target_column}")

            # ä½¿ç”¨source_idä½œä¸ºevaluator nameï¼ˆå¦‚æœæä¾›äº†æ˜ å°„ï¼‰
            if data_source_mapping and dataset_name in data_source_mapping:
                evaluator_name = data_source_mapping[dataset_name]
                logger.info(f"ä½¿ç”¨source_idä½œä¸ºevaluator name: {dataset_name} -> {evaluator_name}")
            else:
                evaluator_name = f"{run_name}-{dataset_name}"
                logger.info(f"ä½¿ç”¨é»˜è®¤åç§°ä½œä¸ºevaluator name: {evaluator_name}")

            evaluator = self.create_evaluator(
                dataset, dataset_target_column, evaluator_name
            )
            
            if evaluator is not None:
                evaluators.append(evaluator)
                logger.info(f"ä¸ºæ•°æ®é›† {dataset_name} åˆ›å»ºè¯„ä¼°å™¨æˆåŠŸ")
        
        if not evaluators:
            logger.info("æ²¡æœ‰æœ‰æ•ˆçš„è¯„ä¼°å™¨ï¼Œè¿”å›None")
            return None
        
        if len(evaluators) == 1:
            logger.info("åªæœ‰ä¸€ä¸ªè¯„ä¼°å™¨ï¼Œç›´æ¥è¿”å›å•ä¸ªè¯„ä¼°å™¨")
            return evaluators[0]
        
        # åˆ›å»º SequentialEvaluator
        sequential_evaluator = SequentialEvaluator(evaluators)
        # å¯ç”¨å‰ç¼€åŠŸèƒ½ï¼Œè¿™æ ·æ¯ä¸ªmetricä¼šåŠ ä¸Ševaluator nameï¼ˆå³source_idï¼‰ä½œä¸ºå‰ç¼€
        sequential_evaluator.prefix_name_to_metrics = True
        logger.info(f"åˆ›å»º SequentialEvaluator æˆåŠŸï¼ŒåŒ…å« {len(evaluators)} ä¸ªè¯„ä¼°å™¨ï¼Œå·²å¯ç”¨metricå‰ç¼€")
        return sequential_evaluator
    
    def _get_dataset_target_column(self, dataset: Dataset) -> str:
        """
        ä¸ºå•ä¸ªæ•°æ®é›†ç¡®å®šç›®æ ‡åˆ—å
        
        Args:
            dataset: æ•°æ®é›†
            
        Returns:
            ç›®æ ‡åˆ—å
        """
        column_names = dataset.column_names
        
        # ä¸‰åˆ—æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨ç¬¬ä¸‰åˆ—ä½œä¸ºç›®æ ‡åˆ—
        if len(column_names) == 3:
            target_col = column_names[2]
            logger.info(f"ğŸ¯ ä¸‰åˆ—æ ¼å¼ï¼Œä½¿ç”¨ç¬¬ä¸‰åˆ—ä½œä¸ºç›®æ ‡åˆ—: '{target_col}'")
            return target_col
        
        # ä¼˜å…ˆä½¿ç”¨æ ‡å‡†åˆ—å
        if "score" in column_names:
            return "score"
        elif "label" in column_names:
            return "label"
        elif "similarity_score" in column_names:
            return "similarity_score"
        else:
            # å¦‚æœæ²¡æœ‰æ ‡å‡†åˆ—åï¼Œä½¿ç”¨æœ€åä¸€åˆ—
            target_col = column_names[-1]
            logger.warning(f"æœªæ‰¾åˆ°æ ‡å‡†ç›®æ ‡åˆ—åï¼Œä½¿ç”¨æœ€åä¸€åˆ—: '{target_col}'")
            return target_col

def create_evaluator(model_type: str, dataset: Dataset, target_column: str, name: str):
    """
    åˆ›å»ºè¯„ä¼°å™¨çš„ä¾¿æ·å‡½æ•°
    
    è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ä¾¿æ·å‡½æ•°ï¼Œç”¨äºå¿«é€Ÿåˆ›å»ºå•ä¸ªè¯„ä¼°å™¨ã€‚
    å¯¹äºæ‰¹é‡åˆ›å»ºå¤šä¸ªè¯„ä¼°å™¨ï¼Œå»ºè®®ä½¿ç”¨UnifiedEvaluatorç±»ã€‚
    
    Args:
        model_type (str): æ¨¡å‹ç±»å‹ï¼Œ'embedding' æˆ– 'reranker'
        dataset (Dataset): è¯„ä¼°æ•°æ®é›†
        target_column (str): ç›®æ ‡åˆ—å
        name (str): è¯„ä¼°å™¨åç§°
        
    Returns:
        è¯„ä¼°å™¨å®ä¾‹ï¼ˆEmbeddingSimilarityEvaluator æˆ– CrossEncoderCorrelationEvaluatorï¼‰
        
    Example:
        evaluator = create_evaluator('embedding', eval_dataset, 'score', 'dev-eval')
    """
    evaluator_factory = UnifiedEvaluator(model_type)
    return evaluator_factory.create_evaluator(dataset, target_column, name)