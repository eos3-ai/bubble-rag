"""
GPUè®¾å¤‡ç®¡ç†å·¥å…·å‡½æ•°
ç»Ÿä¸€å•è¿›ç¨‹å’Œå¤šè¿›ç¨‹è®­ç»ƒçš„GPUè®¾å¤‡å¤„ç†é€»è¾‘
"""
import os
import logging
from typing import Dict, List, Tuple, Optional

logger = logging.getLogger(__name__)


def parse_device_to_cuda_visible_devices(device: str) -> Tuple[str, bool]:
    """
    å°†è®¾å¤‡å­—ç¬¦ä¸²è½¬æ¢ä¸ºCUDA_VISIBLE_DEVICESæ ¼å¼
    
    Args:
        device: è®¾å¤‡å­—ç¬¦ä¸²ï¼Œæ”¯æŒ:
            - "cpu": CPUæ¨¡å¼
            - "auto": è‡ªåŠ¨æ¨¡å¼ï¼ˆä¸è®¾ç½®é™åˆ¶ï¼‰
            - "cuda:0": å•GPU
            - "cuda:0,cuda:1": å¤šGPU
    
    Returns:
        Tuple[str, bool]: (CUDA_VISIBLE_DEVICESå€¼, æ˜¯å¦éœ€è¦è®¾ç½®çŽ¯å¢ƒå˜é‡)
    """
    if device == "cpu":
        return "", True  # ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºCPUæ¨¡å¼
    elif device == "auto":
        return "", False  # ä¸è®¾ç½®çŽ¯å¢ƒå˜é‡ï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©
    elif device and device.startswith("cuda:"):
        try:
            gpu_ids = []
            for device_part in device.split(","):
                device_part = device_part.strip()
                if device_part.startswith("cuda:"):
                    gpu_id = device_part.split(":")[1]
                    gpu_ids.append(gpu_id)
            
            if gpu_ids:
                cuda_visible_devices = ",".join(gpu_ids)
                return cuda_visible_devices, True
                
        except (IndexError, ValueError) as e:
            logger.error(f"è§£æžè®¾å¤‡é…ç½®å¤±è´¥: {device}, é”™è¯¯: {e}")
            return "", False
    
    logger.warning(f"æœªè¯†åˆ«çš„è®¾å¤‡é…ç½®: {device}")
    return "", False


def setup_cuda_environment_for_subprocess(env: Dict[str, str], device: str) -> Dict[str, str]:
    """
    ä¸ºå­è¿›ç¨‹è®¾ç½®CUDAçŽ¯å¢ƒå˜é‡
    
    Args:
        env: çŽ¯å¢ƒå˜é‡å­—å…¸
        device: è®¾å¤‡é…ç½®å­—ç¬¦ä¸²
        
    Returns:
        Dict[str, str]: æ›´æ–°åŽçš„çŽ¯å¢ƒå˜é‡å­—å…¸
    """
    cuda_visible_devices, should_set = parse_device_to_cuda_visible_devices(device)
    
    if should_set:
        env["CUDA_VISIBLE_DEVICES"] = cuda_visible_devices
        if device == "cpu":
            logger.info("ðŸ”§ å­è¿›ç¨‹è®¾ç½®ä¸ºCPUæ¨¡å¼")
        else:
            logger.info(f"ðŸ”§ å­è¿›ç¨‹è®¾ç½®CUDA_VISIBLE_DEVICES: {cuda_visible_devices} (ä»Ž {device})")
    else:
        logger.info("ðŸ”§ å­è¿›ç¨‹ä½¿ç”¨autoæ¨¡å¼ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨é€‰æ‹©å¯ç”¨GPU")
    
    return env


def setup_training_params_env(env: Dict[str, str], training_params: Dict) -> Dict[str, str]:
    """
    è®¾ç½®è®­ç»ƒå‚æ•°çŽ¯å¢ƒå˜é‡
    
    Args:
        env: çŽ¯å¢ƒå˜é‡å­—å…¸
        training_params: è®­ç»ƒå‚æ•°å­—å…¸
        
    Returns:
        Dict[str, str]: æ›´æ–°åŽçš„çŽ¯å¢ƒå˜é‡å­—å…¸
    """
    if not training_params:
        return env
        
    # ç»Ÿä¸€çš„è®­ç»ƒå‚æ•°åˆ—è¡¨
    param_names = [
        "num_train_epochs", "per_device_train_batch_size", "per_device_eval_batch_size",
        "learning_rate", "warmup_ratio", "lr_scheduler_type", "bf16", "fp16",
        "eval_strategy", "eval_steps", "save_strategy", "save_steps", "save_total_limit",
        "logging_steps", "logging_strategy", "logging_dir", "gradient_accumulation_steps", "max_steps",
        "batch_sampler", "dataloader_num_workers", "weight_decay", "adam_beta1", "adam_beta2",
        "adam_epsilon", "max_grad_norm", "seed", "dataloader_drop_last", "eval_accumulation_steps",
        "load_best_model_at_end", "metric_for_best_model", "greater_is_better", "ignore_data_skip",
        "resume_from_checkpoint", "push_to_hub", "hub_model_id", "hub_strategy", "hub_token",
        "prediction_loss_only", "remove_unused_columns", "label_names", "local_rank", "deepspeed",
        "optim", "group_by_length", "length_column_name", "report_to", "ddpbackend", 
        "sample_size"
    ]
    
    for param in param_names:
        if param in training_params:
            env_key = param.upper()
            env_value = str(training_params[param])
            env[env_key] = env_value
            logger.debug(f"è®¾ç½®çŽ¯å¢ƒå˜é‡: {env_key} = {env_value}")
    
    return env


def log_gpu_allocation(device_request: str, allocated_device: str, task_id: str):
    """
    è®°å½•GPUåˆ†é…ä¿¡æ¯çš„ç»Ÿä¸€æ—¥å¿—æ ¼å¼
    
    Args:
        device_request: åŽŸå§‹è®¾å¤‡è¯·æ±‚
        allocated_device: åˆ†é…åˆ°çš„è®¾å¤‡
        task_id: ä»»åŠ¡ID
    """
    logger.info(f"ðŸ”§ ä»»åŠ¡ {task_id} GPUåˆ†é…: {device_request} -> {allocated_device}")
    
    if allocated_device != "cpu" and allocated_device != "auto":
        cuda_visible_devices, _ = parse_device_to_cuda_visible_devices(allocated_device)
        logger.info(f"ðŸ”§ ä»»åŠ¡ {task_id} CUDA_VISIBLE_DEVICESå°†è®¾ç½®ä¸º: {cuda_visible_devices}")