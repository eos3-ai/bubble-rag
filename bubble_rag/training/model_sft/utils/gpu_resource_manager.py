"""
GPUèµ„æºåŠ¨æ€åˆ†é…ç®¡ç†å™¨
åŸºäºç°æœ‰çš„deviceå‚æ•°æ¶æ„ï¼Œå®ç°å¤šä»»åŠ¡GPUåŠ¨æ€åˆ†é…

æ”¯æŒç”¨æˆ·æƒé™æ§åˆ¶ï¼š
- æŠ€æœ¯éš”ç¦»ï¼šç»§ç»­ä½¿ç”¨æœåŠ¡å®ä¾‹IDè¿›è¡Œå­¤å„¿è¿›ç¨‹æ£€æµ‹
- ä¸šåŠ¡éš”ç¦»ï¼šå¢åŠ ç”¨æˆ·æƒé™æ§åˆ¶ï¼Œç®¡ç†å‘˜å¯å…¨å±€ç®¡ç†GPUèµ„æº
"""

import os
import threading
import logging
import time
from typing import Dict, List, Optional, Set
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


class GPUResourceManager:
    """GPUèµ„æºåŠ¨æ€åˆ†é…ç®¡ç†å™¨"""
    
    def __init__(self):
        # æ ¸å¿ƒåˆ†é…æ•°æ®ï¼ˆä¿æŒåŸæœ‰ç»“æ„ï¼‰
        self.gpu_allocations: Dict[int, str] = {}  # {gpu_id: task_id}
        self.task_gpus: Dict[str, Set[int]] = {}   # {task_id: {gpu_ids}}
        self.allocation_times: Dict[str, datetime] = {}  # {task_id: allocation_time}

        # æ–°å¢ï¼šç”¨æˆ·ä¿¡æ¯è¿½è¸ª
        self.task_users: Dict[str, str] = {}  # {task_id: username}
        self.task_user_roles: Dict[str, str] = {}  # {task_id: user_role}

        self._lock = threading.RLock()
        self.max_gpus = self._detect_max_gpus()

        logger.info(f"GPUèµ„æºç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ£€æµ‹åˆ° {self.max_gpus} ä¸ªGPU")

        # å»¶è¿ŸGPUæ¸…ç†ï¼Œé¿å…å¾ªç¯å¯¼å…¥é—®é¢˜
        self._schedule_delayed_cleanup()
    
    def _detect_max_gpus(self) -> int:
        """æ£€æµ‹ç³»ç»ŸGPUæ•°é‡"""
        try:
            # æ–¹æ³•1: ä½¿ç”¨nvidia-ml-py
            import pynvml
            pynvml.nvmlInit()
            gpu_count = pynvml.nvmlDeviceGetCount()
            return gpu_count
        except:
            try:
                # æ–¹æ³•2: ä½¿ç”¨torch
                import torch
                if torch.cuda.is_available():
                    return torch.cuda.device_count()
            except:
                pass
        
        # æ–¹æ³•3: é»˜è®¤å‡è®¾4ä¸ªGPUï¼ˆå¯é…ç½®ï¼‰
        logger.warning("æ— æ³•æ£€æµ‹GPUæ•°é‡ï¼Œä½¿ç”¨é»˜è®¤å€¼4")
        return int(os.environ.get('MAX_GPUS', '4'))
    
    def allocate_gpus_for_task(self, task_id: str, device_request: str = "auto", username: str = None, user_role: str = "user") -> Optional[str]:
        """
        ä¸ºä»»åŠ¡åˆ†é…GPUèµ„æº

        Args:
            task_id: ä»»åŠ¡ID
            device_request: è®¾å¤‡è¯·æ±‚ï¼Œæ”¯æŒ:
                - "auto": è‡ªåŠ¨åˆ†é…1ä¸ªGPU
                - "cpu": ä½¿ç”¨CPU
                - "cuda:0": æŒ‡å®šGPU 0
                - "cuda:0,cuda:1": æŒ‡å®šå¤šä¸ªGPU
                - "auto:2": è‡ªåŠ¨åˆ†é…2ä¸ªGPU
            username: ç”¨æˆ·åï¼ˆç”¨äºæƒé™æ§åˆ¶å’Œèµ„æºè¿½è¸ªï¼‰
            user_role: ç”¨æˆ·è§’è‰²ï¼ˆadmin/userï¼‰

        Returns:
            è®¾å¤‡å­—ç¬¦ä¸²ï¼Œå¦‚ "cuda:0,cuda:1" æˆ– "cpu" æˆ– Noneï¼ˆåˆ†é…å¤±è´¥ï¼‰
        """
        with self._lock:
            try:
                # CPUæ¨¡å¼
                if device_request == "cpu":
                    # è®°å½•ç”¨æˆ·ä¿¡æ¯ï¼ˆå³ä½¿æ˜¯CPUæ¨¡å¼ä¹Ÿè¦è®°å½•ï¼‰
                    if username:
                        self.task_users[task_id] = username
                        self.task_user_roles[task_id] = user_role
                    logger.info(f"ä»»åŠ¡ {task_id} (ç”¨æˆ·: {username}) ä½¿ç”¨CPUæ¨¡å¼")
                    return "cpu"
                
                # è§£æè®¾å¤‡è¯·æ±‚
                requested_gpus, num_gpus = self._parse_device_request(device_request)
                
                if requested_gpus is not None:
                    # æŒ‡å®šGPUæ¨¡å¼
                    if self._can_allocate_specific_gpus(requested_gpus):
                        return self._do_allocate_gpus(task_id, requested_gpus, username, user_role)
                    else:
                        logger.error(f"æŒ‡å®šçš„GPU {requested_gpus} ä¸å¯ç”¨")
                        return None
                else:
                    # è‡ªåŠ¨åˆ†é…æ¨¡å¼
                    available_gpus = self._get_available_gpus(num_gpus)
                    if available_gpus:
                        return self._do_allocate_gpus(task_id, available_gpus, username, user_role)
                    else:
                        logger.error(f"æ— æ³•è‡ªåŠ¨åˆ†é… {num_gpus} ä¸ªGPUï¼Œå½“å‰å¯ç”¨GPUä¸è¶³")
                        return None
                        
            except Exception as e:
                logger.error(f"GPUåˆ†é…å¤±è´¥: {e}")
                return None
    
    def _parse_device_request(self, device_request: str) -> tuple[Optional[List[int]], int]:
        """
        è§£æè®¾å¤‡è¯·æ±‚
        
        Returns:
            (æŒ‡å®šçš„GPUåˆ—è¡¨æˆ–None, è¯·æ±‚çš„GPUæ•°é‡)
        """
        if device_request == "auto":
            return None, 1
        
        if device_request.startswith("auto:"):
            try:
                num_gpus = int(device_request.split(":")[1])
                return None, num_gpus
            except (IndexError, ValueError):
                logger.warning(f"æ— æ³•è§£æautoè¯·æ±‚: {device_request}ï¼Œä½¿ç”¨é»˜è®¤1ä¸ªGPU")
                return None, 1
        
        if device_request.startswith("cuda:"):
            try:
                gpu_ids = []
                for part in device_request.split(","):
                    part = part.strip()
                    if part.startswith("cuda:"):
                        gpu_id = int(part.split(":")[1])
                        if 0 <= gpu_id < self.max_gpus:
                            gpu_ids.append(gpu_id)
                        else:
                            logger.warning(f"GPU ID {gpu_id} è¶…å‡ºèŒƒå›´ [0, {self.max_gpus-1}]")
                
                return gpu_ids if gpu_ids else None, len(gpu_ids)
                
            except (IndexError, ValueError) as e:
                logger.warning(f"æ— æ³•è§£æCUDAè¯·æ±‚: {device_request}, é”™è¯¯: {e}")
                return None, 1
        
        logger.warning(f"æœªè¯†åˆ«çš„è®¾å¤‡è¯·æ±‚: {device_request}ï¼Œä½¿ç”¨autoæ¨¡å¼")
        return None, 1
    
    def _can_allocate_specific_gpus(self, gpu_ids: List[int]) -> bool:
        """æ£€æŸ¥æŒ‡å®šçš„GPUæ˜¯å¦å¯ä»¥åˆ†é…"""
        # é¦–å…ˆæ¸…ç†å¤±è´¥ä»»åŠ¡çš„GPUåˆ†é…
        self._cleanup_failed_task_allocations()

        for gpu_id in gpu_ids:
            if gpu_id >= self.max_gpus:
                logger.warning(f"GPU {gpu_id} è¶…å‡ºç³»ç»ŸGPUæ•°é‡ ({self.max_gpus})")
                return False
            if gpu_id in self.gpu_allocations:
                allocated_task = self.gpu_allocations[gpu_id]
                logger.warning(f"GPU {gpu_id} å·²è¢«ä»»åŠ¡ {allocated_task} å ç”¨")
                return False
        return True
    
    def _get_available_gpus(self, num_needed: int) -> Optional[List[int]]:
        """è·å–å¯ç”¨çš„GPU"""
        # é¦–å…ˆæ¸…ç†å¤±è´¥ä»»åŠ¡çš„åˆ†é…
        self._cleanup_failed_task_allocations()

        available = []
        for gpu_id in range(self.max_gpus):
            if gpu_id not in self.gpu_allocations:
                available.append(gpu_id)
                if len(available) >= num_needed:
                    break

        logger.info(f"è¯·æ±‚ {num_needed} ä¸ªGPUï¼Œæ‰¾åˆ° {len(available)} ä¸ªå¯ç”¨GPU: {available}")
        return available[:num_needed] if len(available) >= num_needed else None
    
    def _do_allocate_gpus(self, task_id: str, gpu_ids: List[int], username: str = None, user_role: str = "user") -> str:
        """æ‰§è¡ŒGPUåˆ†é…"""
        # åˆ†é…GPU
        for gpu_id in gpu_ids:
            self.gpu_allocations[gpu_id] = task_id

        # è®°å½•ä»»åŠ¡GPUæ˜ å°„
        self.task_gpus[task_id] = set(gpu_ids)
        self.allocation_times[task_id] = datetime.now()

        # è®°å½•ç”¨æˆ·ä¿¡æ¯
        if username:
            self.task_users[task_id] = username
            self.task_user_roles[task_id] = user_role

        # ç”ŸæˆCUDAè®¾å¤‡å­—ç¬¦ä¸²
        if len(gpu_ids) == 1:
            device_str = f"cuda:{gpu_ids[0]}"
        else:
            device_str = ",".join([f"cuda:{gpu_id}" for gpu_id in gpu_ids])

        user_info = f" (ç”¨æˆ·: {username}, è§’è‰²: {user_role})" if username else ""
        logger.info(f"ä¸ºä»»åŠ¡ {task_id}{user_info} åˆ†é…GPU: {device_str} (ç‰©ç†GPU: {gpu_ids})")
        return device_str
    
    def release_gpus_for_task(self, task_id: str) -> bool:
        """é‡Šæ”¾ä»»åŠ¡çš„GPUèµ„æº"""
        with self._lock:
            try:
                if task_id not in self.task_gpus:
                    logger.debug(f"ä»»åŠ¡ {task_id} æ²¡æœ‰åˆ†é…GPUèµ„æº")
                    return True

                # è·å–ä»»åŠ¡çš„GPU
                gpu_ids = self.task_gpus[task_id]

                # é‡Šæ”¾GPU
                for gpu_id in gpu_ids:
                    if gpu_id in self.gpu_allocations:
                        del self.gpu_allocations[gpu_id]

                # æ¸…ç†è®°å½•
                del self.task_gpus[task_id]
                if task_id in self.allocation_times:
                    del self.allocation_times[task_id]

                # æ¸…ç†ç”¨æˆ·ä¿¡æ¯
                username = self.task_users.pop(task_id, None)
                user_role = self.task_user_roles.pop(task_id, None)

                user_info = f" (ç”¨æˆ·: {username})" if username else ""
                logger.info(f"ğŸ”“ é‡Šæ”¾ä»»åŠ¡ {task_id}{user_info} çš„GPUèµ„æº: {list(gpu_ids)}")
                return True

            except Exception as e:
                logger.error(f"é‡Šæ”¾GPUèµ„æºå¤±è´¥: {e}")
                # å¼ºåˆ¶æ¸…ç†ï¼šå³ä½¿å‡ºé”™ä¹Ÿè¦å°è¯•æ¸…ç†è®°å½•ï¼Œé¿å…æ°¸ä¹…æ³„æ¼
                self._force_cleanup_task_records(task_id)
                return False

    def _force_cleanup_task_records(self, task_id: str):
        """å¼ºåˆ¶æ¸…ç†ä»»åŠ¡è®°å½•ï¼Œé¿å…æ°¸ä¹…èµ„æºæ³„æ¼"""
        try:
            logger.warning(f"å¼ºåˆ¶æ¸…ç†ä»»åŠ¡ {task_id} çš„GPUè®°å½•")

            # å¼ºåˆ¶åˆ é™¤ä»»åŠ¡GPUæ˜ å°„
            if task_id in self.task_gpus:
                gpu_ids = self.task_gpus[task_id]
                logger.info(f"å¼ºåˆ¶é‡Šæ”¾GPU: {list(gpu_ids)}")

                # å¼ºåˆ¶æ¸…ç†GPUåˆ†é…è®°å½•
                for gpu_id in gpu_ids:
                    if gpu_id in self.gpu_allocations:
                        del self.gpu_allocations[gpu_id]
                        logger.info(f"å¼ºåˆ¶æ¸…ç†GPU {gpu_id} åˆ†é…è®°å½•")

                # å¼ºåˆ¶æ¸…ç†ä»»åŠ¡è®°å½•
                del self.task_gpus[task_id]

            # å¼ºåˆ¶æ¸…ç†åˆ†é…æ—¶é—´è®°å½•
            if task_id in self.allocation_times:
                del self.allocation_times[task_id]

            # å¼ºåˆ¶æ¸…ç†ç”¨æˆ·ä¿¡æ¯è®°å½•
            username = self.task_users.pop(task_id, None)
            user_role = self.task_user_roles.pop(task_id, None)
            user_info = f" (ç”¨æˆ·: {username})" if username else ""

            logger.info(f"å¼ºåˆ¶æ¸…ç†ä»»åŠ¡ {task_id}{user_info} GPUè®°å½•å®Œæˆ")

        except Exception as force_error:
            logger.critical(f"å¼ºåˆ¶æ¸…ç†ä¹Ÿå¤±è´¥ï¼ä»»åŠ¡ {task_id} çš„GPUèµ„æºå¯èƒ½æ°¸ä¹…æ³„æ¼: {force_error}")

    def force_release_gpu_for_task(self, task_id: str) -> bool:
        """å¼ºåˆ¶é‡Šæ”¾æŒ‡å®šä»»åŠ¡çš„GPUèµ„æºï¼ˆç”¨äºå¼‚å¸¸æƒ…å†µä¸‹çš„èµ„æºæ¢å¤ï¼‰"""
        with self._lock:
            logger.warning(f"ğŸš¨ å¼ºåˆ¶é‡Šæ”¾ä»»åŠ¡ {task_id} çš„GPUèµ„æº")

            try:
                # æ— æ¡ä»¶æ¸…ç†æ‰€æœ‰ç›¸å…³è®°å½•
                gpu_ids = []
                if task_id in self.task_gpus:
                    gpu_ids = list(self.task_gpus[task_id])

                # å¼ºåˆ¶æ¸…ç†GPUåˆ†é…
                for gpu_id in list(self.gpu_allocations.keys()):
                    if self.gpu_allocations[gpu_id] == task_id:
                        del self.gpu_allocations[gpu_id]
                        logger.info(f"å¼ºåˆ¶é‡Šæ”¾GPU {gpu_id}")

                # å¼ºåˆ¶æ¸…ç†ä»»åŠ¡è®°å½•
                if task_id in self.task_gpus:
                    del self.task_gpus[task_id]
                if task_id in self.allocation_times:
                    del self.allocation_times[task_id]

                # å¼ºåˆ¶æ¸…ç†ç”¨æˆ·ä¿¡æ¯
                username = self.task_users.pop(task_id, None)
                user_role = self.task_user_roles.pop(task_id, None)
                user_info = f" (ç”¨æˆ·: {username})" if username else ""

                logger.info(f"å¼ºåˆ¶é‡Šæ”¾ä»»åŠ¡ {task_id}{user_info} GPUèµ„æºå®Œæˆ: {gpu_ids}")
                return True

            except Exception as e:
                logger.critical(f"å¼ºåˆ¶é‡Šæ”¾ä¹Ÿå¤±è´¥ï¼ç³»ç»Ÿå¯èƒ½éœ€è¦é‡å¯æ¥æ¢å¤GPUèµ„æº: {e}")
                return False

    def _cleanup_failed_task_allocations(self):
        """æ¸…ç†å¤±è´¥ä»»åŠ¡çš„GPUåˆ†é…"""
        failed_tasks = []

        # æ£€æŸ¥æ‰€æœ‰å·²åˆ†é…GPUçš„ä»»åŠ¡çŠ¶æ€
        for task_id in list(self.task_gpus.keys()):
            if self._is_task_failed_or_stopped(task_id):
                failed_tasks.append(task_id)

        # é‡Šæ”¾å¤±è´¥ä»»åŠ¡çš„GPU
        for task_id in failed_tasks:
            logger.info(f"æ£€æµ‹åˆ°å®Œæˆ/å¤±è´¥ä»»åŠ¡ï¼Œæ¸…ç†GPUåˆ†é…: {task_id}")
            self.release_gpus_for_task(task_id)

            # é¢å¤–çš„GPUå†…å­˜æ¸…ç†
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.ipc_collect()
                    logger.debug(f"å·²æ¸…ç†ä»»åŠ¡ {task_id} çš„GPUå†…å­˜")
            except:
                pass

    def _is_task_failed_or_stopped(self, task_id: str) -> bool:
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å¤±è´¥æˆ–åœæ­¢"""
        try:
            from bubble_rag.training.mysql_service.service.training_task_service import training_task_service
            task = training_task_service.get_training_task(task_id)
            if task and task.status in ["FAILED", "STOPPED", "SUCCEEDED"]:
                return True
        except Exception as e:
            logger.debug(f"æ£€æŸ¥ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return False

    def force_cleanup_task(self, task_id: str) -> bool:
        """å¼ºåˆ¶æ¸…ç†æŒ‡å®šä»»åŠ¡çš„GPUèµ„æº"""
        logger.info(f"ğŸ”¨ å¼ºåˆ¶æ¸…ç†ä»»åŠ¡ {task_id} çš„GPUèµ„æº")
        return self.release_gpus_for_task(task_id)

    def _get_gpu_memory_info(self, gpu_id: int) -> Dict:
        """è·å–GPUæ˜¾å­˜ä¿¡æ¯"""
        try:
            try:
                import pynvml
            except ImportError:
                # å¦‚æœpynvmlä¸å¯ç”¨ï¼Œè¿”å›ç©ºä¿¡æ¯
                raise Exception("pynvml not available")

            pynvml.nvmlInit()
            handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_id)
            
            # è·å–æ˜¾å­˜ä¿¡æ¯
            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            total_memory = mem_info.total
            used_memory = mem_info.used
            free_memory = mem_info.free
            
            # è·å–GPUåˆ©ç”¨ç‡
            try:
                utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_util = utilization.gpu
                memory_util = utilization.memory
            except:
                gpu_util = None
                memory_util = None
            
            # è·å–GPUåç§°å’Œæ¸©åº¦
            try:
                gpu_name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')
            except:
                gpu_name = f"GPU {gpu_id}"
            
            try:
                temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
            except:
                temperature = None
            
            return {
                "gpu_name": gpu_name,
                "memory": {
                    "total": total_memory,
                    "used": used_memory,
                    "free": free_memory,
                    "total_gb": round(total_memory / 1024**3, 2),
                    "used_gb": round(used_memory / 1024**3, 2),
                    "free_gb": round(free_memory / 1024**3, 2),
                    "usage_percent": round((used_memory / total_memory) * 100, 1) if total_memory > 0 else 0
                },
                "utilization": {
                    "gpu_percent": gpu_util,
                    "memory_percent": memory_util
                },
                "temperature": temperature
            }
        except Exception as e:
            logger.debug(f"è·å–GPU {gpu_id} è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "gpu_name": f"GPU {gpu_id}",
                "memory": {
                    "total": None,
                    "used": None,
                    "free": None,
                    "total_gb": None,
                    "used_gb": None,
                    "free_gb": None,
                    "usage_percent": None
                },
                "utilization": {
                    "gpu_percent": None,
                    "memory_percent": None
                },
                "temperature": None,
                "error": str(e)
            }

    def get_resource_status(self) -> Dict:
        """è·å–èµ„æºçŠ¶æ€æŠ¥å‘Šï¼ˆåŒ…å«æ˜¾å­˜ä¿¡æ¯ï¼‰"""
        with self._lock:
            # ä¸»åŠ¨æ¸…ç†å·²ç»“æŸçš„ä»»åŠ¡ï¼Œç¡®ä¿GPUçŠ¶æ€å®æ—¶å‡†ç¡®
            self._cleanup_failed_task_allocations()

            total_gpus = self.max_gpus
            # æ³¨æ„ï¼šåˆ†é…çš„GPUæ•°é‡ä¼šåœ¨åé¢çš„å¾ªç¯ä¸­åŠ¨æ€è®¡ç®—ï¼Œå› ä¸ºå¯èƒ½æœ‰å®æ—¶æ¸…ç†
            
            # æ„å»ºè¯¦ç»†çŠ¶æ€
            gpu_details = {}
            total_memory_gb = 0
            used_memory_gb = 0
            
            for gpu_id in range(total_gpus):
                # è·å–GPUç¡¬ä»¶ä¿¡æ¯
                gpu_hw_info = self._get_gpu_memory_info(gpu_id)
                
                # åŸºç¡€åˆ†é…çŠ¶æ€
                if gpu_id in self.gpu_allocations:
                    task_id = self.gpu_allocations[gpu_id]
                    alloc_time = self.allocation_times.get(task_id)
                    username = self.task_users.get(task_id)
                    user_role = self.task_user_roles.get(task_id)

                    # å®æ—¶æ£€æŸ¥ä»»åŠ¡çŠ¶æ€ï¼Œå¦‚æœä»»åŠ¡å·²ç»“æŸåˆ™ç«‹å³æ¸…ç†GPU
                    should_release = False
                    task_status = "UNKNOWN"

                    try:
                        # æ£€æŸ¥æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€
                        if self._is_task_failed_or_stopped(task_id):
                            logger.info(f"å®æ—¶æ£€æµ‹åˆ°ä»»åŠ¡ {task_id} å·²ç»“æŸï¼Œç«‹å³é‡Šæ”¾GPU {gpu_id}")
                            should_release = True
                            task_status = "FINISHED"
                        else:
                            # ä»»åŠ¡ä»åœ¨è¿è¡Œï¼Œè·å–æœ€æ–°çŠ¶æ€
                            from bubble_rag.training.mysql_service.service.training_task_service import training_task_service
                            task_db = training_task_service.get_training_task(task_id)
                            task_status = task_db.status if task_db else "NOT_FOUND"
                    except Exception as e:
                        logger.warning(f"æ£€æŸ¥ä»»åŠ¡ {task_id} çŠ¶æ€å¤±è´¥: {e}")
                        should_release = False

                    if should_release:
                        # ç«‹å³é‡Šæ”¾GPUå¹¶è·³è¿‡è¿™ä¸ªGPUçš„çŠ¶æ€æ„å»º
                        try:
                            self.release_gpus_for_task(task_id)
                            logger.info(f"å®æ—¶æ¸…ç†å®Œæˆ: ä»»åŠ¡ {task_id} GPU {gpu_id}")
                            # é‡æ–°æ„å»ºä¸ºfreeçŠ¶æ€
                            gpu_status = {
                                "status": "free",
                                "task_id": None,
                                "username": None,
                                "user_role": None,
                                "allocated_at": None,
                                "duration": None
                            }
                        except Exception as release_error:
                            logger.error(f"å®æ—¶é‡Šæ”¾GPUå¤±è´¥: {release_error}")
                            # å¦‚æœé‡Šæ”¾å¤±è´¥ï¼Œä»æ˜¾ç¤ºä¸ºå·²åˆ†é…ä½†æ ‡è®°ä¸ºå¼‚å¸¸
                            gpu_status = {
                                "status": "allocated",
                                "task_id": task_id,
                                "username": username,
                                "user_role": user_role,
                                "allocated_at": alloc_time.isoformat() if alloc_time else None,
                                "duration": str(datetime.now() - alloc_time) if alloc_time else None,
                                "task_status": task_status,
                                "warning": "ä»»åŠ¡å·²ç»“æŸä½†GPUé‡Šæ”¾å¤±è´¥"
                            }
                    else:
                        # ä»»åŠ¡ä»åœ¨è¿è¡Œï¼Œæ­£å¸¸æ˜¾ç¤ºåˆ†é…çŠ¶æ€
                        gpu_status = {
                            "status": "allocated",
                            "task_id": task_id,
                            "username": username,
                            "user_role": user_role,
                            "allocated_at": alloc_time.isoformat() if alloc_time else None,
                            "duration": str(datetime.now() - alloc_time) if alloc_time else None,
                            "task_status": task_status
                        }
                else:
                    gpu_status = {
                        "status": "free",
                        "task_id": None,
                        "username": None,
                        "user_role": None,
                        "allocated_at": None,
                        "duration": None
                    }
                
                # åˆå¹¶ç¡¬ä»¶ä¿¡æ¯
                gpu_details[gpu_id] = {**gpu_status, **gpu_hw_info}
                
                # ç´¯è®¡å†…å­˜ç»Ÿè®¡
                if gpu_hw_info["memory"]["total_gb"]:
                    total_memory_gb += gpu_hw_info["memory"]["total_gb"]
                if gpu_hw_info["memory"]["used_gb"]:
                    used_memory_gb += gpu_hw_info["memory"]["used_gb"]

            # é‡æ–°è®¡ç®—åˆ†é…ç»Ÿè®¡ï¼ˆå®æ—¶æ¸…ç†åå¯èƒ½å·²å˜åŒ–ï¼‰
            allocated_gpus = len(self.gpu_allocations)
            free_gpus = total_gpus - allocated_gpus

            return {
                "total_gpus": total_gpus,
                "allocated_gpus": allocated_gpus,
                "free_gpus": free_gpus,
                "utilization_rate": allocated_gpus / total_gpus if total_gpus > 0 else 0,
                "memory_summary": {
                    "total_gb": round(total_memory_gb, 2),
                    "used_gb": round(used_memory_gb, 2),
                    "free_gb": round(total_memory_gb - used_memory_gb, 2),
                    "usage_percent": round((used_memory_gb / total_memory_gb) * 100, 1) if total_memory_gb > 0 else 0
                },
                "gpu_details": gpu_details,
                "active_tasks": len(self.task_gpus)
            }

    def get_resource_status_for_user(self, username: str = None, user_role: str = "user") -> Dict:
        """
        è·å–ç”¨æˆ·å¯è§çš„èµ„æºçŠ¶æ€æŠ¥å‘Š

        Args:
            username: ç”¨æˆ·åï¼ŒNoneè¡¨ç¤ºè·å–å½“å‰ç”¨æˆ·
            user_role: ç”¨æˆ·è§’è‰²ï¼Œadminå¯ä»¥çœ‹åˆ°æ‰€æœ‰èµ„æº

        Returns:
            Dict: è¿‡æ»¤åçš„èµ„æºçŠ¶æ€
        """
        # è·å–å®Œæ•´çš„èµ„æºçŠ¶æ€
        full_status = self.get_resource_status()

        # ç®¡ç†å‘˜å¯ä»¥çœ‹åˆ°æ‰€æœ‰ä¿¡æ¯
        if user_role == "admin":
            return full_status

        # æ™®é€šç”¨æˆ·åªèƒ½çœ‹åˆ°è‡ªå·±çš„ä»»åŠ¡ä¿¡æ¯
        if username:
            filtered_gpu_details = {}
            user_allocated_gpus = 0

            for gpu_id, gpu_info in full_status["gpu_details"].items():
                if gpu_info["status"] == "allocated":
                    if gpu_info["username"] == username:
                        # æ˜¾ç¤ºè‡ªå·±çš„GPUåˆ†é…
                        filtered_gpu_details[gpu_id] = gpu_info
                        user_allocated_gpus += 1
                    else:
                        # éšè—å…¶ä»–ç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯
                        filtered_gpu_details[gpu_id] = {
                            "status": "allocated",
                            "task_id": "***",
                            "username": "***",
                            "user_role": "***",
                            "allocated_at": "***",
                            "duration": "***",
                            "gpu_name": gpu_info.get("gpu_name", f"GPU {gpu_id}"),
                            "memory": gpu_info.get("memory", {}),
                            "utilization": gpu_info.get("utilization", {}),
                            "temperature": gpu_info.get("temperature")
                        }
                else:
                    # æ˜¾ç¤ºç©ºé—²çš„GPU
                    filtered_gpu_details[gpu_id] = gpu_info

            # è¿”å›è¿‡æ»¤åçš„çŠ¶æ€
            return {
                "total_gpus": full_status["total_gpus"],
                "allocated_gpus": full_status["allocated_gpus"],
                "free_gpus": full_status["free_gpus"],
                "user_allocated_gpus": user_allocated_gpus,  # æ–°å¢ï¼šç”¨æˆ·åˆ†é…çš„GPUæ•°é‡
                "utilization_rate": full_status["utilization_rate"],
                "memory_summary": full_status["memory_summary"],
                "gpu_details": filtered_gpu_details,
                "active_tasks": full_status["active_tasks"],
                "user_tasks": len([tid for tid, uid in self.task_users.items() if uid == username])  # æ–°å¢ï¼šç”¨æˆ·ä»»åŠ¡æ•°é‡
            }

        # å¦‚æœæ²¡æœ‰æä¾›ç”¨æˆ·IDï¼Œè¿”å›åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
        return {
            "total_gpus": full_status["total_gpus"],
            "allocated_gpus": full_status["allocated_gpus"],
            "free_gpus": full_status["free_gpus"],
            "utilization_rate": full_status["utilization_rate"],
            "memory_summary": full_status["memory_summary"]
        }
    
    def cleanup_stale_allocations(self, max_age_hours: int = 24):
        """æ¸…ç†é•¿æ—¶é—´æœªé‡Šæ”¾çš„åˆ†é…ï¼ˆé˜²æ­¢èµ„æºæ³„æ¼ï¼‰"""
        with self._lock:
            current_time = datetime.now()
            stale_tasks = []
            
            for task_id, alloc_time in self.allocation_times.items():
                if current_time - alloc_time > timedelta(hours=max_age_hours):
                    stale_tasks.append(task_id)
            
            for task_id in stale_tasks:
                logger.warning(f"æ¸…ç†è¿‡æœŸçš„GPUåˆ†é…: ä»»åŠ¡ {task_id}")
                self.release_gpus_for_task(task_id)
            
            return len(stale_tasks)

    def _cleanup_completed_tasks_on_startup(self):
        """
        å¯åŠ¨æ—¶æ¢å¤å…¨å±€GPUåˆ†é…çŠ¶æ€
        ä¿®å¤æœåŠ¡é‡å¯åGPUèµ„æºç®¡ç†ä¸ä¸€è‡´çš„é—®é¢˜

        å…¨å±€æ¢å¤ç­–ç•¥ï¼š
        1. ä»æ•°æ®åº“æ¢å¤æ‰€æœ‰æœåŠ¡å®ä¾‹çš„GPUä½¿ç”¨æƒ…å†µ
        2. æ¸…ç†å·²å®Œæˆä»»åŠ¡çš„GPUèµ„æºè®°å½•
        3. é‡å»ºæ‰€æœ‰æ­£åœ¨è¿è¡Œä»»åŠ¡çš„GPUåˆ†é…è®°å½•
        4. ç¡®ä¿GPUèµ„æºçš„å…¨å±€ä¸€è‡´æ€§
        """
        try:
            logger.info("å¯åŠ¨æ—¶æ£€æŸ¥æ•°æ®åº“ä¸­çš„GPUåˆ†é…çŠ¶æ€...")

            from bubble_rag.training.mysql_service.service.training_task_service import training_task_service

            # æ¸…ç©ºå†…å­˜ä¸­çš„åˆ†é…è®°å½•
            self.gpu_allocations.clear()
            self.task_gpus.clear()
            self.allocation_times.clear()
            # æ¸…ç©ºç”¨æˆ·ä¿¡æ¯è®°å½•
            self.task_users.clear()
            self.task_user_roles.clear()

            # å…¨å±€GPUç®¡ç†ï¼šæ¢å¤æ‰€æœ‰æœåŠ¡å®ä¾‹çš„GPUåˆ†é…çŠ¶æ€
            # é¿å…æœåŠ¡é‡å¯æ—¶æ¸…ç©ºå…¶ä»–æœåŠ¡çš„GPUåˆ†é…è®°å½•
            try:
                all_status_tasks = training_task_service.get_all_training_tasks(limit=1000)
                logger.info(f"å…¨å±€GPUæ¢å¤ï¼šæ£€æŸ¥æ‰€æœ‰æœåŠ¡çš„ä»»åŠ¡ï¼Œå…± {len(all_status_tasks)} ä¸ªä»»åŠ¡")
            except Exception as e:
                logger.warning(f"è·å–ä»»åŠ¡å¤±è´¥: {e}")
                # å‘ç”Ÿé”™è¯¯æ—¶é‡‡ç”¨å®‰å…¨ç­–ç•¥ï¼šæ¸…ç©ºæ‰€æœ‰GPUåˆ†é…
                all_status_tasks = []

            completed_statuses = ["SUCCEEDED", "FAILED", "STOPPED"]
            running_statuses = ["RUNNING", "PENDING"]
            cleaned_count = 0
            restored_count = 0

            # ç­›é€‰ä½¿ç”¨GPUçš„ä»»åŠ¡
            tasks_with_device = [task for task in all_status_tasks if task.device and task.device != "cpu"]
            logger.info(f"å‘ç° {len(tasks_with_device)} ä¸ªä½¿ç”¨GPUçš„ä»»åŠ¡")

            for task in tasks_with_device:
                task_id = task.task_id
                device = task.device
                status = task.status

                try:
                    if status in completed_statuses:
                        # å·²å®Œæˆçš„ä»»åŠ¡ï¼šç¡®ä¿GPUèµ„æºå·²æ¸…ç†
                        logger.info(f"å·²å®Œæˆä»»åŠ¡ {task_id} (çŠ¶æ€: {status}), ç¡®ä¿GPUèµ„æºæ¸…ç†")
                        # è¿™é‡Œä¸éœ€è¦è°ƒç”¨releaseï¼Œå› ä¸ºå†…å­˜ä¸­æœ¬æ¥å°±æ˜¯ç©ºçš„
                        cleaned_count += 1

                    elif status in running_statuses:
                        # æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ï¼šæ£€æŸ¥è¿›ç¨‹çŠ¶æ€åå†³å®šæ˜¯å¦é‡å»ºGPUåˆ†é…è®°å½•
                        logger.info(f"æ£€æŸ¥è¿è¡Œä¸­ä»»åŠ¡ {task_id} (çŠ¶æ€: {status}) çš„å®é™…è¿›ç¨‹çŠ¶æ€")

                        # å…³é”®æ”¹è¿›ï¼šæ£€æŸ¥å®é™…è¿›ç¨‹çŠ¶æ€
                        should_restore_gpu = False
                        process_check_result = "UNKNOWN"

                        try:
                            # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦çœŸçš„è¿˜åœ¨è¿è¡Œ
                            if hasattr(task, 'process_pid') and task.process_pid:
                                import psutil
                                if psutil.pid_exists(task.process_pid):
                                    try:
                                        process = psutil.Process(task.process_pid)
                                        if process.is_running() and 'python' in process.name().lower():
                                            should_restore_gpu = True
                                            process_check_result = "RUNNING"
                                            logger.info(f"ä»»åŠ¡ {task_id} è¿›ç¨‹ {task.process_pid} ç¡®å®åœ¨è¿è¡Œï¼Œæ¢å¤GPUåˆ†é…")
                                        else:
                                            process_check_result = "NOT_PYTHON"
                                            logger.warning(f"ä»»åŠ¡ {task_id} PID {task.process_pid} å­˜åœ¨ä½†ä¸æ˜¯Pythonè¿›ç¨‹: {process.name()}")
                                    except psutil.NoSuchProcess:
                                        process_check_result = "PROCESS_DEAD"
                                        logger.warning(f"ä»»åŠ¡ {task_id} è¿›ç¨‹ {task.process_pid} å·²æ­»äº¡")
                                else:
                                    process_check_result = "PID_NOT_EXISTS"
                                    logger.warning(f"ä»»åŠ¡ {task_id} PID {task.process_pid} ä¸å­˜åœ¨")
                            else:
                                process_check_result = "NO_PID"
                                logger.warning(f"ä»»åŠ¡ {task_id} æ²¡æœ‰PIDè®°å½•ï¼Œå¯èƒ½æ˜¯æ—§ä»»åŠ¡")

                        except Exception as process_error:
                            logger.error(f"æ£€æŸ¥ä»»åŠ¡ {task_id} è¿›ç¨‹çŠ¶æ€å¤±è´¥: {process_error}")
                            process_check_result = "CHECK_FAILED"

                        if should_restore_gpu:
                            # è¿›ç¨‹ç¡®å®åœ¨è¿è¡Œï¼Œæ¢å¤GPUåˆ†é…
                            gpu_ids = self._parse_device_to_gpu_ids(device)
                            if gpu_ids:
                                # é‡å»ºåˆ†é…è®°å½•
                                for gpu_id in gpu_ids:
                                    self.gpu_allocations[gpu_id] = task_id
                                self.task_gpus[task_id] = set(gpu_ids)
                                self.allocation_times[task_id] = datetime.now()

                                # æ¢å¤ç”¨æˆ·ä¿¡æ¯
                                if hasattr(task, 'username') and task.username:
                                    self.task_users[task_id] = task.username
                                    # ä»ç”¨æˆ·ä¿¡æ¯æ¨æ–­ç”¨æˆ·è§’è‰²ï¼Œæˆ–ä½¿ç”¨é»˜è®¤å€¼
                                    self.task_user_roles[task_id] = "user"  # é»˜è®¤ä¸ºæ™®é€šç”¨æˆ·

                                restored_count += 1
                                username_info = f" (ç”¨æˆ·: {task.username})" if hasattr(task, 'username') and task.username else ""
                                logger.info(f"å·²é‡å»ºä»»åŠ¡ {task_id} çš„GPUåˆ†é…: {gpu_ids}{username_info}")
                        else:
                            # è¿›ç¨‹ä¸åœ¨è¿è¡Œï¼Œæ ‡è®°ä»»åŠ¡ä¸ºå¤±è´¥å¹¶æ¸…ç†
                            logger.warning(f"ä»»åŠ¡ {task_id} æ•°æ®åº“çŠ¶æ€ä¸º{status}ä½†è¿›ç¨‹ä¸å­˜åœ¨({process_check_result})ï¼Œæ ‡è®°ä¸ºå¤±è´¥")
                            try:
                                # æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€
                                training_task_service.update_task_status(task_id, "FAILED")
                                cleaned_count += 1
                                logger.info(f"å·²å°†å­¤å„¿ä»»åŠ¡ {task_id} æ ‡è®°ä¸ºFAILED")
                            except Exception as update_error:
                                logger.error(f"æ›´æ–°å­¤å„¿ä»»åŠ¡çŠ¶æ€å¤±è´¥: {update_error}")

                except Exception as task_error:
                    logger.warning(f"å¤„ç†ä»»åŠ¡ {task_id} æ—¶å‡ºé”™: {task_error}")

            logger.info(f"å…¨å±€GPUçŠ¶æ€æ¢å¤å®Œæˆ: æ¸…ç†äº†{cleaned_count}ä¸ªå·²å®Œæˆä»»åŠ¡, é‡å»ºäº†{restored_count}ä¸ªè¿è¡Œä¸­ä»»åŠ¡çš„GPUåˆ†é…")

        except Exception as e:
            logger.error(f"å¯åŠ¨æ—¶GPUçŠ¶æ€æ¢å¤å¤±è´¥: {e}")

    def _parse_device_to_gpu_ids(self, device: str) -> List[int]:
        """è§£ædeviceå­—ç¬¦ä¸²ä¸ºGPU IDåˆ—è¡¨"""
        try:
            if not device or device == "cpu":
                return []

            gpu_ids = []
            # å¤„ç† "cuda:0,cuda:1" æ ¼å¼
            if "," in device:
                parts = device.split(",")
                for part in parts:
                    part = part.strip()
                    if part.startswith("cuda:"):
                        gpu_id = int(part.split(":")[1])
                        gpu_ids.append(gpu_id)
            else:
                # å¤„ç† "cuda:0" æ ¼å¼
                if device.startswith("cuda:"):
                    gpu_id = int(device.split(":")[1])
                    gpu_ids.append(gpu_id)

            return gpu_ids
        except Exception as e:
            logger.warning(f"è§£æè®¾å¤‡å­—ç¬¦ä¸²å¤±è´¥ '{device}': {e}")
            return []

    def _schedule_delayed_cleanup(self):
        """å»¶è¿Ÿæ‰§è¡ŒGPUæ¸…ç†ï¼Œé¿å…å¾ªç¯å¯¼å…¥"""
        import threading
        import time

        def delayed_cleanup():
            # ç­‰å¾…3ç§’è®©æœåŠ¡å®Œå…¨åˆå§‹åŒ–
            time.sleep(3)
            try:
                logger.info("å¼€å§‹å»¶è¿ŸGPUæ¸…ç†...")
                self._cleanup_completed_tasks_on_startup()
            except Exception as e:
                logger.error(f"å»¶è¿ŸGPUæ¸…ç†å¤±è´¥: {e}")

        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œæ¸…ç†
        cleanup_thread = threading.Thread(target=delayed_cleanup, daemon=True)
        cleanup_thread.start()


# å…¨å±€GPUèµ„æºç®¡ç†å™¨å®ä¾‹
gpu_resource_manager = GPUResourceManager()