"""
GPUèµ„æºåŠ¨æ€åˆ†é…ç®¡ç†å™¨
åŸºäºç°æœ‰çš„deviceå‚æ•°æ¶æ„ï¼Œå®ç°å¤šä»»åŠ¡GPUåŠ¨æ€åˆ†é…
"""

import os
import threading
import logging
import time
from typing import Dict, List, Optional, Set
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


class GPUResourceManager:
    """GPUèµ„æºåŠ¨æ€åˆ†é…ç®¡ç†å™¨"""
    
    def __init__(self):
        self.gpu_allocations: Dict[int, str] = {}  # {gpu_id: task_id}
        self.task_gpus: Dict[str, Set[int]] = {}   # {task_id: {gpu_ids}}
        self.allocation_times: Dict[str, datetime] = {}  # {task_id: allocation_time}
        self._lock = threading.RLock()
        self.max_gpus = self._detect_max_gpus()
        
        logger.info(f"ğŸ”§ GPUèµ„æºç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ£€æµ‹åˆ° {self.max_gpus} ä¸ªGPU")
    
    def _detect_max_gpus(self) -> int:
        """æ£€æµ‹ç³»ç»ŸGPUæ•°é‡"""
        try:
            # æ–¹æ³•1: ä½¿ç”¨nvidia-ml-py
            import pynvml
            pynvml.nvmlInit()
            gpu_count = pynvml.nvmlDeviceGetCount()
            return gpu_count
        except:
            try:
                # æ–¹æ³•2: ä½¿ç”¨torch
                import torch
                if torch.cuda.is_available():
                    return torch.cuda.device_count()
            except:
                pass
        
        # æ–¹æ³•3: é»˜è®¤å‡è®¾4ä¸ªGPUï¼ˆå¯é…ç½®ï¼‰
        logger.warning("æ— æ³•æ£€æµ‹GPUæ•°é‡ï¼Œä½¿ç”¨é»˜è®¤å€¼4")
        return int(os.environ.get('MAX_GPUS', '4'))
    
    def allocate_gpus_for_task(self, task_id: str, device_request: str = "auto") -> Optional[str]:
        """
        ä¸ºä»»åŠ¡åˆ†é…GPUèµ„æº
        
        Args:
            task_id: ä»»åŠ¡ID
            device_request: è®¾å¤‡è¯·æ±‚ï¼Œæ”¯æŒ:
                - "auto": è‡ªåŠ¨åˆ†é…1ä¸ªGPU
                - "cpu": ä½¿ç”¨CPU
                - "cuda:0": æŒ‡å®šGPU 0
                - "cuda:0,cuda:1": æŒ‡å®šå¤šä¸ªGPU
                - "auto:2": è‡ªåŠ¨åˆ†é…2ä¸ªGPU
        
        Returns:
            è®¾å¤‡å­—ç¬¦ä¸²ï¼Œå¦‚ "cuda:0,cuda:1" æˆ– "cpu" æˆ– Noneï¼ˆåˆ†é…å¤±è´¥ï¼‰
        """
        with self._lock:
            try:
                # CPUæ¨¡å¼
                if device_request == "cpu":
                    logger.info(f"âœ… ä»»åŠ¡ {task_id} ä½¿ç”¨CPUæ¨¡å¼")
                    return "cpu"
                
                # è§£æè®¾å¤‡è¯·æ±‚
                requested_gpus, num_gpus = self._parse_device_request(device_request)
                
                if requested_gpus is not None:
                    # æŒ‡å®šGPUæ¨¡å¼
                    if self._can_allocate_specific_gpus(requested_gpus):
                        return self._do_allocate_gpus(task_id, requested_gpus)
                    else:
                        logger.error(f"âŒ æŒ‡å®šçš„GPU {requested_gpus} ä¸å¯ç”¨")
                        return None
                else:
                    # è‡ªåŠ¨åˆ†é…æ¨¡å¼
                    available_gpus = self._get_available_gpus(num_gpus)
                    if available_gpus:
                        return self._do_allocate_gpus(task_id, available_gpus)
                    else:
                        logger.error(f"âŒ æ— æ³•è‡ªåŠ¨åˆ†é… {num_gpus} ä¸ªGPUï¼Œå½“å‰å¯ç”¨GPUä¸è¶³")
                        return None
                        
            except Exception as e:
                logger.error(f"âŒ GPUåˆ†é…å¤±è´¥: {e}")
                return None
    
    def _parse_device_request(self, device_request: str) -> tuple[Optional[List[int]], int]:
        """
        è§£æè®¾å¤‡è¯·æ±‚
        
        Returns:
            (æŒ‡å®šçš„GPUåˆ—è¡¨æˆ–None, è¯·æ±‚çš„GPUæ•°é‡)
        """
        if device_request == "auto":
            return None, 1
        
        if device_request.startswith("auto:"):
            try:
                num_gpus = int(device_request.split(":")[1])
                return None, num_gpus
            except (IndexError, ValueError):
                logger.warning(f"æ— æ³•è§£æautoè¯·æ±‚: {device_request}ï¼Œä½¿ç”¨é»˜è®¤1ä¸ªGPU")
                return None, 1
        
        if device_request.startswith("cuda:"):
            try:
                gpu_ids = []
                for part in device_request.split(","):
                    part = part.strip()
                    if part.startswith("cuda:"):
                        gpu_id = int(part.split(":")[1])
                        if 0 <= gpu_id < self.max_gpus:
                            gpu_ids.append(gpu_id)
                        else:
                            logger.warning(f"GPU ID {gpu_id} è¶…å‡ºèŒƒå›´ [0, {self.max_gpus-1}]")
                
                return gpu_ids if gpu_ids else None, len(gpu_ids)
                
            except (IndexError, ValueError) as e:
                logger.warning(f"æ— æ³•è§£æCUDAè¯·æ±‚: {device_request}, é”™è¯¯: {e}")
                return None, 1
        
        logger.warning(f"æœªè¯†åˆ«çš„è®¾å¤‡è¯·æ±‚: {device_request}ï¼Œä½¿ç”¨autoæ¨¡å¼")
        return None, 1
    
    def _can_allocate_specific_gpus(self, gpu_ids: List[int]) -> bool:
        """æ£€æŸ¥æŒ‡å®šçš„GPUæ˜¯å¦å¯ä»¥åˆ†é…"""
        for gpu_id in gpu_ids:
            if gpu_id in self.gpu_allocations:
                return False
        return True
    
    def _get_available_gpus(self, num_needed: int) -> Optional[List[int]]:
        """è·å–å¯ç”¨çš„GPU"""
        available = []
        for gpu_id in range(self.max_gpus):
            if gpu_id not in self.gpu_allocations:
                available.append(gpu_id)
                if len(available) >= num_needed:
                    break
        
        return available[:num_needed] if len(available) >= num_needed else None
    
    def _do_allocate_gpus(self, task_id: str, gpu_ids: List[int]) -> str:
        """æ‰§è¡ŒGPUåˆ†é…"""
        # åˆ†é…GPU
        for gpu_id in gpu_ids:
            self.gpu_allocations[gpu_id] = task_id
        
        # è®°å½•ä»»åŠ¡GPUæ˜ å°„
        self.task_gpus[task_id] = set(gpu_ids)
        self.allocation_times[task_id] = datetime.now()
        
        # ç”ŸæˆCUDAè®¾å¤‡å­—ç¬¦ä¸²
        if len(gpu_ids) == 1:
            device_str = f"cuda:{gpu_ids[0]}"
        else:
            device_str = ",".join([f"cuda:{gpu_id}" for gpu_id in gpu_ids])
        
        logger.info(f"âœ… ä¸ºä»»åŠ¡ {task_id} åˆ†é…GPU: {device_str} (ç‰©ç†GPU: {gpu_ids})")
        return device_str
    
    def release_gpus_for_task(self, task_id: str) -> bool:
        """é‡Šæ”¾ä»»åŠ¡çš„GPUèµ„æº"""
        with self._lock:
            try:
                if task_id not in self.task_gpus:
                    logger.debug(f"ä»»åŠ¡ {task_id} æ²¡æœ‰åˆ†é…GPUèµ„æº")
                    return True
                
                # è·å–ä»»åŠ¡çš„GPU
                gpu_ids = self.task_gpus[task_id]
                
                # é‡Šæ”¾GPU
                for gpu_id in gpu_ids:
                    if gpu_id in self.gpu_allocations:
                        del self.gpu_allocations[gpu_id]
                
                # æ¸…ç†è®°å½•
                del self.task_gpus[task_id]
                if task_id in self.allocation_times:
                    del self.allocation_times[task_id]
                
                logger.info(f"ğŸ”“ é‡Šæ”¾ä»»åŠ¡ {task_id} çš„GPUèµ„æº: {list(gpu_ids)}")
                return True
                
            except Exception as e:
                logger.error(f"é‡Šæ”¾GPUèµ„æºå¤±è´¥: {e}")
                return False
    
    def _get_gpu_memory_info(self, gpu_id: int) -> Dict:
        """è·å–GPUæ˜¾å­˜ä¿¡æ¯"""
        try:
            import pynvml
            pynvml.nvmlInit()
            handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_id)
            
            # è·å–æ˜¾å­˜ä¿¡æ¯
            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            total_memory = mem_info.total
            used_memory = mem_info.used
            free_memory = mem_info.free
            
            # è·å–GPUåˆ©ç”¨ç‡
            try:
                utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_util = utilization.gpu
                memory_util = utilization.memory
            except:
                gpu_util = None
                memory_util = None
            
            # è·å–GPUåç§°å’Œæ¸©åº¦
            try:
                gpu_name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')
            except:
                gpu_name = f"GPU {gpu_id}"
            
            try:
                temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
            except:
                temperature = None
            
            return {
                "gpu_name": gpu_name,
                "memory": {
                    "total": total_memory,
                    "used": used_memory,
                    "free": free_memory,
                    "total_gb": round(total_memory / 1024**3, 2),
                    "used_gb": round(used_memory / 1024**3, 2),
                    "free_gb": round(free_memory / 1024**3, 2),
                    "usage_percent": round((used_memory / total_memory) * 100, 1) if total_memory > 0 else 0
                },
                "utilization": {
                    "gpu_percent": gpu_util,
                    "memory_percent": memory_util
                },
                "temperature": temperature
            }
        except Exception as e:
            logger.debug(f"è·å–GPU {gpu_id} è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "gpu_name": f"GPU {gpu_id}",
                "memory": {
                    "total": None,
                    "used": None,
                    "free": None,
                    "total_gb": None,
                    "used_gb": None,
                    "free_gb": None,
                    "usage_percent": None
                },
                "utilization": {
                    "gpu_percent": None,
                    "memory_percent": None
                },
                "temperature": None,
                "error": str(e)
            }

    def get_resource_status(self) -> Dict:
        """è·å–èµ„æºçŠ¶æ€æŠ¥å‘Šï¼ˆåŒ…å«æ˜¾å­˜ä¿¡æ¯ï¼‰"""
        with self._lock:
            total_gpus = self.max_gpus
            allocated_gpus = len(self.gpu_allocations)
            free_gpus = total_gpus - allocated_gpus
            
            # æ„å»ºè¯¦ç»†çŠ¶æ€
            gpu_details = {}
            total_memory_gb = 0
            used_memory_gb = 0
            
            for gpu_id in range(total_gpus):
                # è·å–GPUç¡¬ä»¶ä¿¡æ¯
                gpu_hw_info = self._get_gpu_memory_info(gpu_id)
                
                # åŸºç¡€åˆ†é…çŠ¶æ€
                if gpu_id in self.gpu_allocations:
                    task_id = self.gpu_allocations[gpu_id]
                    alloc_time = self.allocation_times.get(task_id)
                    gpu_status = {
                        "status": "allocated",
                        "task_id": task_id,
                        "allocated_at": alloc_time.isoformat() if alloc_time else None,
                        "duration": str(datetime.now() - alloc_time) if alloc_time else None
                    }
                else:
                    gpu_status = {
                        "status": "free",
                        "task_id": None,
                        "allocated_at": None,
                        "duration": None
                    }
                
                # åˆå¹¶ç¡¬ä»¶ä¿¡æ¯
                gpu_details[gpu_id] = {**gpu_status, **gpu_hw_info}
                
                # ç´¯è®¡å†…å­˜ç»Ÿè®¡
                if gpu_hw_info["memory"]["total_gb"]:
                    total_memory_gb += gpu_hw_info["memory"]["total_gb"]
                if gpu_hw_info["memory"]["used_gb"]:
                    used_memory_gb += gpu_hw_info["memory"]["used_gb"]
            
            return {
                "total_gpus": total_gpus,
                "allocated_gpus": allocated_gpus,
                "free_gpus": free_gpus,
                "utilization_rate": allocated_gpus / total_gpus if total_gpus > 0 else 0,
                "memory_summary": {
                    "total_gb": round(total_memory_gb, 2),
                    "used_gb": round(used_memory_gb, 2),
                    "free_gb": round(total_memory_gb - used_memory_gb, 2),
                    "usage_percent": round((used_memory_gb / total_memory_gb) * 100, 1) if total_memory_gb > 0 else 0
                },
                "gpu_details": gpu_details,
                "active_tasks": len(self.task_gpus)
            }
    
    def cleanup_stale_allocations(self, max_age_hours: int = 24):
        """æ¸…ç†é•¿æ—¶é—´æœªé‡Šæ”¾çš„åˆ†é…ï¼ˆé˜²æ­¢èµ„æºæ³„æ¼ï¼‰"""
        with self._lock:
            current_time = datetime.now()
            stale_tasks = []
            
            for task_id, alloc_time in self.allocation_times.items():
                if current_time - alloc_time > timedelta(hours=max_age_hours):
                    stale_tasks.append(task_id)
            
            for task_id in stale_tasks:
                logger.warning(f"âš ï¸ æ¸…ç†è¿‡æœŸçš„GPUåˆ†é…: ä»»åŠ¡ {task_id}")
                self.release_gpus_for_task(task_id)
            
            return len(stale_tasks)


# å…¨å±€GPUèµ„æºç®¡ç†å™¨å®ä¾‹
gpu_resource_manager = GPUResourceManager()