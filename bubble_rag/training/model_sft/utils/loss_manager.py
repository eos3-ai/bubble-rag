"""
è®­ç»ƒLossæœ¬åœ°æ–‡ä»¶ç®¡ç†å™¨
è´Ÿè´£å°†è®­ç»ƒè¿‡ç¨‹ä¸­çš„lossæ•°æ®ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œé¿å…æ•°æ®åº“æ€§èƒ½é—®é¢˜
"""

import json
import os
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
from threading import Lock

logger = logging.getLogger(__name__)


class LossManager:
    """è®­ç»ƒLossæœ¬åœ°æ–‡ä»¶ç®¡ç†å™¨"""
    
    def __init__(self, output_dir: str, task_id: str):
        """
        åˆå§‹åŒ–Lossç®¡ç†å™¨
        
        Args:
            output_dir: è®­ç»ƒè¾“å‡ºç›®å½•
            task_id: è®­ç»ƒä»»åŠ¡ID
        """
        self.output_dir = output_dir
        self.task_id = task_id
        self.lock = Lock()
        
        # è®¾ç½®æ—¥å¿—ç›®å½•ç»“æ„: {output_dir}/logs/training/{task_id}/
        self.logs_dir = Path(output_dir) / "logs" / "training" / task_id
        self.loss_history_file = self.logs_dir / "loss_history.jsonl"
        self.training_metrics_file = self.logs_dir / "training_metrics.json"
        self.metadata_file = self.logs_dir / "metadata.json"  # æ–°å¢ï¼šå…ƒæ•°æ®æ–‡ä»¶
        
        # åˆ›å»ºç›®å½•
        self._ensure_directories()
        
        # åˆå§‹åŒ–è®­ç»ƒæŒ‡æ ‡
        self.training_metrics = {
            "task_id": task_id,
            "start_time": datetime.now().isoformat(),
            "total_steps": 0,
            "epochs_completed": 0,
            "best_train_loss": None,
            "best_eval_loss": None,
            "loss_records_count": 0,
            "last_updated": None,
            "epoch_summaries": [],  # æ¯ä¸ªepochçš„æ±‡æ€»ä¿¡æ¯
            "final_results": {}     # æœ€ç»ˆè¯„ä¼°ç»“æœ
        }
        
        logger.info(f"Lossç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ: {self.logs_dir}")
    
    def _ensure_directories(self):
        """ç¡®ä¿ç›®å½•ç»“æ„å­˜åœ¨"""
        try:
            self.logs_dir.mkdir(parents=True, exist_ok=True)
            logger.debug(f"åˆ›å»ºæ—¥å¿—ç›®å½•: {self.logs_dir}")
        except Exception as e:
            logger.error(f"åˆ›å»ºæ—¥å¿—ç›®å½•å¤±è´¥: {e}")
            raise
    
    def save_loss_record(self, step: int, metrics: Dict[str, Any], epoch: Optional[float] = None, save_to_db: bool = True, data_source_mapping: Dict[str, str] = None):
        """
        ä¿å­˜å•æ¬¡lossè®°å½•åˆ°JSONLæ–‡ä»¶
        
        Args:
            step: è®­ç»ƒæ­¥æ•°
            metrics: æŒ‡æ ‡å­—å…¸ï¼ˆåŒ…å«train_loss, eval_lossç­‰ï¼‰
            epoch: å½“å‰epochï¼ˆå¯é€‰ï¼‰
        """
        with self.lock:
            try:
                # æ„å»ºè®°å½•
                record = {
                    "step": step,
                    "timestamp": datetime.now().isoformat(),
                    **metrics
                }
                
                if epoch is not None:
                    record["epoch"] = epoch
                
                # å†™å…¥JSONLæ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªJSONï¼‰
                with open(self.loss_history_file, 'a', encoding='utf-8') as f:
                    json.dump(record, f, ensure_ascii=False)
                    f.write('\n')
                
                # æ›´æ–°è®­ç»ƒæŒ‡æ ‡
                self._update_training_metrics(step, metrics, epoch)

                # ä¿å­˜åˆ°æ•°æ®åº“
                if save_to_db:
                    self._save_loss_to_database(metrics, step, epoch, data_source_mapping)

                logger.debug(f"ä¿å­˜lossè®°å½•: step={step}, metrics={list(metrics.keys())}")

            except Exception as e:
                logger.error(f"ä¿å­˜lossè®°å½•å¤±è´¥: {e}")

    def save_or_merge_loss_record(self, step: int, metrics: Dict[str, Any], epoch: Optional[float] = None, merge_key: str = None):
        """
        ä¿å­˜æˆ–åˆå¹¶lossè®°å½•ï¼Œæ”¯æŒåŒstepå†…çš„åˆå¹¶

        Args:
            step: è®­ç»ƒæ­¥æ•°
            metrics: æŒ‡æ ‡å­—å…¸
            epoch: å½“å‰epoch
            merge_key: åˆå¹¶é”®ï¼Œç”¨äºæ ‡è¯†å¯ä»¥åˆå¹¶çš„è®°å½•
        """
        with self.lock:
            try:
                # å¦‚æœæ²¡æœ‰æä¾›merge_keyï¼Œç›´æ¥ä¿å­˜
                if merge_key is None:
                    self.save_loss_record(step, metrics, epoch)
                    return

                # å°è¯•è¯»å–ç°æœ‰è®°å½•å¹¶æŸ¥æ‰¾å¯åˆå¹¶çš„è®°å½•
                existing_records = []
                merged = False

                if self.loss_history_file.exists():
                    with open(self.loss_history_file, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip():
                                existing_records.append(json.loads(line.strip()))

                # æŸ¥æ‰¾åŒstepçš„è®°å½•è¿›è¡Œåˆå¹¶
                for i, record in enumerate(existing_records):
                    if (record.get('step') == step and
                        record.get('epoch') == epoch):

                        # æ£€æŸ¥æ˜¯å¦æ˜¯åŒä¸€æ•°æ®æºï¼ˆé€šè¿‡æŒ‡æ ‡å‰ç¼€åˆ¤æ–­ï¼‰
                        record_source_ids = set()
                        new_source_ids = set()

                        for key in record.keys():
                            if key.startswith('eval_') and '_' in key[5:]:
                                parts = key[5:].split('_')
                                if parts[0].isdigit():
                                    record_source_ids.add(parts[0])

                        for key in metrics.keys():
                            if key.startswith('eval_') and '_' in key[5:]:
                                parts = key[5:].split('_')
                                if parts[0].isdigit():
                                    new_source_ids.add(parts[0])

                        # å¦‚æœæœ‰ç›¸åŒçš„source_idï¼Œåˆ™åˆå¹¶
                        if record_source_ids & new_source_ids:
                            existing_records[i].update(metrics)
                            merged = True
                            logger.info(f"ğŸ”— åˆå¹¶åŒstepè®°å½•: step={step}, source_ids={new_source_ids}")
                            break

                if not merged:
                    # æ²¡æœ‰æ‰¾åˆ°å¯åˆå¹¶çš„è®°å½•ï¼Œæ·»åŠ æ–°è®°å½•
                    new_record = {
                        "step": step,
                        "timestamp": datetime.now().isoformat(),
                        **metrics
                    }
                    if epoch is not None:
                        new_record["epoch"] = epoch
                    existing_records.append(new_record)
                    logger.info(f"æ·»åŠ æ–°è®°å½•: step={step}, metrics={list(metrics.keys())}")

                # é‡å†™æ•´ä¸ªæ–‡ä»¶
                with open(self.loss_history_file, 'w', encoding='utf-8') as f:
                    for record in existing_records:
                        json.dump(record, f, ensure_ascii=False)
                        f.write('\n')

                # æ›´æ–°è®­ç»ƒæŒ‡æ ‡ï¼ˆåªåœ¨æ·»åŠ æ–°è®°å½•æ—¶æ›´æ–°ï¼‰
                if not merged:
                    self._update_training_metrics(step, metrics, epoch)

            except Exception as e:
                logger.error(f"ä¿å­˜/åˆå¹¶lossè®°å½•å¤±è´¥: {e}")
                # å›é€€åˆ°æ™®é€šä¿å­˜
                self.save_loss_record(step, metrics, epoch)

    def _update_training_metrics(self, step: int, metrics: Dict[str, Any], epoch: Optional[float]):
        """æ›´æ–°è®­ç»ƒæŒ‡æ ‡æ±‡æ€»"""
        try:
            # æ›´æ–°åŸºç¡€ä¿¡æ¯
            self.training_metrics["total_steps"] = max(self.training_metrics["total_steps"], step)
            self.training_metrics["loss_records_count"] += 1
            self.training_metrics["last_updated"] = datetime.now().isoformat()
            
            if epoch is not None:
                self.training_metrics["epochs_completed"] = max(
                    self.training_metrics["epochs_completed"], epoch
                )
            
            # æ›´æ–°æœ€ä½³loss
            if "train_loss" in metrics:
                train_loss = metrics["train_loss"]
                if (self.training_metrics["best_train_loss"] is None or 
                    train_loss < self.training_metrics["best_train_loss"]):
                    self.training_metrics["best_train_loss"] = train_loss
            
            if "eval_loss" in metrics:
                eval_loss = metrics["eval_loss"]
                if (self.training_metrics["best_eval_loss"] is None or 
                    eval_loss < self.training_metrics["best_eval_loss"]):
                    self.training_metrics["best_eval_loss"] = eval_loss
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(self.training_metrics_file, 'w', encoding='utf-8') as f:
                json.dump(self.training_metrics, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"æ›´æ–°è®­ç»ƒæŒ‡æ ‡å¤±è´¥: {e}")
    
    def finalize_epoch(self, epoch: int, epoch_metrics: Dict[str, Any]):
        """
        å®Œæˆä¸€ä¸ªepochï¼Œä¿å­˜epochæ±‡æ€»ä¿¡æ¯
        
        Args:
            epoch: epochç¼–å·
            epoch_metrics: epochæ±‡æ€»æŒ‡æ ‡
        """
        with self.lock:
            try:
                epoch_summary = {
                    "epoch": epoch,
                    "timestamp": datetime.now().isoformat(),
                    **epoch_metrics
                }
                
                # æ·»åŠ åˆ°epochæ±‡æ€»åˆ—è¡¨
                self.training_metrics["epoch_summaries"].append(epoch_summary)
                
                # ä¿å­˜åˆ°æ–‡ä»¶
                with open(self.training_metrics_file, 'w', encoding='utf-8') as f:
                    json.dump(self.training_metrics, f, ensure_ascii=False, indent=2)
                
                logger.info(f"Epoch {epoch} æ±‡æ€»å·²ä¿å­˜: {list(epoch_metrics.keys())}")
                
            except Exception as e:
                logger.error(f"ä¿å­˜epochæ±‡æ€»å¤±è´¥: {e}")
    
    def get_loss_history(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        è·å–losså†å²è®°å½•
        
        Args:
            limit: é™åˆ¶è¿”å›çš„è®°å½•æ•°é‡
            
        Returns:
            lossè®°å½•åˆ—è¡¨
        """
        try:
            if not self.loss_history_file.exists():
                return []
            
            records = []
            with open(self.loss_history_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        records.append(json.loads(line.strip()))
            
            # åº”ç”¨é™åˆ¶
            if limit and len(records) > limit:
                records = records[-limit:]  # è¿”å›æœ€è¿‘çš„è®°å½•
            
            return records
            
        except Exception as e:
            logger.error(f"è·å–losså†å²å¤±è´¥: {e}")
            return []
    
    def get_training_metrics(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒæŒ‡æ ‡æ±‡æ€»"""
        try:
            if self.training_metrics_file.exists():
                with open(self.training_metrics_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                return self.training_metrics
        except Exception as e:
            logger.error(f"è·å–è®­ç»ƒæŒ‡æ ‡å¤±è´¥: {e}")
            return self.training_metrics
    
    def get_database_summary(self) -> Dict[str, Any]:
        """
        è·å–ç”¨äºæ•°æ®åº“å­˜å‚¨çš„æ±‡æ€»ä¿¡æ¯
        
        Returns:
            æ•°æ®åº“æ±‡æ€»ä¿¡æ¯å­—å…¸
        """
        try:
            summary = {
                "task_id": self.task_id,
                "loss_file_path": str(self.loss_history_file),
                "metrics_file_path": str(self.training_metrics_file),
                "total_records": self.training_metrics.get("loss_records_count", 0),
                "training_duration_seconds": self.training_metrics.get("training_duration_seconds"),
                "best_train_loss": self.training_metrics.get("best_train_loss"),
                "best_eval_loss": self.training_metrics.get("best_eval_loss"),
                "total_steps": self.training_metrics.get("total_steps", 0),
                "epochs_completed": self.training_metrics.get("epochs_completed", 0),
                "start_time": self.training_metrics.get("start_time"),
                "end_time": self.training_metrics.get("end_time"),
                "status": self.training_metrics.get("status", "running"),
                "epoch_summaries": self.training_metrics.get("epoch_summaries", []),
                "final_metrics": self.training_metrics.get("final_metrics", {})
            }
            return summary
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ•°æ®åº“æ±‡æ€»ä¿¡æ¯å¤±è´¥: {e}")
            return {}

    def finalize_training(self, final_metrics: Optional[Dict[str, Any]] = None):
        """
        å®Œæˆè®­ç»ƒï¼Œä¿å­˜æœ€ç»ˆæŒ‡æ ‡
        
        Args:
            final_metrics: æœ€ç»ˆè®­ç»ƒæŒ‡æ ‡
        """
        with self.lock:
            try:
                # æ›´æ–°å®Œæˆæ—¶é—´
                self.training_metrics["end_time"] = datetime.now().isoformat()
                self.training_metrics["status"] = "completed"
                
                # æ·»åŠ æœ€ç»ˆæŒ‡æ ‡
                if final_metrics:
                    self.training_metrics["final_metrics"] = final_metrics
                
                # è®¡ç®—è®­ç»ƒæ—¶é•¿
                if "start_time" in self.training_metrics:
                    start_time = datetime.fromisoformat(self.training_metrics["start_time"])
                    end_time = datetime.now()
                    duration = (end_time - start_time).total_seconds()
                    self.training_metrics["training_duration_seconds"] = duration

                    # åŒæ—¶ä¿å­˜æ ¼å¼åŒ–çš„æ—¶é•¿
                    self.training_metrics["duration_formatted"] = self._format_duration(duration)

                # ä¿å­˜æœ€ç»ˆæŒ‡æ ‡
                with open(self.training_metrics_file, 'w', encoding='utf-8') as f:
                    json.dump(self.training_metrics, f, ensure_ascii=False, indent=2)
                
                logger.info(f"è®­ç»ƒæŒ‡æ ‡å·²å®Œæˆä¿å­˜: {self.training_metrics_file}")
                
                # è¿”å›æ•°æ®åº“æ±‡æ€»ä¿¡æ¯ä¾›è°ƒç”¨è€…ä½¿ç”¨
                return self.get_database_summary()
                
            except Exception as e:
                logger.error(f"å®Œæˆè®­ç»ƒæŒ‡æ ‡ä¿å­˜å¤±è´¥: {e}")
                return None

    def save_metadata(self, metadata: Dict[str, Any]):
        """
        ä¿å­˜è®­ç»ƒå…ƒæ•°æ®åˆ°æ–‡ä»¶

        Args:
            metadata: å…ƒæ•°æ®å­—å…¸ï¼ŒåŒ…å«æ•°æ®æºæ˜ å°„ã€è¯„ä¼°å™¨ç±»å‹ç­‰ä¿¡æ¯
        """
        with self.lock:
            try:
                # æ·»åŠ æ—¶é—´æˆ³
                metadata["created_at"] = datetime.now().isoformat()
                metadata["task_id"] = self.task_id

                with open(self.metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)

                logger.info(f"å…ƒæ•°æ®å·²ä¿å­˜: {self.metadata_file}")
                logger.debug(f"å…ƒæ•°æ®å†…å®¹: {list(metadata.keys())}")

            except Exception as e:
                logger.error(f"ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")

    def get_metadata(self) -> Dict[str, Any]:
        """
        è¯»å–è®­ç»ƒå…ƒæ•°æ®

        Returns:
            å…ƒæ•°æ®å­—å…¸ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›ç©ºå­—å…¸
        """
        try:
            if self.metadata_file.exists():
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                logger.debug(f"è¯»å–å…ƒæ•°æ®æˆåŠŸ: {list(metadata.keys())}")
                return metadata
            else:
                logger.debug("å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºå­—å…¸")
                return {}
        except Exception as e:
            logger.error(f"è¯»å–å…ƒæ•°æ®å¤±è´¥: {e}")
            return {}

    def _format_duration(self, seconds: Optional[float]) -> Optional[str]:
        """å°†ç§’æ•°è½¬æ¢ä¸ºäººç±»å¯è¯»æ ¼å¼"""
        if seconds is None:
            return None

        total_seconds = int(seconds)
        days = total_seconds // 86400
        hours = (total_seconds % 86400) // 3600
        minutes = (total_seconds % 3600) // 60
        secs = total_seconds % 60

        if days > 0:
            return f"{days}å¤©{hours}æ—¶{minutes}åˆ†{secs}ç§’"
        elif hours > 0:
            return f"{hours}æ—¶{minutes}åˆ†{secs}ç§’"
        elif minutes > 0:
            return f"{minutes}åˆ†{secs}ç§’"
        else:
            return f"{secs}ç§’"

    def _save_loss_to_database(self, loss_metrics: Dict[str, Any], step: int, epoch: Optional[float] = None, data_source_mapping: Dict[str, str] = None):
        """ä¿å­˜lossæ•°æ®åˆ°æ•°æ®åº“"""
        try:
            from bubble_rag.training.mysql_service.service.training_dataset_service import TrainingDatasetService

            for loss_key, loss_value in loss_metrics.items():
                if 'loss' in loss_key.lower() and isinstance(loss_value, (int, float)):
                    # è®­ç»ƒlossï¼šå­˜å‚¨åˆ°ç¬¬ä¸€ä¸ªæ•°æ®æºçš„è®­ç»ƒé›†
                    if 'train' in loss_key.lower() or loss_key.lower() == 'loss':
                        train_datasets = TrainingDatasetService.get_datasets_by_job_and_split(self.task_id, 'train')
                        if train_datasets:
                            dataset_info = train_datasets[0]
                            dataset_id = dataset_info["id"]
                            TrainingDatasetService.add_loss_record(
                                dataset_id=dataset_id,
                                loss_value=loss_value,
                                step=step,
                                epoch=epoch
                            )
                            logger.debug(f"è®­ç»ƒLosså·²ä¿å­˜åˆ°æ•°æ®åº“: dataset_id={dataset_id}, step={step}, {loss_key}={loss_value}")

                    # è¯„ä¼°lossï¼šä½¿ç”¨ç»Ÿä¸€çš„æ˜ å°„é€»è¾‘æŒ‰æ•°æ®æºç²¾ç¡®å­˜å‚¨
                    elif 'eval' in loss_key.lower() and loss_key.startswith('eval_'):
                        # ä½¿ç”¨ç»Ÿä¸€çš„æ˜ å°„é€»è¾‘å¤„ç†eval loss
                        eval_metrics = {loss_key: loss_value}
                        from bubble_rag.training.model_sft.utils.evaluation_result import _separate_eval_results_by_source
                        source_eval_results = _separate_eval_results_by_source(eval_metrics, data_source_mapping)

                        for source_id, source_results in source_eval_results.items():
                            loss_metrics_for_source = {k: v for k, v in source_results.items() if 'loss' in k}
                            if loss_metrics_for_source:
                                # è·å–æ‰€æœ‰evalæ•°æ®é›†å¹¶ç­›é€‰åŒ¹é…çš„source_id
                                eval_datasets = TrainingDatasetService.get_datasets_by_job_and_split(self.task_id, "eval")
                                matching_dataset = None
                                for dataset in eval_datasets:
                                    if dataset.get("data_source_id") == source_id:
                                        matching_dataset = dataset
                                        break

                                if matching_dataset:
                                    dataset_id = matching_dataset["id"]
                                    for metric_name, metric_value in loss_metrics_for_source.items():
                                        TrainingDatasetService.add_loss_record(
                                            dataset_id=dataset_id,
                                            loss_value=metric_value,
                                            step=step,
                                            epoch=epoch
                                        )
                                        logger.debug(f"è¯„ä¼°Losså·²ä¿å­˜åˆ°æ•°æ®åº“: source_id={source_id}, dataset_id={dataset_id}, step={step}, {metric_name}={metric_value}")
        except Exception as e:
            logger.warning(f"ä¿å­˜lossåˆ°æ•°æ®åº“å¤±è´¥: {e}")



# å…¨å±€lossç®¡ç†å™¨å®ä¾‹å­—å…¸ {task_id: LossManager}
_loss_managers: Dict[str, LossManager] = {}
_managers_lock = Lock()


def get_loss_manager(output_dir: str, task_id: str) -> LossManager:
    """è·å–æˆ–åˆ›å»ºlossç®¡ç†å™¨å®ä¾‹"""
    with _managers_lock:
        if task_id not in _loss_managers:
            _loss_managers[task_id] = LossManager(output_dir, task_id)
        return _loss_managers[task_id]


def cleanup_loss_manager(task_id: str, final_metrics: Optional[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]:
    """
    æ¸…ç†lossç®¡ç†å™¨å®ä¾‹
    
    Args:
        task_id: ä»»åŠ¡ID
        final_metrics: æœ€ç»ˆè®­ç»ƒæŒ‡æ ‡
        
    Returns:
        æ•°æ®åº“æ±‡æ€»ä¿¡æ¯ï¼Œç”¨äºå­˜å‚¨åˆ°æ•°æ®åº“
    """
    with _managers_lock:
        if task_id in _loss_managers:
            try:
                # å®Œæˆè®­ç»ƒå¹¶è·å–æ±‡æ€»ä¿¡æ¯
                summary = _loss_managers[task_id].finalize_training(final_metrics)
                del _loss_managers[task_id]
                logger.info(f"ğŸ§¹ æ¸…ç†lossç®¡ç†å™¨: {task_id}")
                return summary
            except Exception as e:
                logger.error(f"æ¸…ç†lossç®¡ç†å™¨å¤±è´¥: {e}")
                return None
        return None