"""
Embedding model trainer.

Specialized trainer for sentence embedding models using SentenceTransformers.
"""

import logging
import os
from typing import Any, Dict, Optional, Union

from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments
from sentence_transformers.losses import CosineSimilarityLoss, ContrastiveLoss, MultipleNegativesRankingLoss, CoSENTLoss

from ..core.base_trainer import BaseTrainer
from ..utils.common_utils import create_embedding_loss

logger = logging.getLogger(__name__)


class EmbeddingTrainer(BaseTrainer):
    """
    Specialized trainer for embedding models.
    
    Handles SentenceTransformer model training with appropriate loss functions
    and training arguments.
    """
    
    def initialize_model(self, model_name: str) -> SentenceTransformer:
        """
        Initialize SentenceTransformer model.
        
        Args:
            model_name: Name or path of the model
            
        Returns:
            Initialized SentenceTransformer model
        """
        logger.info(f"åˆå§‹åŒ– SentenceTransformer æ¨¡å‹: {model_name}")
        
        try:
            # Try ModelScope download first (if available)
            try:
                from modelscope import snapshot_download
                from bubble_rag.server_config import TRAINING_CACHE
                
                logger.info(f"å°è¯•ä» ModelScope ä¸‹è½½æ¨¡å‹: {model_name}")
                model_dir = snapshot_download(model_name, cache_dir=TRAINING_CACHE)
                model = SentenceTransformer(model_dir)
                logger.info(f"âœ… ModelScope ä¸‹è½½æˆåŠŸï¼Œè·¯å¾„: {model_dir}")
                
            except ImportError:
                logger.info("ModelScope æœªå®‰è£…ï¼Œä½¿ç”¨ HuggingFace æ–¹å¼")
                model = SentenceTransformer(model_name)
                logger.info(f"âœ… HuggingFace ä¸‹è½½æˆåŠŸ: {model_name}")
                
            except Exception as e:
                # Fallback to HuggingFace if ModelScope fails
                logger.warning(f"ModelScope ä¸‹è½½å¤±è´¥: {e}ï¼Œå›é€€åˆ° HuggingFace")
                model = SentenceTransformer(model_name)
                logger.info(f"âœ… HuggingFace ä¸‹è½½æˆåŠŸ: {model_name}")
            
            logger.info(f"SentenceTransformer æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {model_name}")
            return model
            
        except Exception as e:
            # Try offline mode as last resort
            if "couldn't connect" in str(e).lower() or "connection" in str(e).lower():
                logger.warning(f"ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹: {model_name}")
                try:
                    os.environ["TRANSFORMERS_OFFLINE"] = "1"
                    model = SentenceTransformer(model_name, local_files_only=True)
                    logger.info(f"âœ… æœ¬åœ°ç¼“å­˜åŠ è½½æˆåŠŸ: {model_name}")
                    return model
                except Exception as cache_error:
                    error_msg = f"embeddingæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {model_name}ï¼Œç½‘ç»œä¸å¯ç”¨ä¸”æœ¬åœ°ç¼“å­˜æœªæ‰¾åˆ°ã€‚é”™è¯¯: {str(cache_error)}"
                    logger.error(error_msg)
                    raise RuntimeError(error_msg)
            else:
                error_msg = f"embeddingæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {model_name}, é”™è¯¯: {str(e)}"
                logger.error(error_msg)
                raise RuntimeError(error_msg)
    
    def create_loss_function(self, model: SentenceTransformer, train_dataset: Any) -> Any:
        """
        Create appropriate loss function for embedding training.
        
        Args:
            model: The SentenceTransformer model
            train_dataset: Training dataset (can be single dataset or dict of datasets)
            
        Returns:
            Loss function or dict of loss functions for multi-dataset training
        """
        logger.info("åˆ›å»º embedding æŸå¤±å‡½æ•°")
        
        if isinstance(train_dataset, dict):
            # Multi-dataset case: create loss function for each dataset
            losses = {}
            
            for dataset_name, dataset in train_dataset.items():
                # Get target column for this dataset
                target_column = self.evaluator_factory._get_dataset_target_column(dataset)
                
                # Apply column filtering: keep only 2 input columns + target column
                column_names = dataset.column_names
                if len(column_names) >= 3:
                    input_columns = [col for col in column_names if col != target_column][:2]
                    columns_to_keep = input_columns + [target_column]
                    filtered_dataset = dataset.select_columns(columns_to_keep)
                    logger.info(f"Embeddingæ•°æ®é›† '{dataset_name}' åˆ—è¿‡æ»¤: {column_names} â†’ {columns_to_keep}")
                else:
                    filtered_dataset = dataset
                
                # Update the dataset in the training data
                train_dataset[dataset_name] = filtered_dataset
                
                # Create loss function for this dataset
                loss_func = create_embedding_loss(model, filtered_dataset, target_column, dataset_name)
                losses[dataset_name] = loss_func
            
            logger.info(f"ä¸ºå¤šä¸ªembeddingæ•°æ®é›†åˆ›å»ºäº†æŸå¤±å‡½æ•°: {list(losses.keys())}")
            return losses
        else:
            # Single dataset case
            target_column = self.evaluator_factory._get_dataset_target_column(train_dataset)
            
            # Apply column filtering
            column_names = train_dataset.column_names
            if len(column_names) >= 3:
                input_columns = [col for col in column_names if col != target_column][:2]
                columns_to_keep = input_columns + [target_column]
                train_dataset = train_dataset.select_columns(columns_to_keep)
                logger.info(f"å•Embeddingæ•°æ®é›†åˆ—è¿‡æ»¤: {column_names} â†’ {columns_to_keep}")
            
            # Create single loss function
            loss = create_embedding_loss(model, train_dataset, target_column)
            logger.info("å•embeddingæ•°æ®é›†æŸå¤±å‡½æ•°åˆ›å»ºå®Œæˆ")
            return loss
    
    def create_training_args(self, config: Dict[str, Any]) -> SentenceTransformerTrainingArguments:
        """
        Create SentenceTransformerTrainingArguments.

        Args:
            config: Training configuration dictionary (å·²ç»é€šè¿‡base_trainerè¿›è¡Œäº†GPUå…¼å®¹æ€§æ£€æµ‹)

        Returns:
            SentenceTransformerTrainingArguments instance
        """
        logger.info("åˆ›å»º SentenceTransformerTrainingArguments")

        try:
            args = SentenceTransformerTrainingArguments(**config)
            logger.info("SentenceTransformerTrainingArguments åˆ›å»ºæˆåŠŸ")

            # è°ƒè¯•ï¼šè®°å½•å®é™…çš„è®­ç»ƒå‚æ•°å€¼
            logger.info(f"ğŸ” [DEBUG] å®é™…è®­ç»ƒå‚æ•°: metric_for_best_model={getattr(args, 'metric_for_best_model', None)}, "
                       f"load_best_model_at_end={getattr(args, 'load_best_model_at_end', None)}, "
                       f"greater_is_better={getattr(args, 'greater_is_better', None)}")
            logger.info(f"ğŸ” [DEBUG] æ··åˆç²¾åº¦è®¾ç½®: bf16={getattr(args, 'bf16', None)}, fp16={getattr(args, 'fp16', None)}")

            return args
        except Exception as e:
            logger.error(f"SentenceTransformerTrainingArguments åˆ›å»ºå¤±è´¥: {e}")
            raise
    
    def create_trainer_instance(self, model, args, train_dataset, eval_dataset, loss, evaluator) -> SentenceTransformerTrainer:
        """
        Create SentenceTransformerTrainer instance.
        
        Args:
            model: SentenceTransformer model
            args: SentenceTransformerTrainingArguments
            train_dataset: Training dataset
            eval_dataset: Evaluation dataset (optional)
            loss: Loss function
            evaluator: Evaluator (optional)
            
        Returns:
            SentenceTransformerTrainer instance
        """
        logger.info("åˆ›å»º SentenceTransformerTrainer")
        
        # Build trainer arguments
        trainer_kwargs = {
            "model": model,
            "args": args,
            "train_dataset": train_dataset,
            "loss": loss,
        }
        
        # Add evaluation dataset if available
        if eval_dataset is not None:
            # Apply column filtering to evaluation dataset
            if isinstance(eval_dataset, dict):
                # Multi-dataset evaluation
                filtered_eval_dataset = {}
                for dataset_name, dataset in eval_dataset.items():
                    if dataset is not None:
                        target_column = self.evaluator_factory._get_dataset_target_column(dataset)
                        column_names = dataset.column_names
                        if len(column_names) >= 3:
                            input_columns = [col for col in column_names if col != target_column][:2]
                            columns_to_keep = input_columns + [target_column]
                            filtered_eval_dataset[dataset_name] = dataset.select_columns(columns_to_keep)
                            logger.info(f"EmbeddingéªŒè¯æ•°æ®é›† '{dataset_name}' åˆ—è¿‡æ»¤: {column_names} â†’ {columns_to_keep}")
                        else:
                            filtered_eval_dataset[dataset_name] = dataset
                
                trainer_kwargs["eval_dataset"] = filtered_eval_dataset
            else:
                # Single dataset evaluation
                target_column = self.evaluator_factory._get_dataset_target_column(eval_dataset)
                column_names = eval_dataset.column_names
                if len(column_names) >= 3:
                    input_columns = [col for col in column_names if col != target_column][:2]
                    columns_to_keep = input_columns + [target_column]
                    filtered_eval_dataset = eval_dataset.select_columns(columns_to_keep)
                    logger.info(f"å•éªŒè¯æ•°æ®é›†åˆ—è¿‡æ»¤: {column_names} â†’ {columns_to_keep}")
                else:
                    filtered_eval_dataset = eval_dataset
                
                trainer_kwargs["eval_dataset"] = filtered_eval_dataset
        
        # Add evaluator if available
        if evaluator is not None:
            trainer_kwargs["evaluator"] = evaluator
        
        try:
            trainer = SentenceTransformerTrainer(**trainer_kwargs)
            logger.info("SentenceTransformerTrainer åˆ›å»ºæˆåŠŸ")
            return trainer
        except Exception as e:
            logger.error(f"SentenceTransformerTrainer åˆ›å»ºå¤±è´¥: {e}")
            raise
    
    def create_evaluator(self, eval_dataset: Any) -> Optional[Any]:
        """
        Create evaluator for embedding model evaluation.
        
        Args:
            eval_dataset: Evaluation dataset
            
        Returns:
            Evaluator instance or None if no evaluation needed
        """
        logger.info("åˆ›å»º embedding è¯„ä¼°å™¨")
        
        try:
            # Use the unified evaluator factory with correct API
            evaluator = self.evaluator_factory.create_evaluator(
                dataset=eval_dataset,
                target_column="score",  # Default for embedding
                name=f"embedding_evaluator"
            )
            
            if evaluator:
                logger.info("Embedding è¯„ä¼°å™¨åˆ›å»ºæˆåŠŸ")
            else:
                logger.info("æ— éœ€åˆ›å»ºè¯„ä¼°å™¨")
            
            return evaluator
            
        except Exception as e:
            logger.warning(f"è¯„ä¼°å™¨åˆ›å»ºå¤±è´¥: {e}ï¼Œè·³è¿‡è¯„ä¼°")
            return None