"""Embeddingæ¨¡å‹è®­ç»ƒè„šæœ¬ï¼ˆå…¼å®¹æ€§ä¿ç•™ï¼‰

æ³¨æ„ï¼šå»ºè®®ä½¿ç”¨ç»Ÿä¸€çš„train.pyè„šæœ¬ï¼Œè®¾ç½®TRAIN_TYPE=embedding
è¯¥æ–‡ä»¶ä¸»è¦ç”¨äºå‘åå…¼å®¹
"""
from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments
from sentence_transformers.losses import CosineSimilarityLoss, ContrastiveLoss
from sentence_transformers.training_args import BatchSamplers
import os
from dotenv import load_dotenv
import logging
from datetime import datetime
from model_sft.utils.common_utils import init_swanlab, push_to_hub, create_embedding_loss
from model_sft.utils.data_loader import DataLoader
from model_sft.utils.evaluation import UnifiedEvaluator

# é…ç½®æ—¥å¿—
logging.basicConfig(format="%(asctime)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S", level=logging.INFO)
logger = logging.getLogger(__name__)

def train_func():
    """Embeddingæ¨¡å‹è®­ç»ƒå‡½æ•°"""
    logger.info("ğŸš¨ æ­£åœ¨æ‰§è¡Œæ—§ç‰ˆè®­ç»ƒè„šæœ¬ trainers/train_embedding.py")
    load_dotenv()
    init_swanlab()

    # 1. æ¨¡å‹é…ç½®
    model_name = os.getenv("MODEL_NAME_OR_PATH", "distilbert-base-uncased")
    clean_model_name = model_name.replace("/", "-")
    output_dir = os.getenv("OUTPUT_DIR", 
        f"output/training_stsbenchmark_{clean_model_name}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}"
    )
    
    logger.info(f"å¼€å§‹Embeddingæ¨¡å‹è®­ç»ƒ: {model_name}")
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = SentenceTransformer(model_name)

    # 2. æ•°æ®åŠ è½½å’ŒéªŒè¯
    data_loader = DataLoader()
    train_dataset, eval_dataset, test_dataset = data_loader.load_all_splits()
    
    data_loader.validate_dataset(train_dataset)
    target_column = data_loader.get_target_column(train_dataset)
    
    logger.info(f"è®­ç»ƒæ•°æ®é›†: {train_dataset}")
    logger.info(f"ç›®æ ‡åˆ—: {target_column}")

    # 3. æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©æŸå¤±å‡½æ•°ï¼ˆæ”¯æŒå¤šæ•°æ®é›†ï¼‰
    try:
        if data_loader.is_multi_dataset(train_dataset):
            # å¤šæ•°æ®é›†ï¼šä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºå¯¹åº”æŸå¤±å‡½æ•°
            losses = {}
            for dataset_name, dataset in train_dataset.items():
                losses[dataset_name] = create_embedding_loss(model, dataset, target_column, dataset_name)
            loss = losses
            logger.info(f"ä¸ºå¤šä¸ªæ•°æ®é›†åˆ›å»ºäº†æŸå¤±å‡½æ•°: {list(losses.keys())}")
        else:
            # å•æ•°æ®é›†ï¼šåˆ›å»ºå•ä¸ªæŸå¤±å‡½æ•°
            loss = create_embedding_loss(model, train_dataset, target_column)
    except Exception as e:
        logger.warning(f"åˆ›å»ºæŸå¤±å‡½æ•°æ—¶å‡ºé”™: {e}ï¼Œé»˜è®¤ä½¿ç”¨CosineSimilarityLoss")
        loss = CosineSimilarityLoss(model)

    # 4. è®­ç»ƒé…ç½®
    short_model_name = model_name.split("/")[-1] if "/" in model_name else model_name
    run_name = f"embedding-{short_model_name}"

    args = SentenceTransformerTrainingArguments(
        output_dir=os.getenv("OUTPUT_DIR", output_dir),
        num_train_epochs=int(os.getenv("NUM_TRAIN_EPOCHS", "3")),
        per_device_train_batch_size=int(os.getenv("PER_DEVICE_TRAIN_BATCH_SIZE", "16")),
        per_device_eval_batch_size=int(os.getenv("PER_DEVICE_EVAL_BATCH_SIZE", "16")),
        learning_rate=float(os.getenv("LEARNING_RATE", "2e-5")),
        warmup_ratio=float(os.getenv("WARMUP_RATIO", "0.1")),
        lr_scheduler_type=os.getenv("LR_SCHEDULER_TYPE", "linear"),
        fp16=os.getenv("BF16", "False").lower() == "false",
        bf16=os.getenv("BF16", "False").lower() == "true",
        batch_sampler=BatchSamplers.NO_DUPLICATES,
        eval_strategy=os.getenv("EVAL_STRATEGY", "steps"),
        eval_steps=os.getenv("EVAL_STEPS", None),
        save_strategy=os.getenv("SAVE_STRATEGY", "steps"),
        save_steps=int(os.getenv("SAVE_STEPS", 500)),
        save_total_limit=int(os.getenv("SAVE_TOTAL_LIMIT", None)),
        logging_strategy=os.getenv("LOGGING_STRATEGY", "steps"),
        logging_steps=int(os.getenv("LOGGING_STEPS", 500)),
        run_name=run_name,
        gradient_accumulation_steps=int(os.getenv("GRADIENT_ACCUMULATION_STEPS", "1")),
        report_to=os.getenv("REPORT_TO", "none"),
        max_steps=int(os.getenv("MAX_STEPS", "-1")),
    )

    # 5. åˆ›å»ºè¯„ä¼°å™¨
    evaluator_factory = UnifiedEvaluator("embedding")
    evaluators = evaluator_factory.create_evaluators_from_datasets(
        eval_dataset, test_dataset, target_column, run_name
    )

    # 6. åŸºçº¿æ¨¡å‹è¯„ä¼°
    dev_evaluator = None
    if eval_dataset is not None and 'dev' in evaluators:
        dev_evaluator = evaluators['dev']
        dev_results = evaluator_factory.evaluate_model(model, dev_evaluator)
        logger.info(f"åŸºçº¿æ¨¡å‹éªŒè¯ç»“æœ: {dev_results}")

    # 7. åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer_kwargs = {
        "model": model,
        "args": args,
        "train_dataset": train_dataset,
        "loss": loss,
    }
    
    # æ·»åŠ å¯é€‰å‚æ•°
    if eval_dataset is not None:
        trainer_kwargs["eval_dataset"] = eval_dataset
    if dev_evaluator is not None:
        trainer_kwargs["evaluator"] = dev_evaluator
        
    trainer = SentenceTransformerTrainer(**trainer_kwargs)
    trainer.train()
        
    # 8. æµ‹è¯•é›†è¯„ä¼°
    if test_dataset is not None and 'test' in evaluators:
        test_evaluator = evaluators['test']
        test_results = evaluator_factory.evaluate_model(model, test_evaluator)
        logger.info(f"è®­ç»ƒåæ¨¡å‹æµ‹è¯•ç»“æœ: {test_results}")

    # 9. ä¿å­˜è®­ç»ƒåçš„æ¨¡å‹
    save_dir = os.path.join(args.output_dir, "final_model")
    os.makedirs(save_dir, exist_ok=True)
    model.save_pretrained(save_dir)
    logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_dir}")

    # 10. æ¨é€åˆ°Hugging Face Hubï¼ˆå¯é€‰ï¼‰
    # push_to_hub(model, model_name, save_dir)
    
    return model, save_dir

if __name__ == "__main__":
    train_func()

