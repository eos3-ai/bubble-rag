"""
ç»Ÿä¸€çš„æ¨¡å‹è®­ç»ƒå…¥å£è„šæœ¬
é€šè¿‡ TRAIN_TYPE ç¯å¢ƒå˜é‡æ§åˆ¶è®­ç»ƒç±»å‹ï¼Œåªåœ¨è®­ç»ƒé€»è¾‘å±‚åŒºåˆ†ä¸åŒæ¨¡å‹
"""
import os
import logging
from dotenv import load_dotenv
from datetime import datetime
from bubble_rag.training.mysql_service.service.training_dataset_service import TrainingDatasetService
from bubble_rag.training.mysql_service.service.training_task_service import training_task_service
from bubble_rag.training.mysql_service.entity.training_dataset_models import DatasetInfo
from bubble_rag.training.mysql_service.entity.training_task_models import safe_get_session
from .enums import TrainingStatus

# ğŸ”§ å­è¿›ç¨‹æ¨¡å¼ï¼šCUDA_VISIBLE_DEVICESå·²åœ¨å­è¿›ç¨‹å¯åŠ¨å‰è®¾ç½®å¥½
# æ— éœ€æ‰‹åŠ¨é‡ç½®CUDAä¸Šä¸‹æ–‡ï¼Œå­è¿›ç¨‹é¦–æ¬¡å¯¼å…¥torchæ—¶ä¼šè‡ªåŠ¨è¯»å–æ­£ç¡®çš„ç¯å¢ƒå˜é‡
print(f"ğŸ”§ è®­ç»ƒå­è¿›ç¨‹å¯åŠ¨ï¼ŒCUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'æœªè®¾ç½®')}")

# ç°åœ¨æ‰å¯¼å…¥torchç›¸å…³åº“
# æ³¨æ„ï¼šBatchSamplers å’Œå…¶ä»–å·¥å…·ç±»éœ€è¦å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…è¿‡æ—©åˆå§‹åŒ–torch

# è®¾ç½®æ—¥å¿—
logging.basicConfig(format="%(asctime)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S", level=logging.INFO)
logger = logging.getLogger(__name__)


def _get_training_device() -> str:
    """
    è·å–è®­ç»ƒè®¾å¤‡é…ç½®ï¼ˆç¯å¢ƒå˜é‡å·²åœ¨APIå±‚è®¾ç½®å®Œæˆï¼‰
    
    Returns:
        è®¾å¤‡å­—ç¬¦ä¸²ï¼Œ'cpu' æˆ– 'cuda'
    """
    cuda_visible = os.getenv("CUDA_VISIBLE_DEVICES")
    
    if cuda_visible:
        # æœ‰CUDA_VISIBLE_DEVICESï¼Œä½¿ç”¨GPUè®­ç»ƒ
        import torch
        if torch.cuda.is_available():
            visible_gpus = cuda_visible.split(",")
            gpu_count = torch.cuda.device_count()  # è¿™æ˜¯å¯è§çš„GPUæ•°é‡ï¼Œä¸æ˜¯ç³»ç»Ÿæ€»æ•°
            logger.info(f"ğŸ”§ CUDA_VISIBLE_DEVICES: {cuda_visible}")
            logger.info(f"ğŸ–¥ï¸  å°†ä½¿ç”¨ {len(visible_gpus)} ä¸ªæŒ‡å®šçš„GPUè¿›è¡Œè®­ç»ƒ")
            
            # æ˜¾ç¤ºæ¯ä¸ªå¯è§GPUçš„è¯¦ç»†ä¿¡æ¯
            for i in range(gpu_count):  # éå†torchèƒ½çœ‹åˆ°çš„GPU
                try:
                    gpu_name = torch.cuda.get_device_name(i)
                    gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                    original_id = visible_gpus[i] if i < len(visible_gpus) else "unknown"
                    logger.info(f"   GPU {i} (åŸå§‹GPU{original_id}): {gpu_name} ({gpu_memory:.1f}GB)")
                except Exception as e:
                    logger.info(f"   GPU {i}: ä¿¡æ¯è·å–å¤±è´¥ - {e}")
            return "cuda"
        else:
            logger.warning("âš ï¸  è®¾ç½®äº†CUDA_VISIBLE_DEVICESä½†ç³»ç»Ÿä¸æ”¯æŒCUDAï¼Œå›é€€åˆ°CPU")
            return "cpu"
    else:
        # æ²¡æœ‰CUDA_VISIBLE_DEVICESï¼Œå¯èƒ½æ˜¯CPUæ¨¡å¼æˆ–autoæ¨¡å¼
        import torch
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            logger.info(f"ğŸ–¥ï¸  æœªæŒ‡å®šCUDA_VISIBLE_DEVICESï¼Œæ£€æµ‹åˆ° {gpu_count} ä¸ªGPUå¯ç”¨")
            for i in range(gpu_count):
                try:
                    gpu_name = torch.cuda.get_device_name(i)
                    gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                    logger.info(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
                except:
                    logger.info(f"   GPU {i}: ä¿¡æ¯è·å–å¤±è´¥")
            return "cuda"
        else:
            logger.info("ğŸ–¥ï¸  ç³»ç»Ÿä¸æ”¯æŒCUDAï¼Œä½¿ç”¨CPUè®­ç»ƒ")
            return "cpu"


def _prepare_model_for_training(model, device: str):
    """
    å‡†å¤‡æ¨¡å‹è¿›è¡Œè®­ç»ƒ
    
    Args:
        model: è®­ç»ƒæ¨¡å‹
        device: è®¾å¤‡ï¼Œå¦‚ 'cuda' æˆ– 'cpu'
        
    Returns:
        é…ç½®å¥½çš„æ¨¡å‹
    """
    import torch
    
    try:
        # æ£€æŸ¥å¹¶ç§»é™¤ä»»ä½•DataParallelåŒ…è£…
        if hasattr(model, '_modules'):
            for module_name, module in model._modules.items():
                if isinstance(module, torch.nn.DataParallel):
                    # æå–åŸå§‹æ¨¡å—ï¼Œç§»é™¤DataParallelåŒ…è£…
                    original_module = module.module
                    model._modules[module_name] = original_module
                    logger.info(f"ğŸ”“ ç§»é™¤æ¨¡å— {module_name} çš„DataParallelåŒ…è£…")
        
        logger.info(f"âœ… æ¨¡å‹è®­ç»ƒå‡†å¤‡å®Œæˆ")
        return model
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹å‡†å¤‡å¤±è´¥: {str(e)}")
        logger.warning(f"âš ï¸  å°†ç»§ç»­ä½¿ç”¨åŸå§‹æ¨¡å‹é…ç½®")
        return model


def _create_training_config(output_dir: str, run_name: str, training_params: dict = None) -> dict:
    """
    åˆ›å»ºè®­ç»ƒé…ç½®å­—å…¸ï¼Œå®Œå…¨ä½¿ç”¨å‚æ•°ä¼ é€’æ–¹å¼ï¼Œé¿å…ç¯å¢ƒå˜é‡ä¾èµ–
    
    Args:
        output_dir: è¾“å‡ºç›®å½•
        run_name: è¿è¡Œåç§°
        training_params: è®­ç»ƒå‚æ•°å­—å…¸ï¼ˆæ¥è‡ªæ¥å£ä¼ å‚ï¼‰
        
    Returns:
        è®­ç»ƒé…ç½®å­—å…¸
    """
    from .models.training_parameters import TrainingParametersManager
    
    # è¿è¡Œæ—¶å‚æ•°ï¼Œä¸èƒ½é€šè¿‡å‚æ•°ç®¡ç†å™¨é…ç½®
    runtime_params = {
        "output_dir": output_dir,
        "run_name": run_name,
    }
    
    # å‡†å¤‡ä¼ é€’ç»™å‚æ•°ç®¡ç†å™¨çš„é…ç½®
    if training_params:
        # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„æ‰€æœ‰å‚æ•°ï¼Œå®Œå…¨é¿å…ç¯å¢ƒå˜é‡
        logger.info(f"ä½¿ç”¨æ¥å£ä¼ å…¥çš„è®­ç»ƒå‚æ•°: {list(training_params.keys())}")
        param_config = dict(training_params)  # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸå‚æ•°
    else:
        # å¦‚æœæ²¡æœ‰ä¼ å…¥å‚æ•°ï¼Œä½¿ç”¨ç©ºé…ç½®ï¼ˆå°†ä½¿ç”¨TrainingParametersçš„é»˜è®¤å€¼ï¼‰
        logger.info("æœªä¼ å…¥è®­ç»ƒå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        param_config = {}
    
    try:
        # åˆ›å»ºå‚æ•°ç®¡ç†å™¨ï¼Œç›´æ¥ä»é…ç½®å­—å…¸åŠ è½½ï¼ˆä¸ä¾èµ–ç¯å¢ƒå˜é‡ï¼‰
        param_manager = TrainingParametersManager()
        param_manager.load_from_config(param_config)
        
        # è·å–éªŒè¯è¿‡çš„å‚æ•°å­—å…¸
        config = param_manager.get_training_args_dict()
        
        # æ·»åŠ è¿è¡Œæ—¶å‚æ•°
        config.update(runtime_params)
        
        # ç‰¹æ®Šå¤„ç†ç”¨æˆ·æ—¥å¿—ç›®å½•
        user_logging_dir = training_params.get("user_logging_dir") if training_params else None
        if user_logging_dir and not config.get("logging_dir"):
            config["logging_dir"] = user_logging_dir
            logger.info(f"ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ—¥å¿—ç›®å½•: {user_logging_dir}")
            
        logger.info("âœ… è®­ç»ƒé…ç½®åˆ›å»ºæˆåŠŸï¼ˆçº¯å‚æ•°ä¼ é€’æ¨¡å¼ï¼‰")
        return config
        
    except Exception as e:
        logger.error(f"è®­ç»ƒå‚æ•°éªŒè¯å¤±è´¥: {e}")
        logger.warning("å›é€€åˆ°åŸºç¡€é…ç½®æ¨¡å¼")
        
        # å›é€€åˆ°åŸºç¡€é…ç½®ï¼ˆä¸ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œåªæœ‰å¿…è¦çš„é»˜è®¤å€¼ï¼‰
        return _create_basic_training_config(output_dir, run_name, training_params)


def _create_training_config_legacy(output_dir: str, run_name: str) -> dict:
    """
    æ—§ç‰ˆè®­ç»ƒé…ç½®åˆ›å»ºæ–¹æ³•ï¼ˆå‘åå…¼å®¹ï¼‰
    
    Args:
        output_dir: è¾“å‡ºç›®å½•
        run_name: è¿è¡Œåç§°
        
    Returns:
        è®­ç»ƒé…ç½®å­—å…¸
    """
    # é¡¹ç›®è‡ªå®šä¹‰çš„ç¯å¢ƒå˜é‡ï¼Œä¸æ˜¯HuggingFaceè®­ç»ƒå‚æ•°ï¼Œéœ€è¦æ’é™¤
    project_env_vars = {
        "TRAIN_TYPE", "MODEL_NAME_OR_PATH", "OUTPUT_DIR", "SAMPLE_SIZE",
        "DATASET_NAME_OR_PATH", "DEVICE",
        # å‘åå…¼å®¹çš„ç¯å¢ƒå˜é‡
        "TRAIN_DATASET", "EVAL_DATASET", "TEST_DATASET",
        # SwanLabç›¸å…³ç¯å¢ƒå˜é‡
        "SWANLAB_API_KEY", "SWANLAB_WORKSPACE", "SWANLAB_PROJECT", 
        "SWANLAB_EXPERIMENT", "SWANLAB_MODE"
    }
    
    # åŸºç¡€é…ç½®ï¼Œåªè®¾ç½®è¿è¡Œæ—¶ç¡®å®šçš„å‚æ•°
    config = {
        "output_dir": output_dir,
        "run_name": run_name,
    }
    
    # åªæ·»åŠ HuggingFaceè®­ç»ƒç›¸å…³çš„ç¯å¢ƒå˜é‡
    # å®šä¹‰HuggingFaceè®­ç»ƒå‚æ•°çš„å‰ç¼€å’Œå·²çŸ¥å‚æ•°
    hf_training_params = {
        "NUM_TRAIN_EPOCHS", "PER_DEVICE_TRAIN_BATCH_SIZE", "PER_DEVICE_EVAL_BATCH_SIZE",
        "LEARNING_RATE", "WARMUP_RATIO", "LR_SCHEDULER_TYPE", "BF16", "FP16",
        "EVAL_STRATEGY", "EVAL_STEPS", "SAVE_STRATEGY", "SAVE_STEPS", "SAVE_TOTAL_LIMIT",
        "LOGGING_STEPS", "LOGGING_STRATEGY", "LOGGING_DIR", "GRADIENT_ACCUMULATION_STEPS", "MAX_STEPS",
        "BATCH_SAMPLER", "DATALOADER_NUM_WORKERS", "WEIGHT_DECAY", "ADAM_BETA1", "ADAM_BETA2",
        "ADAM_EPSILON", "MAX_GRAD_NORM", "SEED", "DATALOADER_DROP_LAST", "EVAL_ACCUMULATION_STEPS",
        "LOAD_BEST_MODEL_AT_END", "METRIC_FOR_BEST_MODEL", "GREATER_IS_BETTER", "IGNORE_DATA_SKIP",
        "RESUME_FROM_CHECKPOINT", "PUSH_TO_HUB", "HUB_MODEL_ID", "HUB_STRATEGY", "HUB_TOKEN",
        "PREDICTION_LOSS_ONLY", "REMOVE_UNUSED_COLUMNS", "LABEL_NAMES", "LOCAL_RANK", "DEEPSPEED",
        "OPTIM", "GROUP_BY_LENGTH", "LENGTH_COLUMN_NAME", "REPORT_TO", "DDPBACKEND"
    }
    
    # å®šä¹‰éœ€è¦ç±»å‹è½¬æ¢çš„å‚æ•°
    int_params = {
        "num_train_epochs", "per_device_train_batch_size", "per_device_eval_batch_size",
        "gradient_accumulation_steps", "eval_steps", "save_steps", "save_total_limit",
        "logging_steps", "max_steps", "dataloader_num_workers", "eval_accumulation_steps",
        "seed", "local_rank"
    }
    
    float_params = {
        "learning_rate", "warmup_ratio", "weight_decay", "adam_beta1", "adam_beta2",
        "adam_epsilon", "max_grad_norm"
    }
    
    bool_params = {
        "bf16", "fp16", "dataloader_drop_last", "load_best_model_at_end",
        "greater_is_better", "ignore_data_skip", "push_to_hub", "prediction_loss_only",
        "remove_unused_columns", "group_by_length"
    }

    # æ·»åŠ ç¬¦åˆæ¡ä»¶çš„ç¯å¢ƒå˜é‡
    for key, value in os.environ.items():
        key_upper = key.upper()
        if (key_upper not in project_env_vars and 
            (key_upper in hf_training_params or 
             key.lower() in ['run_name', 'output_dir'])):
            # å°†ç¯å¢ƒå˜é‡åè½¬æ¢ä¸ºè®­ç»ƒå‚æ•°åï¼ˆå°å†™ï¼‰
            param_name = key.lower()
            
            # ç±»å‹è½¬æ¢
            try:
                if param_name in int_params:
                    config[param_name] = int(value)
                elif param_name in float_params:
                    config[param_name] = float(value)
                elif param_name in bool_params:
                    # å¤„ç†å¸ƒå°”å€¼
                    config[param_name] = value.lower() in ('true', '1', 'yes', 'on')
                else:
                    config[param_name] = value
            except (ValueError, TypeError) as e:
                logger.warning(f"æ— æ³•è½¬æ¢å‚æ•° {param_name} çš„å€¼ '{value}': {e}")
                config[param_name] = value
            
            logger.info(f"ä¼ é€’è®­ç»ƒå‚æ•°: {param_name} = {config[param_name]}")
    
    # æ³¨æ„ï¼šLegacyå‡½æ•°ä¸å†ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œé¿å…ç¯å¢ƒæ±¡æŸ“
    # report_to å‚æ•°åº”è¯¥é€šè¿‡å‚æ•°ä¼ é€’æ–¹å¼è·å–ï¼Œè€Œä¸æ˜¯ç¯å¢ƒå˜é‡
    
    return config


def _create_basic_training_config(output_dir: str, run_name: str, training_params: dict = None) -> dict:
    """
    åˆ›å»ºåŸºç¡€è®­ç»ƒé…ç½®ï¼Œå®Œå…¨ä¸ä¾èµ–ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨ä¼ å…¥å‚æ•°å’Œåˆç†é»˜è®¤å€¼
    
    Args:
        output_dir: è¾“å‡ºç›®å½•
        run_name: è¿è¡Œåç§°
        training_params: è®­ç»ƒå‚æ•°å­—å…¸
        
    Returns:
        è®­ç»ƒé…ç½®å­—å…¸
    """
    # åŸºç¡€é»˜è®¤é…ç½®
    config = {
        # è¿è¡Œæ—¶å‚æ•°
        "output_dir": output_dir,
        "run_name": run_name,
        
        # è®­ç»ƒåŸºç¡€å‚æ•°é»˜è®¤å€¼
        "num_train_epochs": 3,
        "per_device_train_batch_size": 8,
        "per_device_eval_batch_size": 8,
        "learning_rate": 5e-5,
        "warmup_ratio": 0.1,
        "lr_scheduler_type": "linear",
        
        # è¯„ä¼°å’Œä¿å­˜ç­–ç•¥
        "eval_strategy": "steps",
        "eval_steps": 100,
        "save_strategy": "steps", 
        "save_steps": 100,
        "save_total_limit": 3,
        
        # æ—¥å¿—é…ç½® - å°†ä»ç”¨æˆ·å‚æ•°ä¸­è¦†ç›–
        "logging_steps": 100,  # é»˜è®¤å€¼ï¼Œä¼šè¢«ç”¨æˆ·å‚æ•°è¦†ç›–
        "logging_strategy": "steps",
        
        # ä¼˜åŒ–å™¨é…ç½®
        "optim": "adamw_hf",
        "weight_decay": 0.01,
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_epsilon": 1e-8,
        "max_grad_norm": 1.0,
        
        # æ•°æ®åŠ è½½
        "dataloader_num_workers": 0,
        "dataloader_drop_last": False,
        
        # å…¶ä»–é…ç½®
        "seed": 42,
        "bf16": False,
        "fp16": False,
        "gradient_accumulation_steps": 1,
        "remove_unused_columns": True,
        "prediction_loss_only": True,
        "load_best_model_at_end": True,
        "metric_for_best_model": "eval_loss",
        "greater_is_better": False,
    }
    
    # å¦‚æœæœ‰ä¼ å…¥çš„å‚æ•°ï¼Œä½¿ç”¨ä¼ å…¥çš„å‚æ•°è¦†ç›–é»˜è®¤å€¼
    if training_params:
        logger.info(f"ä½¿ç”¨ä¼ å…¥å‚æ•°è¦†ç›–é»˜è®¤é…ç½®: {list(training_params.keys())}")
        for key, value in training_params.items():
            if value is not None:  # åªè¦†ç›–éNoneçš„å€¼
                config[key] = value
                logger.debug(f"å‚æ•°è¦†ç›–: {key} = {value}")
    
    logger.info("âœ… åŸºç¡€è®­ç»ƒé…ç½®åˆ›å»ºæˆåŠŸï¼ˆçº¯å‚æ•°æ¨¡å¼ï¼Œæ— ç¯å¢ƒå˜é‡ä¾èµ–ï¼‰")
    return config


def _initialize_model_and_loss(train_type: str, model_name: str, train_dataset, target_column: str, training_config: dict):
    """
    æ ¹æ®è®­ç»ƒç±»å‹åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œè®­ç»ƒå‚æ•°ï¼Œæ”¯æŒå¤šæ•°æ®é›†å’Œå¤šæŸå¤±å‡½æ•°
    
    Args:
        train_type: è®­ç»ƒç±»å‹ ('embedding' æˆ– 'reranker')
        model_name: æ¨¡å‹åç§°
        train_dataset: è®­ç»ƒæ•°æ®é›†ï¼ˆå¯èƒ½æ˜¯å•ä¸ªDatasetæˆ–Dict[str, Dataset]ï¼‰
        target_column: ç›®æ ‡åˆ—å
        training_config: è®­ç»ƒé…ç½®
        
    Returns:
        (model, loss, args) å…ƒç»„ï¼Œå…¶ä¸­losså¯èƒ½æ˜¯å•ä¸ªæŸå¤±å‡½æ•°æˆ–Dict[str, æŸå¤±å‡½æ•°]
    """
    from .utils.data_loader import DataLoader
    hf_subset = training_config.get('HF_subset') if training_config else None
    data_loader = DataLoader(
        hf_subset=hf_subset,
        train_sample_size=training_config.get('train_sample_size', 0),
        eval_sample_size=training_config.get('eval_sample_size', 0),
        test_sample_size=training_config.get('test_sample_size', 0)
    )
    is_multi_dataset = data_loader.is_multi_dataset(train_dataset)
    
    if train_type == "embedding":
        from sentence_transformers import SentenceTransformer, SentenceTransformerTrainingArguments
        from sentence_transformers.losses import CosineSimilarityLoss, ContrastiveLoss, MultipleNegativesRankingLoss
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹
        try:
            # è·å–è®¾å¤‡é…ç½®å¹¶éªŒè¯CUDAç¯å¢ƒ
            device = _get_training_device()
            
            # åˆ›å»ºæ¨¡å‹å‰å†æ¬¡éªŒè¯è®¾å¤‡é…ç½®
            import os
            cuda_visible = os.getenv("CUDA_VISIBLE_DEVICES")
            logger.info(f"ğŸ”§ æ¨¡å‹åˆå§‹åŒ–å‰çš„è®¾å¤‡æ£€æŸ¥:")
            logger.info(f"   CUDA_VISIBLE_DEVICES: {cuda_visible}")
            logger.info(f"   è®¡ç®—å¾—åˆ°çš„è®¾å¤‡: {device}")
            
            # å°è¯•ç”¨ModelScopeä¸‹è½½æ¨¡å‹åˆ°ç»Ÿä¸€ç¼“å­˜ç›®å½•ï¼Œç„¶åç”¨SentenceTransformeråŠ è½½
            try:
                from modelscope import snapshot_download
                from bubble_rag.server_config import TRAINING_CACHE
                
                logger.info(f"å°è¯•ä»ModelScopeä¸‹è½½æ¨¡å‹: {model_name}")
                model_dir = snapshot_download(model_name, cache_dir=TRAINING_CACHE)
                logger.info(f"âœ… ModelScopeä¸‹è½½æˆåŠŸï¼Œè·¯å¾„: {model_dir}")
                
                # ä»æœ¬åœ°è·¯å¾„åŠ è½½
                model = SentenceTransformer(model_dir)
                logger.info(f"âœ… embeddingæ¨¡å‹ä»ModelScopeç¼“å­˜åˆå§‹åŒ–æˆåŠŸ: {model_name}")
            except ImportError:
                logger.info("ModelScopeæœªå®‰è£…ï¼Œä½¿ç”¨HuggingFaceæ–¹å¼")
                model = SentenceTransformer(model_name)
                logger.info(f"âœ… embeddingæ¨¡å‹ä»HuggingFaceåˆå§‹åŒ–æˆåŠŸ: {model_name}")
            except Exception as ms_error:
                logger.warning(f"ModelScopeä¸‹è½½å¤±è´¥: {ms_error}ï¼Œå›é€€åˆ°HuggingFace")
                model = SentenceTransformer(model_name)
                logger.info(f"âœ… embeddingæ¨¡å‹ä»HuggingFaceåˆå§‹åŒ–æˆåŠŸ: {model_name}")
            
            # éªŒè¯æ¨¡å‹å®é™…ä½¿ç”¨çš„è®¾å¤‡
            if hasattr(model, 'device'):
                logger.info(f"ğŸ–¥ï¸  æ¨¡å‹å®é™…æ‰€åœ¨è®¾å¤‡: {model.device}")
            else:
                logger.info(f"ğŸ–¥ï¸  æ¨¡å‹è®¾å¤‡ä¿¡æ¯: æ— æ³•ç›´æ¥è·å–")
            
            # å‡†å¤‡æ¨¡å‹è¿›è¡Œè®­ç»ƒ
            model = _prepare_model_for_training(model, device)
        except Exception as e:
            # å¦‚æœç½‘ç»œå¤±è´¥ï¼Œå°è¯•æœ¬åœ°ç¼“å­˜æ¨¡å¼
            if "couldn't connect" in str(e).lower() or "connection" in str(e).lower():
                logger.warning(f"ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°ç¼“å­˜åŠ è½½æ¨¡å‹: {model_name}")
                try:
                    # è®¾ç½®ç¦»çº¿æ¨¡å¼ç¯å¢ƒå˜é‡
                    import os
                    os.environ["TRANSFORMERS_OFFLINE"] = "1"
                    model = SentenceTransformer(model_name, local_files_only=True)
                    logger.info(f"âœ… embeddingæ¨¡å‹ä»æœ¬åœ°ç¼“å­˜åˆå§‹åŒ–æˆåŠŸ: {model_name}")
                except Exception as cache_error:
                    error_msg = f"embeddingæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {model_name}ï¼Œç½‘ç»œä¸å¯ç”¨ä¸”æœ¬åœ°ç¼“å­˜æœªæ‰¾åˆ°ã€‚é”™è¯¯: {str(cache_error)}"
                    logger.error(error_msg)
                    raise RuntimeError(error_msg)
            else:
                error_msg = f"embeddingæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {model_name}, é”™è¯¯: {str(e)}"
                logger.error(error_msg)
                raise RuntimeError(error_msg)
        
        if is_multi_dataset:
            # å¤šæ•°æ®é›†ï¼šå¯¹æ¯ä¸ªæ•°æ®é›†åº”ç”¨åˆ—è¿‡æ»¤å¹¶åˆ›å»ºå¯¹åº”çš„æŸå¤±å‡½æ•°
            from .utils.evaluation import UnifiedEvaluator
            temp_evaluator_factory = UnifiedEvaluator(train_type)
            
            losses = {}
            filtered_train_dataset = {}
            
            for dataset_name, dataset in train_dataset.items():
                # ä¸ºæ¯ä¸ªæ•°æ®é›†å•ç‹¬ç¡®å®šç›®æ ‡åˆ—å
                dataset_target_column = temp_evaluator_factory._get_dataset_target_column(dataset)
                
                # è¿‡æ»¤æ•°æ®é›†åˆ—ï¼šåªä¿ç•™å‰ä¸¤åˆ—ä½œä¸ºè¾“å…¥åˆ— + ç›®æ ‡åˆ—
                column_names = dataset.column_names
                if len(column_names) >= 3:
                    # ç¡®ä¿åªæœ‰3åˆ—ï¼šsentence1, sentence2, target_column
                    input_columns = [col for col in column_names if col != dataset_target_column][:2]
                    columns_to_keep = input_columns + [dataset_target_column]
                    filtered_dataset = dataset.select_columns(columns_to_keep)
                    logger.info(f"Embeddingæ•°æ®é›† '{dataset_name}' åˆ—è¿‡æ»¤: {column_names} â†’ {columns_to_keep}")
                else:
                    filtered_dataset = dataset
                
                filtered_train_dataset[dataset_name] = filtered_dataset
                
                # ä½¿ç”¨è¿‡æ»¤åçš„æ•°æ®é›†åˆ›å»ºæŸå¤±å‡½æ•°
                from .utils.common_utils import create_embedding_loss
                loss_func = create_embedding_loss(model, filtered_dataset, dataset_target_column, dataset_name)
                losses[dataset_name] = loss_func
            
            loss = losses
            # æ›´æ–°è®­ç»ƒæ•°æ®é›†ä¸ºè¿‡æ»¤åçš„ç‰ˆæœ¬
            train_dataset = filtered_train_dataset
            logger.info(f"ä¸ºå¤šä¸ªembeddingæ•°æ®é›†åˆ›å»ºäº†æŸå¤±å‡½æ•°: {list(losses.keys())}")
        else:
            # å•æ•°æ®é›†ï¼šåº”ç”¨åˆ—è¿‡æ»¤å¹¶åˆ›å»ºå•ä¸ªæŸå¤±å‡½æ•°
            from .utils.evaluation import UnifiedEvaluator
            temp_evaluator_factory = UnifiedEvaluator(train_type)
            dataset_target_column = temp_evaluator_factory._get_dataset_target_column(train_dataset)
            
            # è¿‡æ»¤æ•°æ®é›†åˆ—ï¼šåªä¿ç•™å‰ä¸¤åˆ—ä½œä¸ºè¾“å…¥åˆ— + ç›®æ ‡åˆ—
            column_names = train_dataset.column_names
            if len(column_names) >= 3:
                # ç¡®ä¿åªæœ‰3åˆ—ï¼šsentence1, sentence2, target_column
                input_columns = [col for col in column_names if col != dataset_target_column][:2]
                columns_to_keep = input_columns + [dataset_target_column]
                train_dataset = train_dataset.select_columns(columns_to_keep)
                logger.info(f"å•Embeddingæ•°æ®é›†åˆ—è¿‡æ»¤: {column_names} â†’ {columns_to_keep}")
            
            # ä½¿ç”¨è¿‡æ»¤åçš„æ•°æ®é›†åˆ›å»ºæŸå¤±å‡½æ•°
            from .utils.common_utils import create_embedding_loss
            loss = create_embedding_loss(model, train_dataset, dataset_target_column)
        
        # è¿‡æ»¤æ‰å¤šè¿›ç¨‹å’Œsample_sizeç›¸å…³çš„å‚æ•°ï¼Œè¿™äº›å‚æ•°ä¸å±äºè®­ç»ƒå‚æ•°ç±»
        filtered_config = {k: v for k, v in training_config.items() 
                          if k not in ['nproc_per_node', 'local_rank', 'master_port', 'master_addr', 
                                     'train_sample_size', 'eval_sample_size', 'test_sample_size']}
        
        # æ·»åŠ è¿›åº¦æ¡æ§åˆ¶ï¼Œé¿å…å¤šè¿›åº¦æ¡é‡å 
        filtered_config['disable_tqdm'] = False  # å¯ç”¨ä¸»è¿›åº¦æ¡
        
        args = SentenceTransformerTrainingArguments(**filtered_config)
        
    elif train_type == "reranker":
        from sentence_transformers.cross_encoder import CrossEncoder
        from sentence_transformers.cross_encoder.losses.BinaryCrossEntropyLoss import BinaryCrossEntropyLoss
        from sentence_transformers.cross_encoder.training_args import CrossEncoderTrainingArguments
        
        # åˆå§‹åŒ–rerankeræ¨¡å‹
        try:
            # è·å–è®¾å¤‡é…ç½®å¹¶éªŒè¯CUDAç¯å¢ƒ
            device = _get_training_device()
            
            # åˆ›å»ºæ¨¡å‹å‰å†æ¬¡éªŒè¯è®¾å¤‡é…ç½®
            import os
            cuda_visible = os.getenv("CUDA_VISIBLE_DEVICES")
            logger.info(f"ğŸ”§ rerankeræ¨¡å‹åˆå§‹åŒ–å‰çš„è®¾å¤‡æ£€æŸ¥:")
            logger.info(f"   CUDA_VISIBLE_DEVICES: {cuda_visible}")
            logger.info(f"   è®¡ç®—å¾—åˆ°çš„è®¾å¤‡: {device}")

            # å°è¯•ç”¨ModelScopeä¸‹è½½æ¨¡å‹åˆ°ç»Ÿä¸€ç¼“å­˜ç›®å½•ï¼Œç„¶åç”¨CrossEncoderåŠ è½½
            try:
                from modelscope import snapshot_download
                from bubble_rag.server_config import TRAINING_CACHE
                
                logger.info(f"å°è¯•ä»ModelScopeä¸‹è½½æ¨¡å‹: {model_name}")
                model_dir = snapshot_download(model_name, cache_dir=TRAINING_CACHE)
                logger.info(f"âœ… ModelScopeä¸‹è½½æˆåŠŸï¼Œè·¯å¾„: {model_dir}")
                
                # ä»æœ¬åœ°è·¯å¾„åŠ è½½
                model = CrossEncoder(model_dir, num_labels=1)
                logger.info(f"âœ… rerankeræ¨¡å‹ä»ModelScopeç¼“å­˜åˆå§‹åŒ–æˆåŠŸ: {model_name}")
            except ImportError:
                logger.info("ModelScopeæœªå®‰è£…ï¼Œä½¿ç”¨HuggingFaceæ–¹å¼")
                model = CrossEncoder(model_name, num_labels=1)
                logger.info("ğŸ“± ä½¿ç”¨ç¯å¢ƒå˜é‡æ§åˆ¶çš„GPUåˆå§‹åŒ–CrossEncoder")
            except Exception as ms_error:
                logger.warning(f"ModelScopeä¸‹è½½å¤±è´¥: {ms_error}ï¼Œå›é€€åˆ°HuggingFace")
                model = CrossEncoder(model_name, num_labels=1)
                logger.info("ğŸ“± ä½¿ç”¨ç¯å¢ƒå˜é‡æ§åˆ¶çš„GPUåˆå§‹åŒ–CrossEncoder")
            
            actual_device = getattr(model, 'device', 'unknown')
            logger.info(f"âœ… rerankeræ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {model_name}")
            logger.info(f"ğŸ–¥ï¸  æ¨¡å‹å®é™…æ‰€åœ¨è®¾å¤‡: {actual_device}")
            
            # é¢å¤–æ£€æŸ¥æ¨¡å‹çš„å†…éƒ¨è®¾å¤‡
            if hasattr(model, 'model') and hasattr(model.model, 'device'):
                logger.info(f"ğŸ–¥ï¸  æ¨¡å‹å†…éƒ¨deviceå±æ€§: {model.model.device}")
            
            # å‡†å¤‡æ¨¡å‹è¿›è¡Œè®­ç»ƒ
            model = _prepare_model_for_training(model, device)
        except Exception as e:
            # å¦‚æœæ˜¯ç½‘ç»œè¿æ¥ç›¸å…³é”™è¯¯ï¼Œå°è¯•ç¦»çº¿æ¨¡å¼
            error_str = str(e).lower()
            if any(keyword in error_str for keyword in ["connection", "network", "timeout", "huggingface.co", "could not connect"]):
                logger.warning(f"âš ï¸ ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°ç¼“å­˜: {str(e)}")
                try:
                    # ä½¿ç”¨æœ¬åœ°ç¼“å­˜æ¨¡å¼
                    model = CrossEncoder(model_name, num_labels=1,local_files_only=True)
                    logger.info(f"âœ… rerankeræ¨¡å‹ä½¿ç”¨æœ¬åœ°ç¼“å­˜åˆå§‹åŒ–æˆåŠŸ: {model_name}")
                except Exception as local_error:
                    error_msg = f"rerankeræ¨¡å‹åˆå§‹åŒ–å¤±è´¥(ç½‘ç»œå’Œæœ¬åœ°ç¼“å­˜éƒ½å¤±è´¥): {model_name}, ç½‘ç»œé”™è¯¯: {str(e)}, æœ¬åœ°é”™è¯¯: {str(local_error)}"
                    logger.error(error_msg)
                    raise RuntimeError(error_msg)
            else:
                error_msg = f"rerankeræ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {model_name}, é”™è¯¯: {str(e)}"
                logger.error(error_msg)
                raise RuntimeError(error_msg)
        
        # å¤„ç†tokenizer pad token
        if model.tokenizer.pad_token is None:
            model.tokenizer.pad_token = model.tokenizer.eos_token
        model.model.config.pad_token_id = model.tokenizer.pad_token_id
        
        if is_multi_dataset:
            # å¤šæ•°æ®é›†ï¼šæ ¹æ®æ¯ä¸ªæ•°æ®é›†çš„ä»»åŠ¡ç±»å‹é€‰æ‹©æŸå¤±å‡½æ•°
            from sentence_transformers.cross_encoder.losses.MSELoss import MSELoss
            from .utils.evaluation import UnifiedEvaluator
            
            # åˆ›å»ºä¸´æ—¶çš„evaluator_factoryæ¥è·å–ç›®æ ‡åˆ—å
            temp_evaluator_factory = UnifiedEvaluator(train_type)
            
            losses = {}
            filtered_train_dataset = {}
            
            for dataset_name, dataset in train_dataset.items():
                # ä¸ºæ¯ä¸ªæ•°æ®é›†å•ç‹¬ç¡®å®šç›®æ ‡åˆ—åå’Œä»»åŠ¡ç±»å‹
                dataset_target_column = temp_evaluator_factory._get_dataset_target_column(dataset)
                labels = list(dataset[dataset_target_column])
                unique_labels = set(labels)
                
                # è¿‡æ»¤æ•°æ®é›†åˆ—ï¼šåªä¿ç•™å‰ä¸¤åˆ—ä½œä¸ºè¾“å…¥åˆ— + ç›®æ ‡åˆ—
                column_names = dataset.column_names
                if len(column_names) >= 3:
                    # ç¡®ä¿åªæœ‰3åˆ—ï¼šsentence1, sentence2, target_column
                    input_columns = [col for col in column_names if col != dataset_target_column][:2]
                    columns_to_keep = input_columns + [dataset_target_column]
                    filtered_dataset = dataset.select_columns(columns_to_keep)
                    logger.info(f"æ•°æ®é›† '{dataset_name}' åˆ—è¿‡æ»¤: {column_names} â†’ {columns_to_keep}")
                else:
                    filtered_dataset = dataset
                
                filtered_train_dataset[dataset_name] = filtered_dataset
                
                # åˆ¤æ–­ä»»åŠ¡ç±»å‹ï¼šåªæœ‰æ ‡ç­¾ä¸¥æ ¼ä¸º0/1æ—¶æ‰æ˜¯äºŒåˆ†ç±»
                is_binary_classification = (
                    len(unique_labels) <= 2 and 
                    all(label in [0, 1] for label in unique_labels)
                )
                
                if is_binary_classification:
                    losses[dataset_name] = BinaryCrossEntropyLoss(model)
                    logger.info(f"æ•°æ®é›† '{dataset_name}' ä½¿ç”¨BinaryCrossEntropyLossï¼ˆäºŒåˆ†ç±»ä»»åŠ¡ï¼Œæ ‡ç­¾: {sorted(unique_labels)}ï¼‰")
                else:
                    # å›å½’ä»»åŠ¡ä½¿ç”¨MSE Loss
                    # åœ¨åˆ›å»ºMSELosså‰éªŒè¯æ•°æ®é›†æ ¼å¼
                    mse_dataset_columns = len(filtered_dataset.column_names)
                    if mse_dataset_columns != 3:
                        logger.error(f"âŒ MSELossè¦æ±‚3åˆ—æ•°æ®é›†ï¼Œä½†æ•°æ®é›† '{dataset_name}' æœ‰{mse_dataset_columns}åˆ—: {filtered_dataset.column_names}")
                        raise ValueError(f"MSELossæ•°æ®é›†æ ¼å¼é”™è¯¯: {dataset_name}æœ‰{mse_dataset_columns}åˆ—è€Œé3åˆ—")
                    
                    losses[dataset_name] = MSELoss(model)
                    logger.info(f"æ•°æ®é›† '{dataset_name}' ä½¿ç”¨MSELossï¼ˆå›å½’ä»»åŠ¡ï¼Œæ ‡ç­¾èŒƒå›´: {min(labels):.2f}-{max(labels):.2f}ï¼‰ï¼Œæ•°æ®é›†åˆ—æ•°: {mse_dataset_columns}")
            loss = losses
            # æ›´æ–°è®­ç»ƒæ•°æ®é›†ä¸ºè¿‡æ»¤åçš„ç‰ˆæœ¬
            train_dataset = filtered_train_dataset
        else:
            # å•æ•°æ®é›†ï¼šæ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©æŸå¤±å‡½æ•°
            from .utils.evaluation import UnifiedEvaluator
            temp_evaluator_factory = UnifiedEvaluator(train_type)
            dataset_target_column = temp_evaluator_factory._get_dataset_target_column(train_dataset)
            labels = list(train_dataset[dataset_target_column])
            unique_labels = set(labels)
            
            # è¿‡æ»¤æ•°æ®é›†åˆ—ï¼šåªä¿ç•™å‰ä¸¤åˆ—ä½œä¸ºè¾“å…¥åˆ— + ç›®æ ‡åˆ—
            column_names = train_dataset.column_names
            if len(column_names) >= 3:
                # ç¡®ä¿åªæœ‰3åˆ—ï¼šsentence1, sentence2, target_column
                input_columns = [col for col in column_names if col != dataset_target_column][:2]
                columns_to_keep = input_columns + [dataset_target_column]
                train_dataset = train_dataset.select_columns(columns_to_keep)
                logger.info(f"å•æ•°æ®é›†åˆ—è¿‡æ»¤: {column_names} â†’ {columns_to_keep}")
            
            is_binary_classification = (
                len(unique_labels) <= 2 and 
                all(label in [0, 1] for label in unique_labels)
            )
            
            if is_binary_classification:
                loss = BinaryCrossEntropyLoss(model)
                logger.info(f"ä½¿ç”¨BinaryCrossEntropyLossæŸå¤±å‡½æ•°ï¼ˆäºŒåˆ†ç±»ä»»åŠ¡ï¼Œæ ‡ç­¾: {sorted(unique_labels)}ï¼‰")
            else:
                # åœ¨åˆ›å»ºMSELosså‰éªŒè¯æ•°æ®é›†æ ¼å¼
                mse_dataset_columns = len(train_dataset.column_names)
                if mse_dataset_columns != 3:
                    logger.error(f"âŒ å•æ•°æ®é›†MSELossè¦æ±‚3åˆ—ï¼Œä½†æœ‰{mse_dataset_columns}åˆ—: {train_dataset.column_names}")
                    raise ValueError(f"å•æ•°æ®é›†MSELossæ ¼å¼é”™è¯¯: æœ‰{mse_dataset_columns}åˆ—è€Œé3åˆ—")
                
                from sentence_transformers.cross_encoder.losses.MSELoss import MSELoss
                loss = MSELoss(model)
                logger.info(f"ä½¿ç”¨MSELossæŸå¤±å‡½æ•°ï¼ˆå›å½’ä»»åŠ¡ï¼Œæ ‡ç­¾èŒƒå›´: {min(labels):.2f}-{max(labels):.2f}ï¼‰ï¼Œæ•°æ®é›†åˆ—æ•°: {mse_dataset_columns}")
        
        # è¿‡æ»¤æ‰å¤šè¿›ç¨‹å’Œsample_sizeç›¸å…³çš„å‚æ•°ï¼Œè¿™äº›å‚æ•°ä¸å±äºè®­ç»ƒå‚æ•°ç±»
        filtered_config = {k: v for k, v in training_config.items() 
                          if k not in ['nproc_per_node', 'local_rank', 'master_port', 'master_addr', 
                                     'train_sample_size', 'eval_sample_size', 'test_sample_size']}
        
        # æ·»åŠ è¿›åº¦æ¡æ§åˆ¶ï¼Œé¿å…å¤šè¿›åº¦æ¡é‡å   
        filtered_config['disable_tqdm'] = False  # å¯ç”¨ä¸»è¿›åº¦æ¡
        
        args = CrossEncoderTrainingArguments(**filtered_config)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„è®­ç»ƒç±»å‹: {train_type}. åªæ”¯æŒ 'embedding' æˆ– 'reranker'")
    
    return model, loss, args, train_dataset



def _create_trainer(train_type: str, model, args, train_dataset, eval_dataset, loss, dev_evaluator):
    """
    åˆ›å»ºå¯¹åº”ç±»å‹çš„è®­ç»ƒå™¨
    
    Args:
        train_type: è®­ç»ƒç±»å‹
        model: æ¨¡å‹å®ä¾‹
        args: è®­ç»ƒå‚æ•°
        train_dataset: è®­ç»ƒæ•°æ®é›†
        eval_dataset: éªŒè¯æ•°æ®é›†
        loss: æŸå¤±å‡½æ•°
        dev_evaluator: éªŒè¯è¯„ä¼°å™¨
        
    Returns:
        è®­ç»ƒå™¨å®ä¾‹
    """
    # å¯¼å…¥UnifiedEvaluatorç”¨äºæ•°æ®é›†å¤„ç†
    from .utils.evaluation import UnifiedEvaluator
    
    # æ£€æŸ¥è¯„ä¼°é…ç½®ï¼Œå¦‚æœæ²¡æœ‰è¯„ä¼°æ•°æ®æˆ–è¯„ä¼°å™¨ï¼Œåˆ™ç¦ç”¨è¯„ä¼°
    has_eval_data = eval_dataset is not None
    has_evaluator = dev_evaluator is not None
    
    # å¦‚æœè®¾ç½®äº†è¯„ä¼°ç­–ç•¥ä½†æ²¡æœ‰è¯„ä¼°æ•°æ®æˆ–è¯„ä¼°å™¨ï¼Œåˆ™ä¿®æ”¹è¯„ä¼°ç­–ç•¥ä¸º"no"
    if hasattr(args, 'eval_strategy') and args.eval_strategy != "no":
        if not has_eval_data and not has_evaluator:
            logger.warning("è®¾ç½®äº†eval_strategyä½†æ²¡æœ‰æä¾›eval_datasetæˆ–evaluatorï¼Œå°†è‡ªåŠ¨è®¾ç½®eval_strategy='no'")
            args.eval_strategy = "no"
    
    # éªŒè¯è®­ç»ƒæ•°æ®é›†çš„åˆ—æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰
    if isinstance(train_dataset, dict):
        for dataset_name, dataset in train_dataset.items():
            column_count = len(dataset.column_names)
            logger.info(f"ğŸ” è®­ç»ƒå™¨æ„å»ºå‰éªŒè¯ - æ•°æ®é›† '{dataset_name}' åˆ—æ•°: {column_count}, åˆ—å: {dataset.column_names}")
            if column_count != 3:
                logger.error(f"âŒ æ•°æ®é›† '{dataset_name}' åˆ—æ•°å¼‚å¸¸ï¼åº”ä¸º3åˆ—ï¼Œå®é™…ä¸º{column_count}åˆ—")
    else:
        column_count = len(train_dataset.column_names)
        logger.info(f"ğŸ” è®­ç»ƒå™¨æ„å»ºå‰éªŒè¯ - è®­ç»ƒæ•°æ®é›†åˆ—æ•°: {column_count}, åˆ—å: {train_dataset.column_names}")
        if column_count != 3:
            logger.error(f"âŒ è®­ç»ƒæ•°æ®é›†åˆ—æ•°å¼‚å¸¸ï¼åº”ä¸º3åˆ—ï¼Œå®é™…ä¸º{column_count}åˆ—")
    
    # æ„å»ºè®­ç»ƒå™¨å‚æ•°
    trainer_kwargs = {
        "model": model,
        "args": args,
        "train_dataset": train_dataset,
        "loss": loss,
    }
    
    # æ·»åŠ å¯é€‰å‚æ•°
    if has_eval_data:
        # å¯¹éªŒè¯æ•°æ®é›†åº”ç”¨ç›¸åŒçš„åˆ—è¿‡æ»¤é€»è¾‘
        filtered_eval_dataset = eval_dataset
        if isinstance(eval_dataset, dict):
            # å¤šæ•°æ®é›†ï¼šå¯¹æ¯ä¸ªæ•°æ®é›†åº”ç”¨åˆ—è¿‡æ»¤
            filtered_eval_dataset = {}
            for dataset_name, dataset in eval_dataset.items():
                if dataset is not None:
                    # ä¸ºéªŒè¯æ•°æ®é›†ç¡®å®šç›®æ ‡åˆ—å
                    temp_evaluator_factory = UnifiedEvaluator(train_type)
                    dataset_target_column = temp_evaluator_factory._get_dataset_target_column(dataset)
                    
                    # åº”ç”¨åˆ—è¿‡æ»¤
                    column_names = dataset.column_names
                    if len(column_names) >= 3:
                        input_columns = [col for col in column_names if col != dataset_target_column][:2]
                        columns_to_keep = input_columns + [dataset_target_column]
                        filtered_dataset = dataset.select_columns(columns_to_keep)
                        logger.info(f"éªŒè¯æ•°æ®é›† '{dataset_name}' åˆ—è¿‡æ»¤: {column_names} â†’ {columns_to_keep}")
                        filtered_eval_dataset[dataset_name] = filtered_dataset
                    else:
                        filtered_eval_dataset[dataset_name] = dataset
        elif eval_dataset is not None:
            # å•æ•°æ®é›†ï¼šåº”ç”¨åˆ—è¿‡æ»¤
            temp_evaluator_factory = UnifiedEvaluator(train_type)
            dataset_target_column = temp_evaluator_factory._get_dataset_target_column(eval_dataset)
            
            column_names = eval_dataset.column_names
            if len(column_names) >= 3:
                input_columns = [col for col in column_names if col != dataset_target_column][:2]
                columns_to_keep = input_columns + [dataset_target_column]
                filtered_eval_dataset = eval_dataset.select_columns(columns_to_keep)
                logger.info(f"å•éªŒè¯æ•°æ®é›†åˆ—è¿‡æ»¤: {column_names} â†’ {columns_to_keep}")
        
        # éªŒè¯éªŒè¯æ•°æ®é›†çš„åˆ—æ•°
        if isinstance(filtered_eval_dataset, dict):
            for dataset_name, dataset in filtered_eval_dataset.items():
                if dataset is not None:
                    column_count = len(dataset.column_names)
                    logger.info(f"ğŸ” éªŒè¯æ•°æ®é›† '{dataset_name}' åˆ—æ•°: {column_count}, åˆ—å: {dataset.column_names}")
                    if column_count != 3:
                        logger.error(f"âŒ éªŒè¯æ•°æ®é›† '{dataset_name}' åˆ—æ•°å¼‚å¸¸ï¼åº”ä¸º3åˆ—ï¼Œå®é™…ä¸º{column_count}åˆ—")
        elif filtered_eval_dataset is not None:
            column_count = len(filtered_eval_dataset.column_names)
            logger.info(f"ğŸ” éªŒè¯æ•°æ®é›†åˆ—æ•°: {column_count}, åˆ—å: {filtered_eval_dataset.column_names}")
            if column_count != 3:
                logger.error(f"âŒ éªŒè¯æ•°æ®é›†åˆ—æ•°å¼‚å¸¸ï¼åº”ä¸º3åˆ—ï¼Œå®é™…ä¸º{column_count}åˆ—")
        
        trainer_kwargs["eval_dataset"] = filtered_eval_dataset
    if has_evaluator:
        trainer_kwargs["evaluator"] = dev_evaluator
    
    # ğŸ” è°ƒè¯•è®­ç»ƒå™¨å‚æ•°
    logger.info(f"ğŸ” è®­ç»ƒå™¨å‚æ•°è°ƒè¯•:")
    logger.info(f"   args.per_device_train_batch_size: {getattr(args, 'per_device_train_batch_size', 'NOT SET')}")
    logger.info(f"   args.gradient_accumulation_steps: {getattr(args, 'gradient_accumulation_steps', 'NOT SET')}")
    logger.info(f"   args.num_train_epochs: {getattr(args, 'num_train_epochs', 'NOT SET')}")
    logger.info(f"   args.max_steps: {getattr(args, 'max_steps', 'NOT SET')}")
    
    if isinstance(train_dataset, dict):
        total_samples = sum(len(ds) for ds in train_dataset.values())
        logger.info(f"   å®é™…è®­ç»ƒæ•°æ®é›†æ€»å¤§å°: {total_samples}")
        for name, ds in train_dataset.items():
            logger.info(f"   æ•°æ®é›† '{name}' å¤§å°: {len(ds)}")
    else:
        logger.info(f"   è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset) if hasattr(train_dataset, '__len__') else 'æ— æ³•è®¡ç®—'}")
    
    # æ ¹æ®ç±»å‹åˆ›å»ºè®­ç»ƒå™¨
    if train_type == "embedding":
        from sentence_transformers import SentenceTransformerTrainer
        trainer = SentenceTransformerTrainer(**trainer_kwargs)
    elif train_type == "reranker":
        from sentence_transformers.cross_encoder.trainer import CrossEncoderTrainer
        trainer = CrossEncoderTrainer(**trainer_kwargs)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„è®­ç»ƒç±»å‹: {train_type}. åªæ”¯æŒ 'embedding' æˆ– 'reranker'")
    
    return trainer

def main(progress_callback=None, training_config=None):
    """ä¸»è®­ç»ƒå‡½æ•°ï¼Œæ ¹æ®TRAIN_TYPEç¯å¢ƒå˜é‡é€‰æ‹©è®­ç»ƒæ¨¡å¼
    
    Args:
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        training_config: è®­ç»ƒé…ç½®å­—å…¸ï¼ŒåŒ…å«:
            - report_to: æŠ¥å‘Šå·¥å…· (swanlab, tensorboard, etc.)
            - user_logging_dir: ç”¨æˆ·æ—¥å¿—ç›®å½•
            - å…¶ä»–è®­ç»ƒå‚æ•°...
    """
    logger.info("ğŸ”¥ æ­£åœ¨æ‰§è¡Œç»Ÿä¸€è®­ç»ƒè„šæœ¬ train.py")
    load_dotenv()
    
    # ğŸ”§ è·å–ä»»åŠ¡IDç”¨äºå…¨å±€å¼‚å¸¸å¤„ç†
    task_id = None
    if training_config:
        task_id = training_config.get("task_id") 
    
    # SwanLabé…ç½®å¤„ç†ï¼šä»training_configä¸­è·å–é…ç½®
    training_config = training_config or {}
    report_to = training_config.get("report_to", "")
    logger.info(f"æŠ¥å‘Šå·¥å…·é…ç½®: report_to='{report_to}'")
    
    # åˆå§‹åŒ–SwanLabï¼ˆå¦‚æœé…ç½®äº†çš„è¯ï¼‰
    from .enums.training_parameter_enums import ReportTo
    if report_to == ReportTo.SWANLAB or report_to == ReportTo.SWANLAB.value:
        try:
            from .utils.common_utils import init_swanlab
            # ä¼ å…¥å®Œæ•´çš„training_configï¼Œè®©SwanLabé…ç½®ç±»å¤„ç†æ‰€æœ‰å‚æ•°
            init_swanlab(training_config=training_config)
        except Exception as e:
            logger.warning(f"SwanLabåˆå§‹åŒ–å¤±è´¥ï¼Œç»§ç»­è®­ç»ƒ: {e}")
    
    # 1. ä»training_configè·å–é…ç½®ï¼ˆå®Œå…¨ä¸ä¾èµ–ç¯å¢ƒå˜é‡ï¼‰
    train_type = training_config.get("train_type", "embedding").lower()
    model_name = training_config.get("model_name_or_path", "distilbert-base-uncased")
    
    # å¦‚æœæ²¡æœ‰æä¾›output_dirï¼Œç”Ÿæˆé»˜è®¤è·¯å¾„
    if not training_config.get("output_dir"):
        output_dir = f"output/training_{train_type}_{model_name.replace('/', '-')}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}"
    else:
        output_dir = training_config.get("output_dir")
    
    # è·å–ä»»åŠ¡IDï¼ˆåªä»training_configè·å–ï¼‰
    task_id = training_config.get("task_id")
    if task_id:
        logger.info(f"ä½¿ç”¨ä»»åŠ¡ID: {task_id}")
    else:
        logger.warning("æœªè·å–åˆ°ä»»åŠ¡IDï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
        
    logger.info(f"è®­ç»ƒé…ç½® - ç±»å‹: {train_type}, æ¨¡å‹: {model_name}, è¾“å‡ºç›®å½•: {output_dir}")
    
    # æ³¨å†Œè¾“å‡ºç›®å½•åˆ°ä¸´æ—¶æ–‡ä»¶ç®¡ç†å™¨ï¼ˆç”¨äºå¼‚å¸¸æƒ…å†µä¸‹çš„æ¸…ç†ï¼‰
    from .utils.temp_file_manager import temp_file_manager
    temp_file_manager.register_temp_dir(output_dir)
    
    logger.info(f"å¼€å§‹è®­ç»ƒï¼Œè®­ç»ƒç±»å‹: {train_type}")
    logger.info(f"æ¨¡å‹: {model_name}")
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")

    # 2. ç»Ÿä¸€çš„æ•°æ®åŠ è½½
    from .utils.data_loader import DataLoader
    hf_subset = training_config.get('HF_subset') if training_config else None
    data_loader = DataLoader(
        hf_subset=hf_subset,
        train_sample_size=training_config.get('train_sample_size', 0),
        eval_sample_size=training_config.get('eval_sample_size', 0),
        test_sample_size=training_config.get('test_sample_size', 0)
    )
    
    # ä»è®­ç»ƒé…ç½®ä¸­è·å–æ•°æ®é›†è·¯å¾„
    dataset_path = training_config.get("dataset_name_or_path")
    logger.info(f"ğŸ”§ ä½¿ç”¨æ•°æ®é›†è·¯å¾„: {dataset_path}")
    
    train_dataset, eval_dataset, test_dataset = data_loader.load_all_splits(dataset_path)
    
    # æ£€æŸ¥è®­ç»ƒæ•°æ®é›†æ˜¯å¦æˆåŠŸåŠ è½½
    if train_dataset is None:
        dataset_name_or_path = training_config.get("dataset_name_or_path", "æœªæŒ‡å®šæ•°æ®é›†")
        error_msg = f"è®­ç»ƒæ•°æ®é›†åŠ è½½å¤±è´¥: {dataset_name_or_path}"
        logger.error(error_msg)
        raise ValueError(error_msg)
    
    # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥æ•°æ®é›†ç»“æ„
    logger.info(f"ğŸ” train_dataset ç±»å‹: {type(train_dataset)}")
    if data_loader.is_multi_dataset(train_dataset):
        logger.info(f"ğŸ” å¤šæ•°æ®é›†é”®å: {list(train_dataset.keys())}")
        for name, dataset in train_dataset.items():
            logger.info(f"ğŸ” æ•°æ®é›† '{name}' ç±»å‹: {type(dataset)}")
            if hasattr(dataset, 'column_names'):
                logger.info(f"ğŸ” æ•°æ®é›† '{name}' åˆ—å: {dataset.column_names}")
            else:
                logger.error(f"ğŸ” æ•°æ®é›† '{name}' æ²¡æœ‰ column_names å±æ€§!")
    else:
        logger.info(f"ğŸ” å•æ•°æ®é›†åˆ—å: {train_dataset.column_names if hasattr(train_dataset, 'column_names') else 'æ—  column_names å±æ€§'}")
    
    data_loader.validate_dataset(train_dataset)
    target_column = data_loader.get_target_column(train_dataset)
    
    # ğŸ”§ æ ‡å‡†åŒ–æ•°æ®é›†åˆ—åä»¥ç¬¦åˆsentence-transformersè¦æ±‚
    logger.info("ğŸ“‹ å¼€å§‹æ•°æ®é›†åˆ—åæ ‡å‡†åŒ–...")
    train_dataset = data_loader.standardize_dataset_columns(train_dataset, target_column)
    
    # æ ‡å‡†åŒ–åéœ€è¦æ›´æ–°ç›®æ ‡åˆ—åç§°
    if target_column not in ["label", "score"]:
        # æ ¹æ®æ•°æ®ç±»å‹ç¡®å®šæ–°çš„ç›®æ ‡åˆ—å
        if data_loader.is_multi_dataset(train_dataset):
            first_dataset = next(iter(train_dataset.values()))
            # æ£€æŸ¥ first_dataset æ˜¯å¦ä¸º Dataset å¯¹è±¡
            if not hasattr(first_dataset, 'column_names'):
                logger.error(f"å¤šæ•°æ®é›†ä¸­çš„ç¬¬ä¸€ä¸ªæ•°æ®é›†ä¸æ˜¯ Dataset å¯¹è±¡: {type(first_dataset)}")
                logger.error(f"å¤šæ•°æ®é›†ç»“æ„: {list(train_dataset.keys()) if isinstance(train_dataset, dict) else 'Not a dict'}")
                raise ValueError("å¤šæ•°æ®é›†ä¸­çš„æ•°æ®é›†å¯¹è±¡æ ¼å¼å¼‚å¸¸")
        else:
            first_dataset = train_dataset
        
        sample_value = first_dataset[target_column][0] if target_column in first_dataset.column_names else None
        if sample_value is not None:
            if isinstance(sample_value, int) or (isinstance(sample_value, float) and sample_value.is_integer()):
                target_column = "label"
            else:
                target_column = "score"
        logger.info(f"ğŸ”„ ç›®æ ‡åˆ—åå·²æ›´æ–°ä¸º: {target_column}")
    
    # ğŸ”§ åŒæ ·æ ‡å‡†åŒ–éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†
    if eval_dataset is not None:
        logger.info("ğŸ“‹ æ ‡å‡†åŒ–éªŒè¯æ•°æ®é›†åˆ—å...")
        eval_dataset = data_loader.standardize_dataset_columns(eval_dataset, target_column)
        
    if test_dataset is not None:
        logger.info("ğŸ“‹ æ ‡å‡†åŒ–æµ‹è¯•æ•°æ®é›†åˆ—å...")
        test_dataset = data_loader.standardize_dataset_columns(test_dataset, target_column)
    
    logger.info(f"è®­ç»ƒæ•°æ®é›†: {train_dataset}")
    logger.info(f"ç›®æ ‡åˆ—: {target_column}")

    # å»ºç«‹æ•°æ®æºæ˜ å°„ï¼ˆåœ¨æ›´å¤§çš„ä½œç”¨åŸŸä¸­ï¼Œä¾›åç»­æŸå¤±å‡½æ•°æ›´æ–°ä½¿ç”¨ï¼‰
    def generate_data_source_id(index: int, base_name: str) -> str:
        """ç”Ÿæˆæ•°æ®æºID"""
        return str(index + 1)  # 1, 2, 3, ...
    
    data_source_mapping = {}
    if data_loader.is_multi_dataset(train_dataset) and isinstance(train_dataset, dict):
        for idx, base_name in enumerate(train_dataset.keys()):
            data_source_mapping[base_name] = generate_data_source_id(idx, base_name)
    else:
        # å•æ•°æ®é›†ï¼šä½¿ç”¨å›ºå®šçš„æ•°æ®æºIDå’ŒåŸºç¡€åç§°
        dataset_name_or_path = training_config.get("dataset_name_or_path", "unknown")
        base_name = data_loader._extract_dataset_base_name(dataset_name_or_path)
        data_source_mapping[base_name] = "1"


    # è®°å½•æ•°æ®é›†ä¿¡æ¯åˆ°æ•°æ®åº“
    if task_id:
        try:
            dataset_name_or_path = training_config.get("dataset_name_or_path", "unknown")
                    
            # è®°å½•è®­ç»ƒæ•°æ®é›†
            if train_dataset:
                if data_loader.is_multi_dataset(train_dataset):
                    # å¤šæ•°æ®é›†ï¼šæ¯ä¸ªæ•°æ®æºåˆ†é…ç‹¬ç«‹çš„ID
                    for base_name, dataset in train_dataset.items():
                        data_source_id = data_source_mapping[base_name]  # ä½¿ç”¨ç»Ÿä¸€æ˜ å°„
                        
                        TrainingDatasetService.record_dataset_info(
                            task_id=task_id,
                            data_source_id=data_source_id,
                            dataset_name=base_name,  # ğŸ”§ å»æ‰splitåç¼€ï¼Œåªä¿å­˜åŸºç¡€åç§°
                            dataset_base_name=base_name,
                            dataset_path=dataset_name_or_path,
                            dataset_type="auto",
                            split_type="train",
                            dataset=dataset,
                            target_column=target_column,
                            loss_function=None,
                            evaluator=None,
                            hf_subset=training_config.get('HF_subset'),  # HF_subseté…ç½®
                            configured_sample_size=training_config.get('train_sample_size', 0)  # æ–°å¢ï¼šæ ·æœ¬å¤§å°é…ç½®
                        )
                else:
                    # å•æ•°æ®é›†ï¼šä½¿ç”¨å›ºå®šçš„æ•°æ®æºID
                    base_name = data_loader._extract_dataset_base_name(dataset_name_or_path)
                    data_source_id = generate_data_source_id(0, base_name)
                    
                    TrainingDatasetService.record_dataset_info(
                        task_id=task_id,
                        data_source_id=data_source_id,
                        dataset_name=base_name,  # ğŸ”§ å»æ‰splitåç¼€ï¼Œåªä¿å­˜åŸºç¡€åç§°
                        dataset_base_name=base_name,
                        dataset_path=dataset_name_or_path,
                        dataset_type="auto",
                        split_type="train",
                        dataset=train_dataset,
                        target_column=target_column,
                        loss_function=None,
                        evaluator=None,
                        hf_subset=training_config.get('HF_subset'),  # HF_subseté…ç½®
                        configured_sample_size=training_config.get('train_sample_size', 0)  # æ–°å¢ï¼šæ ·æœ¬å¤§å°é…ç½®
                    )
            
            # è®°å½•éªŒè¯æ•°æ®é›†
            if eval_dataset:
                if data_loader.is_multi_dataset(eval_dataset):
                    # å¤šæ•°æ®é›†ï¼šä¸ºæ¯ä¸ªæ•°æ®æºè®°å½•éªŒè¯é›†
                    for base_name, dataset in eval_dataset.items():
                        data_source_id = data_source_mapping.get(base_name, f"ds_{hash(base_name) % 1000:03d}")  # ä½¿ç”¨æ˜ å°„æˆ–å›é€€
                        
                        TrainingDatasetService.record_dataset_info(
                            task_id=task_id,
                            data_source_id=data_source_id,
                            dataset_name=base_name,  # ğŸ”§ å»æ‰splitåç¼€ï¼Œåªä¿å­˜åŸºç¡€åç§°
                            dataset_base_name=base_name,
                            dataset_path=dataset_name_or_path,
                            dataset_type="auto",
                            split_type="eval",
                            dataset=dataset,
                            target_column=target_column,
                            loss_function=None,
                            evaluator=None,
                            hf_subset=training_config.get('HF_subset'),  # HF_subseté…ç½®
                            configured_sample_size=training_config.get('eval_sample_size', 0)  # æ–°å¢ï¼šæ ·æœ¬å¤§å°é…ç½®
                        )
                else:
                    # å•æ•°æ®é›†ï¼šä½¿ç”¨ç›¸åŒçš„æ•°æ®æºID
                    base_name = data_loader._extract_dataset_base_name(dataset_name_or_path)
                    data_source_id = generate_data_source_id(0, base_name)
                    
                    TrainingDatasetService.record_dataset_info(
                        task_id=task_id,
                        data_source_id=data_source_id,
                        dataset_name=base_name,  # ğŸ”§ å»æ‰splitåç¼€ï¼Œåªä¿å­˜åŸºç¡€åç§°
                        dataset_base_name=base_name,
                        dataset_path=dataset_name_or_path,
                        dataset_type="auto",
                        split_type="eval",
                        dataset=eval_dataset,
                        target_column=target_column,
                        loss_function=None,
                        evaluator=None,
                        hf_subset=training_config.get('HF_subset'),  # HF_subseté…ç½®
                        configured_sample_size=training_config.get('eval_sample_size', 0)  # æ–°å¢ï¼šæ ·æœ¬å¤§å°é…ç½®
                    )
            
            # è®°å½•æµ‹è¯•æ•°æ®é›†
            if test_dataset:
                if data_loader.is_multi_dataset(test_dataset):
                    # å¤šæ•°æ®é›†ï¼šä¸ºæ¯ä¸ªæ•°æ®æºè®°å½•æµ‹è¯•é›†
                    for base_name, dataset in test_dataset.items():
                        data_source_id = data_source_mapping.get(base_name, f"ds_{hash(base_name) % 1000:03d}")  # ä½¿ç”¨æ˜ å°„æˆ–å›é€€
                        
                        TrainingDatasetService.record_dataset_info(
                            task_id=task_id,
                            data_source_id=data_source_id,
                            dataset_name=base_name,  # ğŸ”§ å»æ‰splitåç¼€ï¼Œåªä¿å­˜åŸºç¡€åç§°
                            dataset_base_name=base_name,
                            dataset_path=dataset_name_or_path,
                            dataset_type="auto",
                            split_type="test",
                            dataset=dataset,
                            target_column=target_column,
                            loss_function=None,
                            evaluator=None,
                            hf_subset=training_config.get('HF_subset'),  # HF_subseté…ç½®
                            configured_sample_size=training_config.get('test_sample_size', 0)  # æ–°å¢ï¼šæ ·æœ¬å¤§å°é…ç½®
                        )
                else:
                    # å•æ•°æ®é›†ï¼šä½¿ç”¨ç›¸åŒçš„æ•°æ®æºID
                    base_name = data_loader._extract_dataset_base_name(dataset_name_or_path)
                    data_source_id = generate_data_source_id(0, base_name)
                    
                    TrainingDatasetService.record_dataset_info(
                        task_id=task_id,
                        data_source_id=data_source_id,
                        dataset_name=base_name,  # ğŸ”§ å»æ‰splitåç¼€ï¼Œåªä¿å­˜åŸºç¡€åç§°
                        dataset_base_name=base_name,
                        dataset_path=dataset_name_or_path,
                        dataset_type="auto",
                        split_type="test",
                        dataset=test_dataset,
                        target_column=target_column,
                        loss_function=None,
                        evaluator=None,
                        hf_subset=training_config.get('HF_subset'),  # HF_subseté…ç½®
                        configured_sample_size=training_config.get('test_sample_size', 0)  # æ–°å¢ï¼šæ ·æœ¬å¤§å°é…ç½®
                    )
            
            logger.info("æ•°æ®é›†ä¿¡æ¯è®°å½•æˆåŠŸ")
        except Exception as e:
            logger.warning(f"è®°å½•æ•°æ®é›†ä¿¡æ¯å¤±è´¥ï¼ˆä¸å½±å“è®­ç»ƒï¼‰: {e}")

    # 3. åˆ›å»ºç»Ÿä¸€çš„è®­ç»ƒé…ç½®
    short_model_name = model_name.split("/")[-1] if "/" in model_name else model_name
    run_name = f"{train_type}-{short_model_name}"
    
    # æ„å»ºè®­ç»ƒé…ç½®å­—å…¸ï¼ˆembeddingå’Œrerankerå…±ç”¨ï¼‰
    training_config_dict = _create_training_config(output_dir, run_name, training_config)


    # 4. æ ¹æ®è®­ç»ƒç±»å‹åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œè®­ç»ƒå‚æ•°
    model, loss, args, train_dataset = _initialize_model_and_loss(train_type, model_name, train_dataset, target_column, training_config_dict)
    
    # 4.1. æŸå¤±å‡½æ•°åˆ›å»ºåï¼Œæ›´æ–°æ•°æ®é›†ä¿¡æ¯ä¸­çš„æŸå¤±å‡½æ•°åç§°
    if task_id:
        try:
            # è·å–å®é™…ä½¿ç”¨çš„æŸå¤±å‡½æ•°åç§°
            from .utils.data_loader import DataLoader
            hf_subset = training_config_dict.get('HF_subset') if training_config_dict else None
            data_loader = DataLoader(
                hf_subset=hf_subset,
                train_sample_size=training_config_dict.get('train_sample_size', 0),
                eval_sample_size=training_config_dict.get('eval_sample_size', 0),
                test_sample_size=training_config_dict.get('test_sample_size', 0)
            )
            is_multi_dataset = data_loader.is_multi_dataset(train_dataset)
            
            if is_multi_dataset and isinstance(loss, dict):
                # å¤šæ•°æ®é›†æƒ…å†µï¼šä¸ºæ¯ä¸ªæ•°æ®æºæ›´æ–°å¯¹åº”çš„æŸå¤±å‡½æ•°
                for dataset_name, loss_func in loss.items():
                    actual_loss_name = type(loss_func).__name__
                    data_source_id = data_source_mapping.get(dataset_name, f"ds_{hash(dataset_name) % 1000:03d}")  # ä½¿ç”¨ç»Ÿä¸€æ˜ å°„
                    try:
                        TrainingDatasetService.update_dataset_loss_function_by_source(
                            task_id=task_id,
                            data_source_id=data_source_id,
                            split_type="train",
                            loss_function=actual_loss_name
                        )
                        logger.info(f"æ›´æ–°å¤šæ•°æ®é›†æŸå¤±å‡½æ•°: {data_source_id}-train -> {actual_loss_name}")
                    except Exception as e:
                        logger.warning(f"æ›´æ–°æ•°æ®æº {data_source_id} è®­ç»ƒé›†æŸå¤±å‡½æ•°å¤±è´¥: {e}")
                
                # å¤šæ•°æ®é›†æƒ…å†µï¼šéªŒè¯é›†å’Œæµ‹è¯•é›†ä½¿ç”¨å¯¹åº”åŒåè®­ç»ƒæ•°æ®é›†çš„æŸå¤±å‡½æ•°
                
                # æ›´æ–°éªŒè¯é›†æŸå¤±å‡½æ•°ï¼ˆå¦‚æœå­˜åœ¨éªŒè¯é›†ä¸”ä¸ºå¤šæ•°æ®é›†ï¼‰
                if eval_dataset and isinstance(eval_dataset, dict):
                    for dataset_name in eval_dataset.keys():
                        data_source_id = data_source_mapping.get(dataset_name, f"ds_{hash(dataset_name) % 1000:03d}")  # ä½¿ç”¨ç»Ÿä¸€æ˜ å°„
                        # æŸ¥æ‰¾å¯¹åº”çš„è®­ç»ƒæ•°æ®é›†æŸå¤±å‡½æ•°
                        if dataset_name in loss:
                            corresponding_loss_name = type(loss[dataset_name]).__name__
                            try:
                                TrainingDatasetService.update_dataset_loss_function_by_source(
                                    task_id=task_id,
                                    data_source_id=data_source_id,
                                    split_type="eval",
                                    loss_function=corresponding_loss_name
                                )
                                logger.info(f"æ›´æ–°å¤šéªŒè¯æ•°æ®é›†æŸå¤±å‡½æ•°: {data_source_id}-eval -> {corresponding_loss_name} (ç»‘å®šåˆ° {data_source_id}-train)")
                            except Exception as e:
                                logger.warning(f"æ›´æ–°éªŒè¯æ•°æ®æº {data_source_id} æŸå¤±å‡½æ•°å¤±è´¥: {e}")
                        else:
                            logger.warning(f"éªŒè¯æ•°æ®é›† {dataset_name} æ²¡æœ‰å¯¹åº”çš„è®­ç»ƒæ•°æ®é›†æŸå¤±å‡½æ•°")
                
                # æ›´æ–°æµ‹è¯•é›†æŸå¤±å‡½æ•°ï¼ˆå¦‚æœå­˜åœ¨æµ‹è¯•é›†ä¸”ä¸ºå¤šæ•°æ®é›†ï¼‰
                if test_dataset and isinstance(test_dataset, dict):
                    for dataset_name in test_dataset.keys():
                        data_source_id = data_source_mapping.get(dataset_name, f"ds_{hash(dataset_name) % 1000:03d}")  # ä½¿ç”¨ç»Ÿä¸€æ˜ å°„
                        # æŸ¥æ‰¾å¯¹åº”çš„è®­ç»ƒæ•°æ®é›†æŸå¤±å‡½æ•°
                        if dataset_name in loss:
                            corresponding_loss_name = type(loss[dataset_name]).__name__
                            try:
                                TrainingDatasetService.update_dataset_loss_function_by_source(
                                    task_id=task_id,
                                    data_source_id=data_source_id,
                                    split_type="test",
                                    loss_function=corresponding_loss_name
                                )
                                logger.info(f"æ›´æ–°å¤šæµ‹è¯•æ•°æ®é›†æŸå¤±å‡½æ•°: {data_source_id}-test -> {corresponding_loss_name} (ç»‘å®šåˆ° {data_source_id}-train)")
                            except Exception as e:
                                logger.warning(f"æ›´æ–°æµ‹è¯•æ•°æ®æº {data_source_id} æŸå¤±å‡½æ•°å¤±è´¥: {e}")
                        else:
                            logger.warning(f"æµ‹è¯•æ•°æ®é›† {dataset_name} æ²¡æœ‰å¯¹åº”çš„è®­ç»ƒæ•°æ®é›†æŸå¤±å‡½æ•°")
                            
            else:
                # å•æ•°æ®é›†æƒ…å†µï¼šç›´æ¥è·å–æŸå¤±å‡½æ•°ç±»å
                actual_loss_name = type(loss).__name__
                logger.info(f"æ›´æ–°å•æ•°æ®é›†æŸå¤±å‡½æ•°ä¿¡æ¯: {actual_loss_name}")
                
                # å•æ•°æ®é›†ä½¿ç”¨å›ºå®šçš„æ•°æ®æºID
                data_source_id = "1"
                
                # æ›´æ–°è®­ç»ƒæ•°æ®é›†çš„æŸå¤±å‡½æ•°
                if train_dataset:
                    try:
                        TrainingDatasetService.update_dataset_loss_function_by_source(
                            task_id=task_id,
                            data_source_id=data_source_id,
                            split_type="train",
                            loss_function=actual_loss_name
                        )
                        logger.info(f"æ›´æ–°å•è®­ç»ƒæ•°æ®é›†æŸå¤±å‡½æ•°: {data_source_id}-train -> {actual_loss_name}")
                    except Exception as e:
                        logger.warning(f"æ›´æ–°è®­ç»ƒæ•°æ®é›†æŸå¤±å‡½æ•°å¤±è´¥: {e}")
                
                # å•æ•°æ®é›†æƒ…å†µï¼šéªŒè¯é›†å’Œæµ‹è¯•é›†ä¹Ÿä½¿ç”¨ç›¸åŒçš„æŸå¤±å‡½æ•°
                if eval_dataset:
                    try:
                        TrainingDatasetService.update_dataset_loss_function_by_source(
                            task_id=task_id,
                            data_source_id=data_source_id,
                            split_type="eval",
                            loss_function=actual_loss_name
                        )
                        logger.info(f"æ›´æ–°å•éªŒè¯æ•°æ®é›†æŸå¤±å‡½æ•°: {data_source_id}-eval -> {actual_loss_name}")
                    except Exception as e:
                        logger.warning(f"æ›´æ–°éªŒè¯æ•°æ®é›†æŸå¤±å‡½æ•°å¤±è´¥: {e}")
                
                if test_dataset:
                    try:
                        TrainingDatasetService.update_dataset_loss_function_by_source(
                            task_id=task_id,
                            data_source_id=data_source_id,
                            split_type="test",
                            loss_function=actual_loss_name
                        )
                        logger.info(f"æ›´æ–°å•æµ‹è¯•æ•°æ®é›†æŸå¤±å‡½æ•°: {data_source_id}-test -> {actual_loss_name}")
                    except Exception as e:
                        logger.warning(f"æ›´æ–°æµ‹è¯•æ•°æ®é›†æŸå¤±å‡½æ•°å¤±è´¥: {e}")
                    
        except Exception as e:
            logger.warning(f"æ›´æ–°æ•°æ®é›†æŸå¤±å‡½æ•°ä¿¡æ¯å¤±è´¥: {e}")
    
    # 4.5. æ¨¡å‹åŠ è½½æˆåŠŸåï¼Œæ›´æ–°æ¨¡å‹ä¿¡æ¯åˆ°æ•°æ®åº“
    try:
        from .services.task_manager import task_manager
        
        # ä½¿ç”¨å‰é¢è·å–çš„ä»»åŠ¡IDï¼ˆå·²ç»ä»training_configæˆ–ç¯å¢ƒå˜é‡è·å–ï¼‰
        if task_id:
            # æ„å»ºæ¨¡å‹ä¿¡æ¯
            model_info_update = {
                "validation": {
                    "valid": True,
                    "message": "æ¨¡å‹åŠ è½½æˆåŠŸ",
                    "details": {
                        "type": "validated",
                        "name": model_name
                    }
                },
                "recommended_training_types": [train_type],
                "compatibility": {
                    "supported": True,
                    "model_type": "loaded",
                    "notes": ["æ¨¡å‹å·²æˆåŠŸåŠ è½½"]
                }
            }
            
            # è·å–æ¨¡å‹ç»´åº¦ï¼ˆæ”¯æŒembeddingå’Œrerankeræ¨¡å‹ï¼‰
            embedding_dimension = None
            if train_type == "embedding" and hasattr(model, 'get_sentence_embedding_dimension'):
                try:
                    embedding_dimension = model.get_sentence_embedding_dimension()
                    logger.info(f"è·å–åˆ°embeddingæ¨¡å‹ç»´åº¦: {embedding_dimension}")
                except Exception as dim_e:
                    logger.warning(f"è·å–embeddingæ¨¡å‹ç»´åº¦å¤±è´¥: {str(dim_e)}")
            elif train_type == "reranker":
                # å¯¹äºrerankeræ¨¡å‹ï¼Œå°è¯•å¤šç§æ–¹æ³•è·å–ç»´åº¦
                try:
                    # æ–¹æ³•1: é€šè¿‡æ¨¡å‹çš„tokenizerå’Œconfigè·å–hidden_size
                    if hasattr(model, 'model') and hasattr(model.model, 'config') and hasattr(model.model.config, 'hidden_size'):
                        embedding_dimension = model.model.config.hidden_size
                        logger.info(f"è·å–åˆ°rerankeræ¨¡å‹ç»´åº¦ (æ–¹æ³•1): {embedding_dimension}")
                    # æ–¹æ³•2: é€šè¿‡encodeæ–¹æ³•æµ‹è¯•è·å–ç»´åº¦
                    elif hasattr(model, 'encode'):
                        test_texts = ["test"]
                        try:
                            # æŸäº›rerankeræ¨¡å‹çš„encodeæ–¹æ³•è¿”å›embedding
                            test_embedding = model.encode(test_texts)
                            if hasattr(test_embedding, 'shape') and len(test_embedding.shape) > 1:
                                embedding_dimension = test_embedding.shape[1]
                                logger.info(f"è·å–åˆ°rerankeræ¨¡å‹ç»´åº¦ (æ–¹æ³•2): {embedding_dimension}")
                        except:
                            pass
                    # æ–¹æ³•3: æ£€æŸ¥æ˜¯å¦æœ‰classifierå±‚æ¥æ¨æ–­ç»´åº¦
                    elif hasattr(model, 'classifier') and hasattr(model.classifier, 'in_features'):
                        embedding_dimension = model.classifier.in_features
                        logger.info(f"è·å–åˆ°rerankeræ¨¡å‹ç»´åº¦ (æ–¹æ³•3): {embedding_dimension}")
                    
                    if not embedding_dimension:
                        logger.info("æ— æ³•è‡ªåŠ¨è·å–rerankeræ¨¡å‹ç»´åº¦ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼æˆ–è·³è¿‡")
                        
                except Exception as dim_e:
                    logger.warning(f"è·å–rerankeræ¨¡å‹ç»´åº¦å¤±è´¥: {str(dim_e)}")
            
            # å¦‚æœè·å–åˆ°äº†ç»´åº¦ï¼Œæ·»åŠ åˆ°æ¨¡å‹ä¿¡æ¯ä¸­
            if embedding_dimension:
                model_info_update["embedding_dimension"] = embedding_dimension
            
            # æ›´æ–°åˆ°æ•°æ®åº“
            task_manager.update_model_info_after_loading(task_id, model_info_update)
        else:
            logger.info("æœªæ‰¾åˆ°ä»»åŠ¡IDï¼Œè·³è¿‡æ¨¡å‹ä¿¡æ¯æ›´æ–°")
    except Exception as update_e:
        logger.warning(f"æ›´æ–°æ¨¡å‹ä¿¡æ¯åˆ°æ•°æ®åº“å¤±è´¥ï¼Œä¸å½±å“è®­ç»ƒç»§ç»­: {str(update_e)}")
    
    # è¾“å‡º Tensorboard æ—¥å¿—è·¯å¾„ä¿¡æ¯
    if (training_config.get('report_to') == ReportTo.TENSORBOARD or 
        training_config.get('report_to') == ReportTo.TENSORBOARD.value):
        logger.info(f"ğŸ”¥ Tensorboard å·²å¯ç”¨!")
        
        # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æ˜ç¡®æŒ‡å®šäº† logging_dir
        user_logging_dir = training_config.get("user_logging_dir") if training_config else None
        if user_logging_dir:
            logger.info(f"ğŸ“Š Tensorboard æ—¥å¿—ç›®å½•ï¼ˆç”¨æˆ·æŒ‡å®šï¼‰: {user_logging_dir}")
            logger.info(f"ğŸŒ å¯åŠ¨ Tensorboard å‘½ä»¤: tensorboard --logdir=\"{user_logging_dir}\" --host=127.0.0.1 --port=6006")
        else:
            # HuggingFace è‡ªåŠ¨ç”Ÿæˆäº† logging_dirï¼Œæ˜¾ç¤ºå®é™…è·¯å¾„
            if hasattr(args, 'logging_dir') and args.logging_dir:
                actual_log_dir = str(args.logging_dir).replace('\\', '/')  # ç»Ÿä¸€ä½¿ç”¨æ­£æ–œæ 
                logger.info(f"ğŸ“Š Tensorboard æ—¥å¿—ç›®å½•ï¼ˆHuggingFace è‡ªåŠ¨ç”Ÿæˆï¼‰: {actual_log_dir}")
                logger.info(f"ğŸŒ å¯åŠ¨ Tensorboard å‘½ä»¤: tensorboard --logdir=\"{actual_log_dir}\" --host=127.0.0.1 --port=6006")
            else:
                logger.info(f"ğŸ“Š Tensorboard æ—¥å¿—ç›®å½•: HuggingFace å°†è‡ªåŠ¨åœ¨ {output_dir} ä¸‹åˆ›å»º runs/<æ—¶é—´æˆ³> ç›®å½•")
                logger.info(f"ğŸŒ å¯åŠ¨ Tensorboard å‘½ä»¤: tensorboard --logdir=\"{output_dir}/runs\" --host=127.0.0.1 --port=6006")
                logger.info(f"ğŸ’¡ æç¤º: è®­ç»ƒå¼€å§‹åæŸ¥çœ‹ {output_dir}/runs ç›®å½•ä¸‹çš„å®é™…æ—¥å¿—æ–‡ä»¶å¤¹")
        
        logger.info(f"ğŸ”— è®¿é—®åœ°å€: http://127.0.0.1:6006")

    # 5. ç»Ÿä¸€çš„è¯„ä¼°å™¨åˆ›å»º
    from .utils.evaluation import UnifiedEvaluator
    evaluator_factory = UnifiedEvaluator(train_type)
    evaluators = evaluator_factory.create_evaluators_from_datasets(
        eval_dataset, test_dataset, target_column, run_name
    )
    
    # 5.1. è¯„ä¼°å™¨åˆ›å»ºåï¼Œæ›´æ–°æ•°æ®é›†ä¿¡æ¯ä¸­çš„è¯„ä¼°å™¨ç±»å‹
    if task_id:
        try:
            from .utils.data_loader import DataLoader
            hf_subset = training_config_dict.get('HF_subset') if training_config_dict else None
            data_loader = DataLoader(
                hf_subset=hf_subset,
                train_sample_size=training_config_dict.get('train_sample_size', 0),
                eval_sample_size=training_config_dict.get('eval_sample_size', 0),
                test_sample_size=training_config_dict.get('test_sample_size', 0)
            )
            
            # æ›´æ–°éªŒè¯æ•°æ®é›†çš„è¯„ä¼°å™¨ç±»å‹
            if eval_dataset and evaluators.get('dev'):
                evaluator = evaluators['dev']
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ•°æ®é›†
                if data_loader.is_multi_dataset(eval_dataset):
                    # å¤šæ•°æ®é›†æƒ…å†µï¼šä»SequentialEvaluatorä¸­æå–å­è¯„ä¼°å™¨
                    if isinstance(eval_dataset, dict) and hasattr(evaluator, 'evaluators'):
                        # SequentialEvaluatoråŒ…å«å¤šä¸ªå­è¯„ä¼°å™¨
                        dataset_names = list(eval_dataset.keys())
                        sub_evaluators = evaluator.evaluators
                        
                        for dataset_name, sub_evaluator in zip(dataset_names, sub_evaluators):
                            try:
                                if dataset_name in data_source_mapping:
                                    data_source_id = data_source_mapping[dataset_name]
                                    sub_evaluator_name = type(sub_evaluator).__name__
                                    
                                    TrainingDatasetService.update_dataset_evaluator_by_source(
                                        task_id=task_id,
                                        data_source_id=data_source_id,
                                        split_type="eval",
                                        evaluator=sub_evaluator_name
                                    )
                                    logger.info(f"æ›´æ–°å¤šæ•°æ®é›†è¯„ä¼°å™¨ç±»å‹: {dataset_name} (æºID: {data_source_id}) -> {sub_evaluator_name}")
                                else:
                                    logger.warning(f"æ‰¾ä¸åˆ°æ•°æ®é›† {dataset_name} çš„æ•°æ®æºæ˜ å°„")
                            except Exception as e:
                                logger.warning(f"æ›´æ–°æ•°æ®é›† {dataset_name} è¯„ä¼°å™¨ç±»å‹å¤±è´¥: {e}")
                    else:
                        # ä¸æ˜¯SequentialEvaluatorï¼Œæ‰€æœ‰æ•°æ®é›†ä½¿ç”¨åŒä¸€ä¸ªè¯„ä¼°å™¨
                        evaluator_name = type(evaluator).__name__
                        for dataset_name in eval_dataset.keys():
                            try:
                                if dataset_name in data_source_mapping:
                                    data_source_id = data_source_mapping[dataset_name]
                                    TrainingDatasetService.update_dataset_evaluator_by_source(
                                        task_id=task_id,
                                        data_source_id=data_source_id,
                                        split_type="eval",
                                        evaluator=evaluator_name
                                    )
                                    logger.info(f"æ›´æ–°å¤šæ•°æ®é›†è¯„ä¼°å™¨ç±»å‹: {dataset_name} (æºID: {data_source_id}) -> {evaluator_name}")
                            except Exception as e:
                                logger.warning(f"æ›´æ–°æ•°æ®é›† {dataset_name} è¯„ä¼°å™¨ç±»å‹å¤±è´¥: {e}")
                else:
                    # å•æ•°æ®é›†æƒ…å†µï¼šä¸ºè¯¥æ•°æ®æºæ›´æ–°è¯„ä¼°å™¨ç±»å‹
                    evaluator_name = type(evaluator).__name__
                    try:
                        # å•æ•°æ®é›†ä¹Ÿè¦é€šè¿‡data_source_idæ›´æ–°
                        for dataset_name, data_source_id in data_source_mapping.items():
                            TrainingDatasetService.update_dataset_evaluator_by_source(
                                task_id=task_id,
                                data_source_id=data_source_id,
                                split_type="eval",
                                evaluator=evaluator_name
                            )
                            logger.info(f"æ›´æ–°å•éªŒè¯æ•°æ®é›†è¯„ä¼°å™¨ç±»å‹: {dataset_name} (æºID: {data_source_id}) -> {evaluator_name}")
                    except Exception as e:
                        logger.warning(f"æ›´æ–°å•éªŒè¯æ•°æ®é›†è¯„ä¼°å™¨ç±»å‹å¤±è´¥: {e}")
                    
        except Exception as e:
            logger.warning(f"æ›´æ–°éªŒè¯æ•°æ®é›†è¯„ä¼°å™¨ç±»å‹å¤±è´¥: {e}")

            # æ›´æ–°æµ‹è¯•æ•°æ®é›†çš„è¯„ä¼°å™¨ç±»å‹  
            if test_dataset and evaluators.get('test'):
                evaluator = evaluators['test']
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ•°æ®é›†
                if data_loader.is_multi_dataset(test_dataset):
                    # å¤šæ•°æ®é›†æƒ…å†µï¼šä»SequentialEvaluatorä¸­æå–å­è¯„ä¼°å™¨
                    if isinstance(test_dataset, dict) and hasattr(evaluator, 'evaluators'):
                        # SequentialEvaluatoråŒ…å«å¤šä¸ªå­è¯„ä¼°å™¨
                        dataset_names = list(test_dataset.keys())
                        sub_evaluators = evaluator.evaluators
                        
                        for dataset_name, sub_evaluator in zip(dataset_names, sub_evaluators):
                            try:
                                if dataset_name in data_source_mapping:
                                    data_source_id = data_source_mapping[dataset_name]
                                    sub_evaluator_name = type(sub_evaluator).__name__
                                    
                                    TrainingDatasetService.update_dataset_evaluator_by_source(
                                        task_id=task_id,
                                        data_source_id=data_source_id,
                                        split_type="test",
                                        evaluator=sub_evaluator_name
                                    )
                                    logger.info(f"æ›´æ–°å¤šæ•°æ®é›†è¯„ä¼°å™¨ç±»å‹: {dataset_name} (æºID: {data_source_id}) -> {sub_evaluator_name}")
                                else:
                                    logger.warning(f"æ‰¾ä¸åˆ°æ•°æ®é›† {dataset_name} çš„æ•°æ®æºæ˜ å°„")
                            except Exception as e:
                                logger.warning(f"æ›´æ–°æ•°æ®é›† {dataset_name} è¯„ä¼°å™¨ç±»å‹å¤±è´¥: {e}")
                    else:
                        # ä¸æ˜¯SequentialEvaluatorï¼Œæ‰€æœ‰æ•°æ®é›†ä½¿ç”¨åŒä¸€ä¸ªè¯„ä¼°å™¨
                        evaluator_name = type(evaluator).__name__
                        for dataset_name in test_dataset.keys():
                            try:
                                if dataset_name in data_source_mapping:
                                    data_source_id = data_source_mapping[dataset_name]
                                    TrainingDatasetService.update_dataset_evaluator_by_source(
                                        task_id=task_id,
                                        data_source_id=data_source_id,
                                        split_type="test",
                                        evaluator=evaluator_name
                                    )
                                    logger.info(f"æ›´æ–°å¤šæ•°æ®é›†è¯„ä¼°å™¨ç±»å‹: {dataset_name} (æºID: {data_source_id}) -> {evaluator_name}")
                            except Exception as e:
                                logger.warning(f"æ›´æ–°æ•°æ®é›† {dataset_name} è¯„ä¼°å™¨ç±»å‹å¤±è´¥: {e}")
                else:
                    # å•æ•°æ®é›†æƒ…å†µï¼šä¸ºè¯¥æ•°æ®æºæ›´æ–°è¯„ä¼°å™¨ç±»å‹
                    evaluator_name = type(evaluator).__name__
                    try:
                        # å•æ•°æ®é›†ä¹Ÿè¦é€šè¿‡data_source_idæ›´æ–°
                        for dataset_name, data_source_id in data_source_mapping.items():
                            TrainingDatasetService.update_dataset_evaluator_by_source(
                                task_id=task_id,
                                data_source_id=data_source_id,
                                split_type="test",
                                evaluator=evaluator_name
                            )
                            logger.info(f"æ›´æ–°å•æµ‹è¯•æ•°æ®é›†è¯„ä¼°å™¨ç±»å‹: {dataset_name} (æºID: {data_source_id}) -> {evaluator_name}")
                    except Exception as e:
                        logger.warning(f"æ›´æ–°å•æµ‹è¯•æ•°æ®é›†è¯„ä¼°å™¨ç±»å‹å¤±è´¥: {e}")
                
        except Exception as e:
            logger.warning(f"æ›´æ–°æ•°æ®é›†è¯„ä¼°å™¨ç±»å‹ä¿¡æ¯å¤±è´¥: {e}")

    # 6. è¯„ä¼°åŸºçº¿æ¨¡å‹
    dev_evaluator = None
    if eval_dataset is not None:
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ•°æ®é›†
        if data_loader.is_multi_dataset(eval_dataset):
            # å¤šæ•°æ®é›†ï¼šåˆ›å»º SequentialEvaluator
            dev_evaluator = evaluator_factory.create_multi_evaluator(
                eval_dataset, target_column, run_name
            )
        elif 'dev' in evaluators:
            # å•æ•°æ®é›†ï¼šä½¿ç”¨å•ä¸ªè¯„ä¼°å™¨
            dev_evaluator = evaluators['dev']
        
        # è¯„ä¼°åŸºçº¿æ¨¡å‹
        if dev_evaluator is not None:
            dev_results = evaluator_factory.evaluate_model(model, dev_evaluator)
            print(f"Base model dev results: {dev_results}")
            
            # ä¿å­˜åŸºçº¿è¯„ä¼°ç»“æœåˆ°æ•°æ®åº“
            if task_id and dev_results:
                try:
                    # è·å–éªŒè¯é›†çš„æ•°æ®é›†ID
                    eval_datasets = TrainingDatasetService.get_datasets_by_job_and_split(task_id, "eval")
                    for eval_dataset_info in eval_datasets:
                        dataset_id = eval_dataset_info["id"]
                        dataset_name = eval_dataset_info["dataset_name"]
                        
                        TrainingDatasetService.update_eval_results(
                            dataset_id=dataset_id,
                            base_results=dev_results
                        )
                        logger.info(f"âœ… åŸºçº¿éªŒè¯ç»“æœå·²ä¿å­˜: {dataset_name}")
                except Exception as e:
                    logger.warning(f"ä¿å­˜åŸºçº¿éªŒè¯ç»“æœå¤±è´¥: {e}")
        else:
            logger.info("æ²¡æœ‰æœ‰æ•ˆçš„éªŒè¯è¯„ä¼°å™¨ï¼Œè·³è¿‡åŸºçº¿æ¨¡å‹éªŒè¯é›†è¯„ä¼°")
    
    # è®­ç»ƒå‰è¯„ä¼°æµ‹è¯•é›†åŸºçº¿
    base_test_evaluator = None
    if test_dataset is not None:
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ•°æ®é›†
        if data_loader.is_multi_dataset(test_dataset):
            # å¤šæ•°æ®é›†ï¼šåˆ›å»º SequentialEvaluator
            base_test_evaluator = evaluator_factory.create_multi_evaluator(
                test_dataset, target_column, run_name
            )
        elif 'test' in evaluators:
            # å•æ•°æ®é›†ï¼šä½¿ç”¨å•ä¸ªè¯„ä¼°å™¨
            base_test_evaluator = evaluators['test']
        
        # è¯„ä¼°åŸºçº¿æ¨¡å‹
        if base_test_evaluator is not None:
            base_test_results = evaluator_factory.evaluate_model(model, base_test_evaluator)
            print(f"Base model test results: {base_test_results}")
            
            # ä¿å­˜åŸºçº¿è¯„ä¼°ç»“æœåˆ°æ•°æ®åº“
            if task_id and base_test_results:
                try:
                    # è·å–æµ‹è¯•é›†çš„æ•°æ®é›†ID
                    test_datasets = TrainingDatasetService.get_datasets_by_job_and_split(task_id, "test")
                    for test_dataset_info in test_datasets:
                        dataset_id = test_dataset_info["id"]
                        dataset_name = test_dataset_info["dataset_name"]
                        
                        TrainingDatasetService.update_eval_results(
                            dataset_id=dataset_id,
                            base_results=base_test_results
                        )
                        logger.info(f"âœ… åŸºçº¿æµ‹è¯•ç»“æœå·²ä¿å­˜: {dataset_name}")
                except Exception as e:
                    logger.warning(f"ä¿å­˜åŸºçº¿æµ‹è¯•ç»“æœå¤±è´¥: {e}")
        else:
            logger.info("æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•è¯„ä¼°å™¨ï¼Œè·³è¿‡åŸºçº¿æ¨¡å‹æµ‹è¯•é›†è¯„ä¼°")

    # æ›´æ–°è®­ç»ƒçŠ¶æ€ä¸ºè¿è¡Œä¸­
    if task_id:
        try:
            from bubble_rag.training.mysql_service.service.training_task_service import training_task_service
            from .enums import TrainingStatus
            training_task_service.update_task_status(task_id, TrainingStatus.RUNNING.value)
            logger.info("è®­ç»ƒçŠ¶æ€å·²æ›´æ–°ä¸ºè¿è¡Œä¸­")
        except Exception as e:
            logger.warning(f"æ›´æ–°è®­ç»ƒçŠ¶æ€å¤±è´¥ï¼ˆä¸å½±å“è®­ç»ƒï¼‰: {e}")

    # 7. åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = _create_trainer(train_type, model, args, train_dataset, eval_dataset, loss, dev_evaluator)
    
    # æ·»åŠ æ•°æ®ä¿å­˜å›è°ƒ
    if task_id:
        # ğŸ”§ é‡è¦ï¼šä¿å­˜çœŸæ­£çš„åŸå§‹æ–¹æ³•ï¼Œé¿å…æ— é™é€’å½’
        original_log = getattr(trainer, 'log', None)
        # å¦‚æœlogå·²ç»è¢«åŒ…è£…è¿‡ï¼Œå°è¯•æ‰¾åˆ°çœŸæ­£çš„åŸå§‹æ–¹æ³•
        if hasattr(original_log, '__wrapped__'):
            original_log = getattr(original_log, '__wrapped__', original_log)
        
        def wrapped_log(logs, start_time=None):
            nonlocal step_count  # ç¡®ä¿å¯ä»¥ä¿®æ”¹å¤–å±‚çš„step_count
            try:
                # è°ƒç”¨åŸå§‹logæ–¹æ³•
                if original_log and callable(original_log):
                    result = original_log(logs, start_time)
                else:
                    result = None
                
                # âœ… å¯ç”¨lossæœ¬åœ°æ–‡ä»¶ä¿å­˜åŠŸèƒ½
                if logs and any(key in logs for key in ['train_loss', 'eval_loss', 'loss']):
                    try:
                        from bubble_rag.training.model_sft.utils.loss_manager import get_loss_manager
                        
                        # å¦‚æœlogsä¸­æ²¡æœ‰stepï¼Œä½¿ç”¨å¹¶å¢åŠ step_count
                        if 'step' not in logs:
                            step_count += 1
                        
                        # è·å–å½“å‰æ­¥æ•°å’Œepochä¿¡æ¯
                        current_step = logs.get('step', step_count)  # ä½¿ç”¨step_countä½œä¸ºfallback
                        current_epoch = logs.get('epoch', None)
                        
                        # æ„å»ºlossæŒ‡æ ‡å­—å…¸
                        loss_metrics = {}
                        for key, value in logs.items():
                            if 'loss' in key.lower() or key in ['accuracy', 'f1', 'precision', 'recall']:
                                loss_metrics[key] = value
                        
                        if loss_metrics:
                            # è·å–lossç®¡ç†å™¨å¹¶ä¿å­˜è®°å½•
                            loss_manager = get_loss_manager(output_dir, task_id)
                            loss_manager.save_loss_record(current_step, loss_metrics, current_epoch)
                            
                            logger.debug(f"ğŸ“Š Losså·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶: step={current_step}, metrics={list(loss_metrics.keys())}")
                    
                    except Exception as e:
                        logger.warning(f"âš ï¸ ä¿å­˜lossåˆ°æœ¬åœ°æ–‡ä»¶å¤±è´¥: {e}")
                
                # ä¿å­˜è®­ç»ƒè¯„ä¼°å™¨çš„è¯„ä¼°ç»“æœ
                if logs and dev_evaluator is not None:
                    try:
                        # æ£€æŸ¥æ˜¯å¦æœ‰è¯„ä¼°ç»“æœï¼ˆå„ç§å¯èƒ½çš„é”®åï¼‰
                        eval_keys = [k for k in logs.keys() if any(eval_word in k.lower() for eval_word in ['eval', 'dev', 'accuracy', 'spearman', 'pearson'])]
                        if eval_keys:
                            current_step = logs.get('step', step_count)  # ä½¿ç”¨step_countä½œä¸ºfallback
                            current_epoch = logs.get('epoch', 0)
                            
                            # æå–è¯„ä¼°ç»“æœ
                            eval_results = {k: logs[k] for k in eval_keys if k not in ['eval_loss']}
                            if eval_results:
                                # è·å–éªŒè¯é›†æ•°æ®é›†IDå¹¶ä¿å­˜è¯„ä¼°ç»“æœ
                                eval_datasets = TrainingDatasetService.get_datasets_by_job_and_split(task_id, "eval")
                                for eval_dataset_info in eval_datasets:
                                    dataset_id = eval_dataset_info["id"]
                                    TrainingDatasetService.add_training_evaluator_evaluation(
                                        dataset_id=dataset_id,
                                        eval_results=eval_results,
                                        step=current_step,
                                        epoch=current_epoch
                                    )
                                
                                logger.debug(f"ğŸ’¾ è®­ç»ƒè¯„ä¼°ç»“æœå·²ä¿å­˜: step={current_step}, results={eval_results}")
                    except Exception as e:
                        logger.warning(f"ä¿å­˜è®­ç»ƒè¯„ä¼°ç»“æœå¤±è´¥: {e}")
                
                return result
            except Exception as e:
                logger.warning(f"å›è°ƒå‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
                # ğŸ›¡ï¸ é€’å½’ä¿æŠ¤ï¼šç¡®ä¿åŸå§‹åŠŸèƒ½ä¸å—å½±å“ï¼Œä½†é¿å…æ— é™é€’å½’
                if original_log and callable(original_log) and original_log != wrapped_log:
                    try:
                        return original_log(logs, start_time)
                    except Exception as inner_e:
                        logger.error(f"åŸå§‹logæ–¹æ³•ä¹Ÿå¤±è´¥: {inner_e}")
                        return None
                return None
        
        # æ›¿æ¢logæ–¹æ³•ï¼Œå¹¶æ·»åŠ __wrapped__å±æ€§ç”¨äºé€’å½’ä¿æŠ¤
        wrapped_log.__wrapped__ = original_log
        trainer.log = wrapped_log
    
    # å¦‚æœæœ‰è¿›åº¦å›è°ƒï¼Œä½¿ç”¨æ›´å¯é çš„å›è°ƒæœºåˆ¶
    if progress_callback:
        step_count = 0
        max_steps = args.max_steps if hasattr(args, 'max_steps') and args.max_steps > 0 else None
        if max_steps is None and hasattr(args, 'num_train_epochs'):
            # æ›´å‡†ç¡®çš„æ­¥æ•°ä¼°ç®— - æ·»åŠ è¯¦ç»†è°ƒè¯•
            try:
                # è¯¦ç»†è°ƒè¯•ä¿¡æ¯
                logger.info(f"ğŸ” è°ƒè¯•æ•°æ®é›†å¤§å°è®¡ç®—:")
                logger.info(f"   train_datasetç±»å‹: {type(train_dataset)}")
                logger.info(f"   train_datasetæœ‰__len__: {hasattr(train_dataset, '__len__')}")
                logger.info(f"   train_datasetæ˜¯dict: {isinstance(train_dataset, dict)}")
                
                # æ£€æŸ¥trainerå†…éƒ¨æ•°æ®é›†
                if hasattr(trainer, 'train_dataset'):
                    logger.info(f"   trainer.train_datasetç±»å‹: {type(trainer.train_dataset)}")
                    logger.info(f"   trainer.train_datasetæœ‰__len__: {hasattr(trainer.train_dataset, '__len__')}")
                    if hasattr(trainer.train_dataset, '__len__'):
                        logger.info(f"   trainer.train_datasetå¤§å°: {len(trainer.train_dataset)}")
                else:
                    logger.info(f"   traineræ²¡æœ‰train_datasetå±æ€§")
                
                # ä¼˜å…ˆä½¿ç”¨trainerå†…éƒ¨æ•°æ®é›†è¿›è¡Œè®¡ç®—
                actual_dataset = train_dataset
                if hasattr(trainer, 'train_dataset') and trainer.train_dataset is not None:
                    actual_dataset = trainer.train_dataset
                    logger.info("ğŸ¯ ä½¿ç”¨trainer.train_datasetè¿›è¡Œæ­¥æ•°è®¡ç®—")
                else:
                    logger.info("ğŸ¯ ä½¿ç”¨åŸå§‹train_datasetè¿›è¡Œæ­¥æ•°è®¡ç®—")
                
                if hasattr(actual_dataset, '__len__'):
                    dataset_size = len(actual_dataset)
                    logger.info(f"âœ… å•æ•°æ®é›†å¤§å°: {dataset_size}")
                elif isinstance(actual_dataset, dict):
                    # å¤šæ•°æ®é›†æƒ…å†µ
                    logger.info(f"   å­—å…¸é”®: {list(actual_dataset.keys())}")
                    individual_sizes = []
                    for name, ds in actual_dataset.items():
                        logger.info(f"   æ•°æ®é›† '{name}' ç±»å‹: {type(ds)}")
                        if hasattr(ds, '__len__'):
                            size = len(ds)
                            individual_sizes.append(f"{name}: {size}")
                            logger.info(f"   æ•°æ®é›† '{name}' å¤§å°: {size}")
                        else:
                            individual_sizes.append(f"{name}: æ— æ³•è®¡ç®—")
                            logger.info(f"   æ•°æ®é›† '{name}' æ— æ³•è®¡ç®—å¤§å°")
                    dataset_size = sum(len(ds) for ds in actual_dataset.values() if hasattr(ds, '__len__'))
                    logger.info(f"âœ… å¤šæ•°æ®é›†è¯¦æƒ…: {', '.join(individual_sizes)}, æ€»è®¡: {dataset_size}")
                else:
                    dataset_size = 1000  # é»˜è®¤å€¼
                    logger.warning(f"âš ï¸ æ— æ³•ç¡®å®šæ•°æ®é›†å¤§å°ï¼Œå®é™…æ•°æ®é›†ç±»å‹: {type(actual_dataset)}ï¼Œä½¿ç”¨é»˜è®¤å€¼1000")
                
                # è·å–GPUæ•°é‡
                import torch
                num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1
                if hasattr(args, 'device') and args.device and ',' in str(args.device):
                    # å¦‚æœæŒ‡å®šäº†å¤šä¸ªè®¾å¤‡ï¼Œè®¡ç®—è®¾å¤‡æ•°é‡
                    num_gpus = len(str(args.device).split(','))
                
                batch_size = getattr(args, 'per_device_train_batch_size', 16)
                gradient_accumulation = getattr(args, 'gradient_accumulation_steps', 1)
                effective_batch_size = batch_size * gradient_accumulation * num_gpus
                steps_per_epoch = max(1, dataset_size // effective_batch_size)
                calculated_max_steps = steps_per_epoch * args.num_train_epochs
                logger.info(f"æ•°æ®é›†å¤§å°: {dataset_size}, GPUæ•°é‡: {num_gpus}, æ¯è®¾å¤‡æ‰¹æ¬¡: {batch_size}, æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation}")
                logger.info(f"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}, æ¯è½®æ­¥æ•°: {steps_per_epoch}, è®¡ç®—å‡ºçš„æ€»æ­¥æ•°: {calculated_max_steps}")
                
                # ğŸ”§ ä¿®å¤epochæ˜¾ç¤º0.0çš„é—®é¢˜ï¼šç¡®ä¿åˆç†çš„è®­ç»ƒæ­¥æ•°
                if calculated_max_steps < 10:  # å¦‚æœæ­¥æ•°è¿‡å°‘ï¼Œä¿æŒepoch-basedè®­ç»ƒ
                    # logger.warning(f"âš ï¸ è®¡ç®—å‡ºçš„è®­ç»ƒæ­¥æ•°è¿‡å°‘({calculated_max_steps})ï¼Œä¿æŒepoch-basedè®­ç»ƒ")
                    # ç¡®ä¿epoch-basedè®­ç»ƒçš„å‚æ•°è®¾ç½®æ­£ç¡®
                    args.max_steps = -1  # ç¦ç”¨step-basedè®­ç»ƒ
                    # num_train_epochs ä¿æŒåŸå€¼ï¼Œä¸ä¿®æ”¹
                    logger.info(f"âœ… ä½¿ç”¨epoch-basedè®­ç»ƒ: {args.num_train_epochs} epochs (max_stepsè®¾ä¸º-1)")
                else:
                    # æ­¥æ•°åˆç†ï¼Œä½¿ç”¨step-basedè®­ç»ƒ
                    args.max_steps = calculated_max_steps
                    args.num_train_epochs = -1  # ç¦ç”¨epoch-basedè®­ç»ƒ
                    logger.info(f"âœ… ä½¿ç”¨step-basedè®­ç»ƒ: {calculated_max_steps} steps (num_train_epochsè®¾ä¸º-1)")
            except Exception as e:
                logger.warning(f"æ— æ³•ä¼°ç®—è®­ç»ƒæ­¥æ•°: {e}, ä½¿ç”¨é»˜è®¤å€¼")
                max_steps = 1000
        
        # ğŸ” æœ€ç»ˆå‚æ•°è°ƒè¯•ä¿¡æ¯
        final_max_steps = getattr(args, 'max_steps', None)
        final_num_epochs = getattr(args, 'num_train_epochs', None)
        logger.info(f"ğŸ¯ æœ€ç»ˆè®­ç»ƒå‚æ•°: max_steps={final_max_steps}, num_train_epochs={final_num_epochs}")
        
        # ä½¿ç”¨æ›´å¯é çš„å›è°ƒæœºåˆ¶ - åŒæ—¶åŒ…è£…å¤šä¸ªæ–¹æ³•
        original_log = trainer.log if hasattr(trainer, 'log') else None
        original_training_step = None
        
        # å°è¯•åŒ…è£…è®­ç»ƒæ­¥æ–¹æ³•ï¼ˆæ›´ç›´æ¥çš„è¿›åº¦è¿½è¸ªï¼‰
        if hasattr(trainer, 'training_step'):
            original_training_step = trainer.training_step
            
            def wrapped_training_step(*args, **kwargs):
                nonlocal step_count
                result = original_training_step(*args, **kwargs)
                step_count += 1
                
                # æ·»åŠ è°ƒè¯•æ—¥å¿—
                if step_count % 10 == 0:
                    logger.info(f"ğŸ”§ training_stepè¢«è°ƒç”¨: ç¬¬{step_count}æ­¥")
                
                if max_steps and step_count <= max_steps:
                    try:
                        progress_callback(step_count, max_steps, "è®­ç»ƒä¸­")
                        if step_count % 10 == 0:
                            logger.info(f"ğŸ“ progress_callbackè°ƒç”¨æˆåŠŸ: {step_count}/{max_steps}")
                    except KeyboardInterrupt:
                        logger.info("æ£€æµ‹åˆ°è®­ç»ƒåœæ­¢ä¿¡å·ï¼Œä¸­æ–­è®­ç»ƒ")
                        if hasattr(trainer, 'state'):
                            trainer.state.should_epoch_stop = True
                            trainer.state.should_training_stop = True
                        raise
                    except Exception as e:
                        logger.error(f"âŒ progress_callbackè°ƒç”¨å¤±è´¥: {e}")
                
                return result
            
            trainer.training_step = wrapped_training_step
        
        # æ•´åˆè¿›åº¦å›è°ƒåˆ°å·²æœ‰çš„logåŒ…è£…ä¸­
        if hasattr(trainer, 'log') and hasattr(trainer.log, '__wrapped__'):
            # å¦‚æœlogå·²ç»è¢«åŒ…è£…ï¼ˆæ•°æ®ä¿å­˜å›è°ƒï¼‰ï¼Œåˆ™æ·»åŠ è¿›åº¦åŠŸèƒ½
            existing_wrapped_log = trainer.log
            existing_original_log = getattr(existing_wrapped_log, '__wrapped__', None)
            
            def combined_wrapped_log(logs, start_time=None):
                nonlocal step_count
                result = None
                
                # é¦–å…ˆè°ƒç”¨åŸå§‹logæ–¹æ³•ï¼ˆé¿å…é€’å½’ï¼‰
                try:
                    if existing_original_log and callable(existing_original_log):
                        result = existing_original_log(logs, start_time)
                except Exception as e:
                    logger.error(f"åŸå§‹logæ–¹æ³•è°ƒç”¨å¤±è´¥: {e}")
                
                # âœ… å¯ç”¨lossæœ¬åœ°æ–‡ä»¶ä¿å­˜åŠŸèƒ½
                try:
                    if logs and task_id and any(key in logs for key in ['train_loss', 'eval_loss', 'loss']):
                        from bubble_rag.training.model_sft.utils.loss_manager import get_loss_manager
                        
                        # å¦‚æœlogsä¸­æ²¡æœ‰stepï¼Œä½¿ç”¨å¹¶å¢åŠ step_count
                        if 'step' not in logs:
                            step_count += 1
                        
                        # è·å–å½“å‰æ­¥æ•°å’Œepochä¿¡æ¯
                        current_step = logs.get('step', step_count)  # ä½¿ç”¨step_countä½œä¸ºfallback
                        current_epoch = logs.get('epoch', None)
                        
                        # æ„å»ºlossæŒ‡æ ‡å­—å…¸
                        loss_metrics = {}
                        for key, value in logs.items():
                            if 'loss' in key.lower() or key in ['accuracy', 'f1', 'precision', 'recall']:
                                loss_metrics[key] = value
                        
                        if loss_metrics:
                            # è·å–lossç®¡ç†å™¨å¹¶ä¿å­˜è®°å½•
                            loss_manager = get_loss_manager(output_dir, task_id)
                            loss_manager.save_loss_record(current_step, loss_metrics, current_epoch)
                            
                            logger.debug(f"ğŸ“Š Losså·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶: step={current_step}, metrics={list(loss_metrics.keys())}")
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜lossåˆ°æœ¬åœ°æ–‡ä»¶å¤±è´¥: {e}")
                
                # æœ€åæ‰§è¡Œè¿›åº¦å›è°ƒ
                try:
                    # ğŸ”§ å¤„ç†è¿›åº¦å›è°ƒï¼ˆä¼˜å…ˆä½¿ç”¨stepä¿¡æ¯ï¼Œfallbackåˆ°step_countï¼‰
                    if logs and 'step' in logs:
                        current_step = logs['step']
                        if max_steps and current_step <= max_steps:
                            progress_callback(current_step, max_steps, "è®­ç»ƒä¸­")
                            if current_step % 10 == 0:
                                logger.info(f"ğŸ“ logè¿›åº¦å›è°ƒæˆåŠŸ: {current_step}/{max_steps}")
                    elif not original_training_step:
                        # å¦‚æœlogsä¸­æ²¡æœ‰stepä¿¡æ¯ä¸”training_stepæœªè¢«åŒ…è£…ï¼Œä½¿ç”¨step_count
                        step_count += 1
                        if step_count % 10 == 0:
                            logger.info(f"ğŸ”§ logæ–¹æ³•è¢«è°ƒç”¨: ç¬¬{step_count}æ­¥")
                        
                        if max_steps and step_count <= max_steps:
                            progress_callback(step_count, max_steps, "è®­ç»ƒä¸­")
                            if step_count % 10 == 0:
                                logger.info(f"ğŸ“ logä¸­çš„progress_callbackè°ƒç”¨æˆåŠŸ: {step_count}/{max_steps}")
                except KeyboardInterrupt:
                    logger.info("æ£€æµ‹åˆ°è®­ç»ƒåœæ­¢ä¿¡å·ï¼Œä¸­æ–­è®­ç»ƒ")
                    if hasattr(trainer, 'state'):
                        trainer.state.should_epoch_stop = True
                        trainer.state.should_training_stop = True
                    raise
                except Exception as e:
                    logger.error(f"âŒ è¿›åº¦å›è°ƒå¤±è´¥: {e}")
                
                return result
            
            combined_wrapped_log.__wrapped__ = existing_original_log
            trainer.log = combined_wrapped_log
        
        logger.info(f"å·²è®¾ç½®è®­ç»ƒè¿›åº¦å›è°ƒï¼Œé¢„ä¼°æ€»æ­¥æ•°: {max_steps}")
        # åˆå§‹è¿›åº¦æ›´æ–°
        try:
            progress_callback(0, max_steps or 1, "å¼€å§‹è®­ç»ƒ")
            logger.info("âœ… åˆå§‹è¿›åº¦å›è°ƒè°ƒç”¨æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ åˆå§‹è¿›åº¦å›è°ƒè°ƒç”¨å¤±è´¥: {e}")
            
        # è°ƒè¯•ä¿¡æ¯
        if hasattr(trainer, 'training_step'):
            logger.info("âœ… è®­ç»ƒå™¨æœ‰training_stepæ–¹æ³•ï¼Œå·²åŒ…è£…")
        else:
            logger.warning("âš ï¸ è®­ç»ƒå™¨æ²¡æœ‰training_stepæ–¹æ³•")
            
        if hasattr(trainer, 'log'):
            logger.info("âœ… è®­ç»ƒå™¨æœ‰logæ–¹æ³•ï¼Œå·²åŒ…è£…")
        else:
            logger.warning("âš ï¸ è®­ç»ƒå™¨æ²¡æœ‰logæ–¹æ³•")
    
    # è®­ç»ƒå‰çš„æœ€åè®¾å¤‡æ£€æŸ¥
    logger.info(f"ğŸ” è®­ç»ƒå¼€å§‹å‰çš„è®¾å¤‡æ£€æŸ¥:")
    logger.info(f"   æ¨¡å‹è®¾å¤‡: {getattr(model, 'device', 'unknown')}")
    if hasattr(trainer, 'model'):
        logger.info(f"   è®­ç»ƒå™¨æ¨¡å‹è®¾å¤‡: {getattr(trainer.model, 'device', 'unknown')}")
    
    # è®­ç»ƒå‰æœ€ç»ˆéªŒè¯trainerå†…éƒ¨çš„æ•°æ®é›†æ ¼å¼
    if hasattr(trainer, 'train_dataset'):
        internal_train_dataset = trainer.train_dataset
        if isinstance(internal_train_dataset, dict):
            logger.info(f"ğŸ” Trainerå†…éƒ¨å¤šæ•°æ®é›†éªŒè¯:")
            for name, dataset in internal_train_dataset.items():
                logger.info(f"   æ•°æ®é›† '{name}': {len(dataset.column_names)}åˆ— - {dataset.column_names}")
        else:
            logger.info(f"ğŸ” Trainerå†…éƒ¨å•æ•°æ®é›†éªŒè¯: {len(internal_train_dataset.column_names)}åˆ— - {internal_train_dataset.column_names}")
    
    # æ£€æŸ¥trainerçš„ç­¾ååˆ—è®¾ç½®
    if hasattr(trainer, '_signature_columns'):
        logger.info(f"ğŸ” Trainerç­¾ååˆ—: {trainer._signature_columns}")
    else:
        # å¼ºåˆ¶è®¾ç½®ç­¾ååˆ—
        trainer._set_signature_columns_if_needed()
        logger.info(f"ğŸ” Trainerè®¾ç½®åçš„ç­¾ååˆ—: {trainer._signature_columns}")
    
    # æ£€æŸ¥remove_unused_columnsè®¾ç½®
    logger.info(f"ğŸ” remove_unused_columnsè®¾ç½®: {trainer.args.remove_unused_columns}")
    
    # å¼ºåˆ¶ç¦ç”¨remove_unused_columnsä»¥ç¡®ä¿æ•°æ®æ ¼å¼æ§åˆ¶
    if trainer.args.remove_unused_columns:
        logger.info("ğŸ”§ å¼ºåˆ¶è®¾ç½® remove_unused_columns=False ä»¥ç¡®ä¿æ•°æ®æ ¼å¼æ§åˆ¶")
        trainer.args.remove_unused_columns = False
    
    # æ£€æŸ¥trainerçš„è¾“å‡ºç›®å½•è®¾ç½®
    logger.info(f"ğŸ” Trainerè¾“å‡ºç›®å½•: {trainer.args.output_dir}")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    import os
    if not os.path.exists(trainer.args.output_dir):
        os.makedirs(trainer.args.output_dir, exist_ok=True)
        logger.info(f"ğŸ”§ åˆ›å»ºè¾“å‡ºç›®å½•: {trainer.args.output_dir}")
    
    # åˆ›å»ºevalå­ç›®å½•ï¼ˆCrossEncoderCorrelationEvaluatoréœ€è¦ï¼‰
    eval_dir = os.path.join(trainer.args.output_dir, "eval")
    if not os.path.exists(eval_dir):
        os.makedirs(eval_dir, exist_ok=True)
        logger.info(f"ğŸ”§ åˆ›å»ºè¯„ä¼°è¾“å‡ºç›®å½•: {eval_dir}")
    
    logger.info(f"âœ… ä½¿ç”¨output_dir: {trainer.args.output_dir}")
    logger.info(f"âœ… ä½¿ç”¨eval_dir: {eval_dir}")
    
    # ä¸ºè¯„ä¼°å™¨é¢„åˆ›å»ºå¯èƒ½éœ€è¦çš„å­ç›®å½•ç»“æ„
    # CrossEncoderCorrelationEvaluator ä¼šåˆ›å»ºä»¥è¯„ä¼°å™¨åç§°å‘½åçš„å­ç›®å½•
    try:
        # è·å–æ¨¡å‹åç§°ï¼Œç”¨äºæ„å»ºè¯„ä¼°å™¨ç›®å½•å - éœ€è¦ä¸è¯„ä¼°å™¨å®é™…ä½¿ç”¨çš„åç§°ä¸€è‡´
        model_short_name = os.path.basename(trainer.model.config.name_or_path) if hasattr(trainer.model, 'config') and hasattr(trainer.model.config, 'name_or_path') else "model"
        logger.info(f"ğŸ” æ¨¡å‹åç§°è°ƒè¯•: {model_short_name}")
        
        # å¯èƒ½çš„è¯„ä¼°å™¨ç›®å½•åå˜ä½“ - è¦†ç›–ä¸åŒçš„å‘½åè§„åˆ™
        potential_eval_subdirs = [
            # åŸå§‹æ ¼å¼
            f"CrossEncoderCorrelationEvaluator_{model_short_name}",
            f"CrossEncoderClassificationEvaluator_{model_short_name}",
            # å¸¦rerankerå‰ç¼€çš„æ ¼å¼
            f"CrossEncoderCorrelationEvaluator_reranker-{model_short_name}",
            f"CrossEncoderClassificationEvaluator_reranker-{model_short_name}",
            # å¸¦sentence-transformersåç¼€çš„æ ¼å¼
            f"CrossEncoderCorrelationEvaluator_{model_short_name}-sentence-transformers",
            f"CrossEncoderCorrelationEvaluator_reranker-{model_short_name}-sentence-transformers",
        ]
        
        for subdir in potential_eval_subdirs:
            eval_subdir_path = os.path.join(eval_dir, subdir)
            os.makedirs(eval_subdir_path, exist_ok=True)
            logger.info(f"ğŸ”§ é¢„åˆ›å»ºè¯„ä¼°å™¨ç›®å½•: {subdir}")
        logger.info(f"âœ… é¢„åˆ›å»ºè¯„ä¼°å™¨å­ç›®å½•å®Œæˆ")
    except Exception as e:
        logger.warning(f"é¢„åˆ›å»ºè¯„ä¼°å™¨å­ç›®å½•å¤±è´¥ï¼Œä½†ç»§ç»­è®­ç»ƒ: {e}")
    
    try:
        trainer.train()
    except KeyboardInterrupt:
        logger.info("è®­ç»ƒè¢«ç”¨æˆ·åœæ­¢")
        raise
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        raise

    # 8. è®­ç»ƒå®Œæˆåçš„æœ€ç»ˆè¯„ä¼°
    if progress_callback:
        progress_callback(1, 1, "è¯„ä¼°æ¨¡å‹æ€§èƒ½")
    
    # 8.1 éªŒè¯é›†æœ€ç»ˆè¯„ä¼°
    if eval_dataset is not None:
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ•°æ®é›†
        if data_loader.is_multi_dataset(eval_dataset):
            # å¤šæ•°æ®é›†ï¼šåˆ›å»º SequentialEvaluator
            final_eval_evaluator = evaluator_factory.create_multi_evaluator(
                eval_dataset, target_column, run_name
            )
        elif 'dev' in evaluators:
            # å•æ•°æ®é›†ï¼šä½¿ç”¨å•ä¸ªè¯„ä¼°å™¨
            final_eval_evaluator = evaluators['dev']
        else:
            final_eval_evaluator = None
        
        # è¯„ä¼°è®­ç»ƒåéªŒè¯é›†
        if final_eval_evaluator is not None:
            final_eval_results = evaluator_factory.evaluate_model(model, final_eval_evaluator)
            print(f"Trained model eval results: {final_eval_results}")
            
            # ä¿å­˜éªŒè¯é›†æœ€ç»ˆè¯„ä¼°ç»“æœåˆ°æ•°æ®åº“
            if task_id and final_eval_results:
                try:
                    # è·å–éªŒè¯é›†çš„æ•°æ®é›†ID
                    eval_datasets = TrainingDatasetService.get_datasets_by_job_and_split(task_id, "eval")
                    for eval_dataset_info in eval_datasets:
                        dataset_id = eval_dataset_info["id"]
                        dataset_name = eval_dataset_info["dataset_name"]
                        
                        TrainingDatasetService.update_eval_results(
                            dataset_id=dataset_id,
                            final_results=final_eval_results
                        )
                        logger.info(f"âœ… éªŒè¯é›†æœ€ç»ˆç»“æœå·²ä¿å­˜: {dataset_name}")
                except Exception as e:
                    logger.warning(f"ä¿å­˜éªŒè¯é›†æœ€ç»ˆç»“æœå¤±è´¥: {e}")
        else:
            logger.info("æ²¡æœ‰æœ‰æ•ˆçš„éªŒè¯è¯„ä¼°å™¨ï¼Œè·³è¿‡éªŒè¯é›†æœ€ç»ˆè¯„ä¼°")
    
    # 8.2 æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°
    if test_dataset is not None:
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ•°æ®é›†
        if data_loader.is_multi_dataset(test_dataset):
            # å¤šæ•°æ®é›†ï¼šåˆ›å»º SequentialEvaluator
            test_evaluator = evaluator_factory.create_multi_evaluator(
                test_dataset, target_column, run_name
            )
        elif 'test' in evaluators:
            # å•æ•°æ®é›†ï¼šä½¿ç”¨å•ä¸ªè¯„ä¼°å™¨
            test_evaluator = evaluators['test']
        else:
            test_evaluator = None
        
        # è¯„ä¼°è®­ç»ƒåæ¨¡å‹
        if test_evaluator is not None:
            test_results = evaluator_factory.evaluate_model(model, test_evaluator)
            print(f"Trained model test results: {test_results}")
            
            # ä¿å­˜æœ€ç»ˆè¯„ä¼°ç»“æœåˆ°æ•°æ®åº“
            if task_id and test_results:
                try:
                    # è·å–æµ‹è¯•é›†çš„æ•°æ®é›†ID
                    test_datasets = TrainingDatasetService.get_datasets_by_job_and_split(task_id, "test")
                    for test_dataset_info in test_datasets:
                        dataset_id = test_dataset_info["id"]
                        dataset_name = test_dataset_info["dataset_name"]
                        
                        TrainingDatasetService.update_eval_results(
                            dataset_id=dataset_id,
                            final_results=test_results
                        )
                        logger.info(f"âœ… æœ€ç»ˆæµ‹è¯•ç»“æœå·²ä¿å­˜: {dataset_name}")
                except Exception as e:
                    logger.warning(f"ä¿å­˜æœ€ç»ˆæµ‹è¯•ç»“æœå¤±è´¥: {e}")
        else:
            logger.info("æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•è¯„ä¼°å™¨ï¼Œè·³è¿‡æµ‹è¯•é›†è¯„ä¼°")

    # 9. ä¿å­˜æ¨¡å‹
    if progress_callback:
        progress_callback(1, 1, "ä¿å­˜æ¨¡å‹")
        
    save_dir = os.path.join(output_dir, "final_model")
    try:
        logger.info(f"å¼€å§‹ä¿å­˜æ¨¡å‹åˆ°: {save_dir}")
        os.makedirs(save_dir, exist_ok=True)
        logger.info(f"åˆ›å»ºä¿å­˜ç›®å½•æˆåŠŸ: {save_dir}")
        
        # ä¿å­˜æ¨¡å‹
        model.save_pretrained(save_dir)
        logger.info(f"âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ: {save_dir}")
        
        # æ£€æŸ¥ä¿å­˜çš„æ–‡ä»¶
        saved_files = os.listdir(save_dir)
        logger.info(f"ä¿å­˜çš„æ–‡ä»¶åˆ—è¡¨: {saved_files}")
        
        # push_to_hub(model, model_name, save_dir)

        # è®­ç»ƒæˆåŠŸå®Œæˆï¼Œä»ä¸´æ—¶æ–‡ä»¶ç®¡ç†å™¨ä¸­ç§»é™¤è¾“å‡ºç›®å½•ï¼ˆå› ä¸ºè¿™æ˜¯æˆåŠŸçš„è¾“å‡ºï¼‰
        temp_file_manager.unregister_temp_dir(output_dir)
        
        # ğŸ”§ æ›´æ–°è®­ç»ƒçŠ¶æ€ä¸ºå·²å®Œæˆï¼ˆåªæœ‰åœ¨å­è¿›ç¨‹ä¸­è¿è¡Œæ—¶æ‰æ›´æ–°ï¼Œé¿å…é‡å¤æ›´æ–°ï¼‰
        if task_id and not hasattr(training_config, '_is_multiprocess_child'):
            try:
                # ä½¿ç”¨ç»Ÿä¸€çš„ä»»åŠ¡ç®¡ç†å™¨æ›´æ–°é€»è¾‘
                from .services.task_manager import task_manager
                task_manager.complete_task(task_id, save_dir)
                task_manager.update_task_progress(task_id, 100.0, "è®­ç»ƒå®Œæˆ")
                
                # ğŸ”§ æ›´æ–°æ•°æ®åº“ä»»åŠ¡çŠ¶æ€ä¸ºSUCCEEDEDï¼ˆä¸unified_training_serviceä¿æŒä¸€è‡´ï¼‰
                from .enums import TrainingStatus
                from bubble_rag.training.mysql_service.service.training_task_service import training_task_service
                training_task_service.update_task_status(task_id, TrainingStatus.SUCCEEDED.value)
                training_task_service.update_task_result(task_id, final_model_path=save_dir)
                logger.info(f"âœ… æ•°æ®åº“ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸ºSUCCEEDED: {task_id}")
                
                # æ›´æ–°è¿›ç¨‹çŠ¶æ€ä¸ºå·²å®Œæˆ
                from .enums.training_task_enums import ProcessStatus
                training_task_service.update_process_info(task_id, None, ProcessStatus.TERMINATED.value)
                
                logger.info(f"è®­ç»ƒçŠ¶æ€å·²æ›´æ–°ä¸ºå·²å®Œæˆï¼Œæ¨¡å‹ä¿å­˜åœ¨: {save_dir}")
                
                # âœ… å®Œæˆlossç®¡ç†å™¨çš„æœ€ç»ˆåŒ–å¤„ç†å¹¶ä¿å­˜æ±‡æ€»åˆ°æ•°æ®åº“
                try:
                    from bubble_rag.training.model_sft.utils.loss_manager import cleanup_loss_manager
                    
                    # è·å–æœ€ç»ˆè®­ç»ƒæŒ‡æ ‡
                    final_metrics = {
                        "final_model_path": save_dir,
                        "saved_files": saved_files,
                        "training_completed": True
                    }
                    
                    # æ¸…ç†lossç®¡ç†å™¨å¹¶è·å–æ•°æ®åº“æ±‡æ€»ä¿¡æ¯
                    loss_summary = cleanup_loss_manager(task_id, final_metrics)
                    
                    if loss_summary:
                        # å°†lossæ±‡æ€»ä¿¡æ¯ä¿å­˜åˆ°æ•°æ®åº“
                        try:
                            import json
                            loss_data_json = json.dumps(loss_summary, ensure_ascii=False)
                            training_task_service.update_task_result(task_id, loss_data=loss_data_json)
                            logger.info(f"âœ… Lossæ±‡æ€»ä¿¡æ¯å·²ä¿å­˜åˆ°æ•°æ®åº“: {len(loss_summary)} é¡¹æŒ‡æ ‡")
                        except Exception as db_e:
                            logger.warning(f"ä¿å­˜lossæ±‡æ€»åˆ°æ•°æ®åº“å¤±è´¥: {db_e}")
                    
                    logger.info("âœ… Lossç®¡ç†å™¨å·²å®Œæˆæœ€ç»ˆåŒ–å¤„ç†")
                except Exception as loss_e:
                    logger.warning(f"Lossç®¡ç†å™¨æ¸…ç†å¤±è´¥: {loss_e}")
                    
            except Exception as e:
                logger.warning(f"æ›´æ–°è®­ç»ƒå®ŒæˆçŠ¶æ€å¤±è´¥ï¼ˆä¸å½±å“ç»“æœï¼‰: {e}")
        
        logger.info(f"è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹ä¿å­˜åœ¨: {save_dir}")
        return model, save_dir
        
    except Exception as e:
        # æ›´æ–°è®­ç»ƒçŠ¶æ€ä¸ºå¤±è´¥
        if task_id:
            try:
                from bubble_rag.training.mysql_service.service.training_task_service import training_task_service
                from .enums import TrainingStatus
                error_msg = f"æ¨¡å‹ä¿å­˜å¤±è´¥: {str(e)}"
                training_task_service.update_task_status(task_id, TrainingStatus.FAILED.value)
                training_task_service.update_task_result(task_id, error_message=error_msg)
                logger.info("è®­ç»ƒçŠ¶æ€å·²æ›´æ–°ä¸ºå¤±è´¥")
            except Exception as status_e:
                logger.warning(f"æ›´æ–°è®­ç»ƒå¤±è´¥çŠ¶æ€å¤±è´¥: {status_e}")
        
        logger.error(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {str(e)}", exc_info=True)
        logger.error(f"ä¿å­˜ç›®å½•: {save_dir}")
        logger.error(f"è¾“å‡ºç›®å½•: {output_dir}")
        raise Exception(f"æ¨¡å‹ä¿å­˜å¤±è´¥: {str(e)}")
    
    except Exception as e:
        # ğŸ”§ å…¨å±€å¼‚å¸¸å¤„ç†ï¼šç¡®ä¿ä»»ä½•è®­ç»ƒå¤±è´¥éƒ½èƒ½æ­£ç¡®æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if task_id:
            try:
                from bubble_rag.training.mysql_service.service.training_task_service import training_task_service
                from .enums import TrainingStatus
                error_msg = f"è®­ç»ƒæ‰§è¡Œå¤±è´¥: {str(e)}"
                training_task_service.update_task_status(task_id, TrainingStatus.FAILED.value)
                training_task_service.update_task_result(task_id, error_message=error_msg)
                logger.info(f"âœ… å…¨å±€å¼‚å¸¸å¤„ç†ï¼šä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸ºFAILED: {task_id}")
            except Exception as status_e:
                logger.warning(f"å…¨å±€å¼‚å¸¸å¤„ç†ï¼šæ›´æ–°è®­ç»ƒå¤±è´¥çŠ¶æ€å¤±è´¥: {status_e}")
        
        logger.error(f"âŒ è®­ç»ƒæ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        raise

if __name__ == "__main__":
    main()