"""
Device management for training.

Handles GPU detection, configuration, and device allocation.
"""

import os
import logging
import torch

logger = logging.getLogger(__name__)


class DeviceManager:
    """Manages device configuration and GPU setup for training."""

    @staticmethod
    def cleanup_gpu_environment():
        """
        æ¸…ç†GPUç¯å¢ƒï¼Œç§»é™¤å¯èƒ½çš„æ®‹ç•™çŠ¶æ€
        åœ¨è®­ç»ƒå¼€å§‹å‰è°ƒç”¨ï¼Œç¡®ä¿å¹²å‡€çš„GPUç¯å¢ƒ
        """
        try:
            import torch
            if torch.cuda.is_available():
                logger.info("ğŸ§¹ æ¸…ç†GPUç¯å¢ƒ...")

                # æ¸…ç©ºæ‰€æœ‰GPUç¼“å­˜
                torch.cuda.empty_cache()

                # æ”¶é›†IPCå†…å­˜
                try:
                    torch.cuda.ipc_collect()
                except:
                    pass

                # é‡ç½®CUDAä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå¯èƒ½ï¼‰
                try:
                    torch.cuda.reset_accumulated_memory_stats()
                except:
                    pass

                logger.info(f"âœ… GPUç¯å¢ƒæ¸…ç†å®Œæˆï¼Œå¯è§GPUæ•°é‡: {torch.cuda.device_count()}")
            else:
                logger.info("CPUæ¨¡å¼ï¼Œè·³è¿‡GPUç¯å¢ƒæ¸…ç†")

        except Exception as e:
            logger.error(f"âŒ GPUç¯å¢ƒæ¸…ç†å¤±è´¥ï¼Œå¯èƒ½å½±å“åç»­GPUä½¿ç”¨ã€‚é”™è¯¯: {e}")

    @staticmethod
    def get_training_device(user_device=None):
        """
        è·å–è®­ç»ƒè®¾å¤‡é…ç½®ï¼ˆåŒ¹é…åŸtrain.pyé€»è¾‘ï¼‰

        Args:
            user_device: ç”¨æˆ·æŒ‡å®šçš„è®¾å¤‡é…ç½®ï¼ˆè¢«å¿½ç•¥ï¼Œå› ä¸ºAPIå±‚ä¼šå¤„ç†å¹¶è®¾ç½®ç¯å¢ƒå˜é‡ï¼‰

        Returns:
            str: è®¾å¤‡å­—ç¬¦ä¸²ï¼Œ'cuda' æˆ– 'cpu'
        """
        # å®Œå…¨éµå¾ªåŸtrain.pyçš„å¤„ç†é€»è¾‘ï¼š
        # 1. ç”¨æˆ·è®¾å¤‡å‚æ•°ç”±APIå±‚çš„GPUResourceManagerå¤„ç†å¹¶è®¾ç½®CUDA_VISIBLE_DEVICES
        # 2. è®­ç»ƒå±‚åªéœ€è¦ç®€å•åœ°æ£€æŸ¥ç¯å¢ƒå˜é‡å¹¶è¿”å›'cuda'æˆ–'cpu'

        cuda_visible = os.getenv("CUDA_VISIBLE_DEVICES")

        if cuda_visible:
            # æœ‰CUDA_VISIBLE_DEVICESï¼Œä½¿ç”¨GPUè®­ç»ƒ
            if torch.cuda.is_available():
                visible_gpus = cuda_visible.split(",")
                gpu_count = torch.cuda.device_count()  # è¿™æ˜¯å¯è§çš„GPUæ•°é‡ï¼Œä¸æ˜¯ç³»ç»Ÿæ€»æ•°
                logger.info(f"ğŸ”§ CUDA_VISIBLE_DEVICES: {cuda_visible}")
                logger.info(f"ğŸ–¥ï¸  å°†ä½¿ç”¨ {len(visible_gpus)} ä¸ªæŒ‡å®šçš„GPUè¿›è¡Œè®­ç»ƒ")

                # æ˜¾ç¤ºæ¯ä¸ªå¯è§GPUçš„è¯¦ç»†ä¿¡æ¯
                for i in range(gpu_count):  # éå†torchèƒ½çœ‹åˆ°çš„GPU
                    try:
                        gpu_name = torch.cuda.get_device_name(i)
                        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                        original_id = visible_gpus[i] if i < len(visible_gpus) else "unknown"
                        logger.info(f"   GPU {i} (åŸå§‹GPU{original_id}): {gpu_name} ({gpu_memory:.1f}GB)")
                    except Exception as e:
                        logger.info(f"   GPU {i}: ä¿¡æ¯è·å–å¤±è´¥ - {e}")
                return "cuda"
            else:
                logger.warning("âš ï¸  è®¾ç½®äº†CUDA_VISIBLE_DEVICESä½†ç³»ç»Ÿä¸æ”¯æŒCUDAï¼Œå›é€€åˆ°CPU")
                return "cpu"
        else:
            # æ²¡æœ‰CUDA_VISIBLE_DEVICESï¼Œå¯èƒ½æ˜¯CPUæ¨¡å¼æˆ–autoæ¨¡å¼
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                logger.info(f"ğŸ–¥ï¸  æœªæŒ‡å®šCUDA_VISIBLE_DEVICESï¼Œæ£€æµ‹åˆ° {gpu_count} ä¸ªGPUå¯ç”¨")
                for i in range(gpu_count):
                    try:
                        gpu_name = torch.cuda.get_device_name(i)
                        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                        logger.info(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
                    except:
                        logger.info(f"   GPU {i}: ä¿¡æ¯è·å–å¤±è´¥")
                return "cuda"
            else:
                logger.info("ğŸ–¥ï¸  ç³»ç»Ÿä¸æ”¯æŒCUDAï¼Œä½¿ç”¨CPUè®­ç»ƒ")
                return "cpu"
    
    @staticmethod
    def prepare_model_for_training(model, device):
        """
        ä¸ºè®­ç»ƒå‡†å¤‡æ¨¡å‹ï¼ˆåŒ¹é…åŸtrain.pyé€»è¾‘ï¼‰

        Args:
            model: è¦å‡†å¤‡çš„æ¨¡å‹
            device: è®¾å¤‡é…ç½®å­—ç¬¦ä¸²ï¼Œ'cuda' æˆ– 'cpu'

        Returns:
            å‡†å¤‡å¥½çš„æ¨¡å‹
        """
        try:
            # æ£€æŸ¥å¹¶ç§»é™¤ä»»ä½•DataParallelåŒ…è£…ï¼ˆåŒ¹é…åŸtrain.pyé€»è¾‘ï¼‰
            if hasattr(model, '_modules'):
                for module_name, module in model._modules.items():
                    if isinstance(module, torch.nn.DataParallel):
                        # æå–åŸå§‹æ¨¡å—ï¼Œç§»é™¤DataParallelåŒ…è£…
                        original_module = module.module
                        model._modules[module_name] = original_module
                        logger.info(f"ğŸ”“ ç§»é™¤æ¨¡å— {module_name} çš„DataParallelåŒ…è£…")

            logger.info(f"âœ… æ¨¡å‹è®­ç»ƒå‡†å¤‡å®Œæˆ")
            return model

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹å‡†å¤‡å¤±è´¥: {str(e)}")
            logger.warning(f"âš ï¸  å°†ç»§ç»­ä½¿ç”¨åŸå§‹æ¨¡å‹é…ç½®")
            return model
    
    @staticmethod
    def get_gpu_count(device_config=None):
        """
        è·å–å¯ç”¨çš„GPUæ•°é‡
        
        Args:
            device_config: è®¾å¤‡é…ç½®å­—ç¬¦ä¸²ï¼Œå¦‚ 'cuda:0,1'
            
        Returns:
            int: GPUæ•°é‡
        """
        if not torch.cuda.is_available():
            return 0
        
        if device_config and ',' in str(device_config):
            # ä»è®¾å¤‡é…ç½®ä¸­è®¡ç®—
            return len(str(device_config).split(','))
        else:
            # ä½¿ç”¨ç³»ç»Ÿæ£€æµ‹åˆ°çš„GPUæ•°é‡
            return torch.cuda.device_count()