"""
Configuration builder for training.

Handles creation and validation of training configurations using Pydantic.
"""

import logging
from typing import Dict, Any, Optional
from datetime import datetime
from ..models.training_parameters import TrainingParameters, TrainingParametersManager

logger = logging.getLogger(__name__)


class ConfigBuilder:
    """Builds and validates training configurations using Pydantic."""
    
    def __init__(self, training_config: Dict[str, Any]):
        """
        Initialize config builder.
        
        Args:
            training_config: Raw training configuration dictionary
        """
        self.raw_config = training_config
        
    def build_training_config(self, train_type: str) -> Dict[str, Any]:
        """
        Build validated training configuration using Pydantic.

        Args:
            train_type: Training type ('embedding' or 'reranker')

        Returns:
            Validated training configuration dictionary
        """
        logger.info(f"æž„å»º {train_type} è®­ç»ƒé…ç½®")

        # å‡†å¤‡é…ç½®æ•°æ®
        config_data = self.raw_config.copy()

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ï¼ˆå› ä¸ºå®˜æ–¹é»˜è®¤æ˜¯Noneï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®é»˜è®¤å€¼ï¼‰
        if not config_data.get("output_dir"):
            config_data["output_dir"] = self._generate_default_output_dir(train_type)



        # å¤„ç†SwanLabé…ç½®
        config_data = self._handle_swanlab_config(config_data)

        # ä½¿ç”¨å‡çº§åŽçš„ TrainingParametersManager è¿›è¡ŒéªŒè¯å’Œè½¬æ¢
        try:
            param_manager = TrainingParametersManager()
            param_manager.load_from_config(config_data)

            # èŽ·å–å®˜æ–¹è®­ç»ƒå‚æ•°ï¼ˆä¼šä¼ ç»™ *TrainingArgumentsï¼‰
            training_args_dict = param_manager.get_training_args_dict()

            logger.info(f"è®­ç»ƒé…ç½®æž„å»ºå®Œæˆ: {len(training_args_dict)} ä¸ªå®˜æ–¹è®­ç»ƒå‚æ•°")
            return training_args_dict

        except Exception as e:
            logger.error(f"é…ç½®éªŒè¯å¤±è´¥: {e}")
            # å¦‚æžœæœ‰è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼Œè®°å½•ä¸‹æ¥
            if hasattr(e, 'errors'):
                for error in e.errors():
                    logger.error(f"é…ç½®é”™è¯¯: {error.get('loc', [])} - {error.get('msg', '')} (è¾“å…¥å€¼: {error.get('input', 'N/A')})")
            raise ValueError(f"è®­ç»ƒé…ç½®éªŒè¯å¤±è´¥: {e}") from e


    def _generate_default_output_dir(self, train_type: str) -> str:
        """Generate default output directory."""
        model_name = self.raw_config.get("model_name_or_path", "unknown-model")
        model_name = model_name.replace("/", "-")
        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        return f"output/training_{train_type}_{model_name}_{timestamp}"
    

    def _handle_swanlab_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Handle SwanLab configuration to prevent initialization errors."""
        report_to = config.get('report_to', 'none')
        logger.info(f"ðŸ” SwanLabé…ç½®æ£€æŸ¥ - report_to: {report_to}, ç±»åž‹: {type(report_to)}")

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†SwanLab
        if isinstance(report_to, str) and 'swanlab' in report_to.lower():
            # æ£€æŸ¥æ˜¯å¦æœ‰APIå¯†é’¥
            swanlab_api_key = self.raw_config.get('swanlab_api_key', '')
            if not swanlab_api_key:
                logger.warning("SwanLabå·²é…ç½®ä½†ç¼ºå°‘APIå¯†é’¥ï¼Œå°†ç¦ç”¨SwanLab")
                # ä»Žreport_toä¸­ç§»é™¤swanlab
                if report_to == 'swanlab':
                    config['report_to'] = 'none'
                else:
                    # å¤„ç†å¤šä¸ªæŠ¥å‘Šå·¥å…·çš„æƒ…å†µ
                    report_tools = [tool.strip() for tool in report_to.split(',')]
                    filtered_tools = [tool for tool in report_tools if 'swanlab' not in tool.lower()]
                    config['report_to'] = ','.join(filtered_tools) if filtered_tools else 'none'

                logger.info(f"SwanLabå·²ä»Žreport_toä¸­ç§»é™¤ï¼Œå½“å‰report_to: {config['report_to']}")

        return config

    
    def get_data_config(self) -> Dict[str, Any]:
        """Get data-related configuration."""
        return {
            'dataset_name_or_path': self.raw_config.get('dataset_name_or_path'),
            'HF_subset': self.raw_config.get('HF_subset'),
            'train_sample_size': self.raw_config.get('train_sample_size', -1),
            'eval_sample_size': self.raw_config.get('eval_sample_size', -1),
            'test_sample_size': self.raw_config.get('test_sample_size', -1)
        }
    
    def get_model_config(self) -> Dict[str, Any]:
        """Get model-related configuration."""
        return {
            'model_name_or_path': self.raw_config.get('model_name_or_path'),
            'train_type': self.raw_config.get('train_type', 'embedding').lower()
        }