"""
Abstract base trainer for all training types.

Defines the common interface and shared functionality for embedding and reranker training.
"""

import logging
import os
from abc import ABC, abstractmethod
from typing import Any, Dict, Tuple, Optional, Union
from dataclasses import dataclass
from datetime import datetime

from .device_manager import DeviceManager
from .config_builder import ConfigBuilder
from ..utils.data_loader import DataLoader
from ..utils.evaluation import UnifiedEvaluator
from ..models.training_parameters import TrainingParametersManager

logger = logging.getLogger(__name__)


@dataclass
class TrainingResult:
    """Training result container."""
    model: Any
    save_dir: str
    final_metrics: Optional[Dict[str, float]] = None
    training_stats: Optional[Dict[str, Any]] = None


class BaseTrainer(ABC):
    """
    Abstract base class for all trainers.
    
    Defines the common training pipeline and abstract methods that 
    must be implemented by specific trainer types.
    """
    
    def __init__(self, training_config: Dict[str, Any]):
        """
        Initialize base trainer.
        
        Args:
            training_config: Complete training configuration dictionary
        """
        self.raw_config = training_config
        self.config_builder = ConfigBuilder(training_config)
        self.device_manager = DeviceManager()
        self.data_loader = None
        self.evaluator_factory = None
        
        # Extract common configurations
        self.model_config = self.config_builder.get_model_config()
        self.data_config = self.config_builder.get_data_config()
        self.train_type = self.model_config['train_type']

        # Cache for target column to avoid repeated computation
        self._cached_target_column = None

        logger.info(f"åˆå§‹åŒ– {self.train_type} è®­ç»ƒå™¨")
    
    def train(self, progress_callback=None) -> TrainingResult:
        """
        Execute the complete training pipeline.
        
        Args:
            progress_callback: Optional progress callback function
            
        Returns:
            TrainingResult containing model and metadata
        """
        logger.info(f"å¼€å§‹ {self.train_type} è®­ç»ƒ")
        task_id = self.raw_config.get('task_id')

        try:
            # Step 1: Initialize components
            self._initialize_components()
            
            # Step 2: Load and prepare data
            datasets = self._load_datasets()

            # Step 2.5: Record dataset information to database
            self._record_dataset_info(datasets)

            # Step 3: Initialize model and loss
            model, loss = self._initialize_model_and_loss(datasets['train'])

            # Step 3.5: Update loss function information in database
            self._update_loss_function_info(datasets, loss)

            # Step 3.6: Update model information in database
            self._update_model_info(model)

            # Step 4: Create training configuration
            training_args = self._create_training_args()
            
            # Step 5: Create evaluators (matching original train.py logic)
            evaluators = self._create_evaluators_from_datasets(datasets)
            evaluator = evaluators.get('dev')

            # Step 5.5: Update evaluator information in database
            self._update_evaluator_info(datasets, evaluators)

            # Step 6: Create trainer instance
            trainer = self.create_trainer_instance(
                model=model,
                args=training_args,
                train_dataset=datasets['train'],
                eval_dataset=datasets.get('eval'),
                loss=loss,
                evaluator=evaluator
            )
            
            # Step 7: Pre-training baseline evaluation
            self._perform_baseline_evaluation(model, datasets, evaluators)

            # Step 8: Show Tensorboard logging info (matching original train.py)
            from .base_trainer_tensorboard import show_tensorboard_info
            show_tensorboard_info(self, training_args)

            # Step 9: Update task status to RUNNING - çœŸæ­£å¼€å§‹è®­ç»ƒ
            self._update_task_status_running(task_id)

            # Step 10: Execute training
            result = self._execute_training(trainer, model, progress_callback)

            # Step 11: è®­ç»ƒå®Œæˆï¼Œè®¾ç½®ä¸ºPENDINGçŠ¶æ€è¿›è¡Œæœ€ç»ˆè¯„ä¼°
            self._update_task_status_post_training_evaluation(task_id)

            # Step 12: Post-training final evaluation
            self._perform_final_evaluation(result.model, datasets, evaluators)

            # Step 13: Update task status to SUCCEEDED (matching original train.py)
            self._update_task_status_succeeded(task_id, result.save_dir)

            logger.info(f"{self.train_type} è®­ç»ƒå®Œæˆ")
            return result

        except Exception as e:
            # Update task status to FAILED (matching original train.py)
            self._update_task_status_failed(task_id, str(e))
            logger.error(f"{self.train_type} è®­ç»ƒå¤±è´¥: {e}")
            raise
        finally:
            # æœ€ç»ˆä¿é™©ï¼šæ— è®ºæˆåŠŸå¤±è´¥éƒ½ç¡®ä¿é‡Šæ”¾GPUèµ„æº
            if task_id:
                try:
                    from ..utils.gpu_resource_manager import gpu_resource_manager
                    success = gpu_resource_manager.release_gpus_for_task(task_id)
                    if success:
                        logger.info(f"Finallyå—ï¼šç¡®ä¿é‡Šæ”¾ä»»åŠ¡ {task_id} çš„GPUèµ„æº")
                    else:
                        logger.warning(f"Finallyå—ï¼šå¸¸è§„GPUé‡Šæ”¾å¤±è´¥ï¼Œå°è¯•å¼ºåˆ¶é‡Šæ”¾")
                        gpu_resource_manager.force_release_gpu_for_task(task_id)
                except Exception as e:
                    logger.critical(f"ä¸¥é‡é”™è¯¯ï¼šFinallyå—GPUèµ„æºé‡Šæ”¾å¤±è´¥ï¼å°è¯•å¼ºåˆ¶æ¢å¤ã€‚ä»»åŠ¡: {task_id}, é”™è¯¯: {e}")
                    try:
                        from ..utils.gpu_resource_manager import gpu_resource_manager
                        gpu_resource_manager.force_release_gpu_for_task(task_id)
                        logger.warning(f"å¼ºåˆ¶GPUé‡Šæ”¾å·²æ‰§è¡Œï¼Œè¯·æ£€æŸ¥èµ„æºçŠ¶æ€")
                    except Exception as force_error:
                        logger.critical(f"å¼ºåˆ¶GPUé‡Šæ”¾ä¹Ÿå¤±è´¥ï¼ä»»åŠ¡ {task_id} çš„GPUèµ„æºå¯èƒ½æ°¸ä¹…æ³„æ¼: {force_error}")
    
    def _initialize_components(self):
        """Initialize data loader and evaluator factory."""
        self.data_loader = DataLoader(
            hf_subset=self.data_config.get('HF_subset'),
            train_sample_size=self.data_config.get('train_sample_size', -1),
            eval_sample_size=self.data_config.get('eval_sample_size', -1),
            test_sample_size=self.data_config.get('test_sample_size', -1)
        )
        
        self.evaluator_factory = UnifiedEvaluator(self.train_type)
    
    def _load_datasets(self) -> Dict[str, Any]:
        """Load and return datasets."""
        dataset_path = self.data_config['dataset_name_or_path']
        logger.info(f"åŠ è½½æ•°æ®é›†: {dataset_path}")
        
        train_dataset, eval_dataset, test_dataset = self.data_loader.load_all_splits(dataset_path)
        
        if train_dataset is None:
            raise ValueError(f"è®­ç»ƒæ•°æ®é›†åŠ è½½å¤±è´¥: {dataset_path}")
        
        return {
            'train': train_dataset,
            'eval': eval_dataset,
            'test': test_dataset
        }
    
    def _initialize_model_and_loss(self, train_dataset) -> Tuple[Any, Any]:
        """Initialize model and loss function."""
        model_name = self.model_config['model_name_or_path']
        
        # Initialize model
        model = self.initialize_model(model_name)
        
        # Prepare model for training
        user_device = self.raw_config.get('device')
        device = self.device_manager.get_training_device(user_device)
        model = self.device_manager.prepare_model_for_training(model, device)
        
        # Create loss function
        loss = self.create_loss_function(model, train_dataset)
        
        return model, loss
    
    def _check_gpu_bf16_compatibility(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ£€æŸ¥GPUçš„bf16å…¼å®¹æ€§å¹¶è‡ªåŠ¨è°ƒæ•´é…ç½®

        Args:
            config: è®­ç»ƒé…ç½®å­—å…¸

        Returns:
            Dict[str, Any]: è°ƒæ•´åçš„é…ç½®
        """
        config_copy = config.copy()

        if config_copy.get('bf16', False):
            logger.info("æ£€æµ‹åˆ°bf16=Trueï¼Œå¼€å§‹GPUå…¼å®¹æ€§æ£€æŸ¥")

            try:
                import torch
                if torch.cuda.is_available():
                    # æ£€æŸ¥GPUæ˜¯å¦æ”¯æŒbf16
                    device = torch.cuda.current_device()
                    gpu_capability = torch.cuda.get_device_capability(device)

                    # Ampereæ¶æ„ï¼ˆè®¡ç®—èƒ½åŠ›8.0+ï¼‰æ‰å®Œå…¨æ”¯æŒbf16
                    supports_bf16 = gpu_capability[0] >= 8

                    if not supports_bf16:
                        logger.warning(f"GPUè®¡ç®—èƒ½åŠ› {gpu_capability[0]}.{gpu_capability[1]} ä¸æ”¯æŒbf16ï¼Œè‡ªåŠ¨é™çº§ä¸ºfp16")
                        config_copy['bf16'] = False
                        config_copy['fp16'] = True
                    else:
                        logger.info(f"GPUè®¡ç®—èƒ½åŠ› {gpu_capability[0]}.{gpu_capability[1]} æ”¯æŒbf16")
                else:
                    logger.warning("CUDAä¸å¯ç”¨ï¼Œbf16å·²ç¦ç”¨")
                    config_copy['bf16'] = False
                    config_copy['fp16'] = False

            except Exception as gpu_check_error:
                logger.warning(f"GPUå…¼å®¹æ€§æ£€æŸ¥å¤±è´¥ï¼Œç¦ç”¨bf16: {gpu_check_error}")
                config_copy['bf16'] = False
                config_copy['fp16'] = True

        return config_copy

    def _create_training_args(self) -> Any:
        """Create training arguments."""
        config = self.config_builder.build_training_config(self.train_type)

        # è°ƒè¯•ï¼šè®°å½•å…³é”®å‚æ•°
        logger.info(f"[DEBUG] è®­ç»ƒé…ç½®å…³é”®å‚æ•°: metric_for_best_model={config.get('metric_for_best_model')}, "
                   f"load_best_model_at_end={config.get('load_best_model_at_end')}, "
                   f"greater_is_better={config.get('greater_is_better')}")

        # å…¼å®¹æ€§ä¿®å¤ï¼šå¦‚æœmetric_for_best_modelè®¾ç½®ä¸ºä¸å­˜åœ¨çš„eval_lossï¼Œè‡ªåŠ¨ä¿®æ­£
        if config.get('metric_for_best_model') == 'eval_loss':
            logger.warning("æ£€æµ‹åˆ°è¿‡æ—¶çš„metric_for_best_model='eval_loss'ï¼Œè‡ªåŠ¨ä¿®æ­£ä¸º'eval_sequential_score'")
            config['metric_for_best_model'] = 'eval_sequential_score'
            config['greater_is_better'] = True

        # ğŸ”§ GPUå…¼å®¹æ€§æ£€æµ‹ï¼šæ£€æŸ¥bf16æ”¯æŒ
        config = self._check_gpu_bf16_compatibility(config)

        return self.create_training_args(config)
    
    def _create_evaluators_from_datasets(self, datasets) -> Dict[str, Any]:
        """Create evaluators from datasets (matching original train.py logic)."""
        # Get target column using the same logic as original train.py
        target_column = self._get_target_column(datasets)

        # Get run name (model name for evaluator naming)
        model_name = self.model_config.get('model_name_or_path', 'unknown')
        short_model_name = model_name.split("/")[-1] if "/" in model_name else model_name
        run_name = f"{self.train_type}-{short_model_name}"

        # è·å–æ•°æ®æºæ˜ å°„
        data_source_mapping = self._get_data_source_mapping(datasets['train'])
        # ä¿å­˜åˆ°å®ä¾‹å˜é‡ï¼Œä¾›åç»­åˆ†ç¦»é€»è¾‘ä½¿ç”¨
        self._current_data_source_mapping = data_source_mapping
        logger.info(f"[DEBUG] è®¾ç½®_current_data_source_mapping: {data_source_mapping}")

        evaluators = {}

        # åˆ›å»ºéªŒè¯è¯„ä¼°å™¨ï¼ˆä½¿ç”¨data_source_mappingç¡®ä¿source_idå‘½åï¼‰
        if datasets.get('eval') is not None:
            if self.data_loader.is_multi_dataset(datasets['eval']):
                # å¤šæ•°æ®é›†ï¼šä½¿ç”¨create_multi_evaluatorå¹¶ä¼ é€’data_source_mapping
                evaluators['dev'] = self.evaluator_factory.create_multi_evaluator(
                    datasets['eval'], target_column, f"{run_name}-eval", data_source_mapping
                )
                logger.info(f"åˆ›å»ºå¤šæ•°æ®é›†éªŒè¯è¯„ä¼°å™¨æˆåŠŸï¼Œä½¿ç”¨source_idå‘½å: {run_name}-eval")
            else:
                # å•æ•°æ®é›†ï¼šä½¿ç”¨source_idä½œä¸ºè¯„ä¼°å™¨åç§°ç¡®ä¿å‘½åä¸€è‡´
                dataset_name = list(datasets['eval'].keys())[0] if isinstance(datasets['eval'], dict) else 'default'
                evaluator_name = data_source_mapping.get(dataset_name, "1") if data_source_mapping else "1"
                evaluators['dev'] = self.evaluator_factory.create_evaluator(
                    datasets['eval'], target_column, evaluator_name
                )
                logger.info(f"åˆ›å»ºå•æ•°æ®é›†éªŒè¯è¯„ä¼°å™¨æˆåŠŸï¼Œä½¿ç”¨source_idå‘½å: {evaluator_name}")

        # åˆ›å»ºæµ‹è¯•è¯„ä¼°å™¨ï¼ˆä½¿ç”¨data_source_mappingç¡®ä¿source_idå‘½åï¼‰
        if datasets.get('test') is not None:
            if self.data_loader.is_multi_dataset(datasets['test']):
                # å¤šæ•°æ®é›†ï¼šä½¿ç”¨create_multi_evaluatorå¹¶ä¼ é€’data_source_mapping
                evaluators['test'] = self.evaluator_factory.create_multi_evaluator(
                    datasets['test'], target_column, f"{run_name}-test", data_source_mapping
                )
                logger.info(f"åˆ›å»ºå¤šæ•°æ®é›†æµ‹è¯•è¯„ä¼°å™¨æˆåŠŸï¼Œä½¿ç”¨source_idå‘½å: {run_name}-test")
            else:
                # å•æ•°æ®é›†ï¼šä½¿ç”¨source_idä½œä¸ºè¯„ä¼°å™¨åç§°ç¡®ä¿å‘½åä¸€è‡´
                dataset_name = list(datasets['test'].keys())[0] if isinstance(datasets['test'], dict) else 'default'
                evaluator_name = data_source_mapping.get(dataset_name, "1") if data_source_mapping else "1"
                evaluators['test'] = self.evaluator_factory.create_evaluator(
                    datasets['test'], target_column, evaluator_name
                )
                logger.info(f"åˆ›å»ºå•æ•°æ®é›†æµ‹è¯•è¯„ä¼°å™¨æˆåŠŸï¼Œä½¿ç”¨source_idå‘½å: {evaluator_name}")

        return evaluators

    def _create_evaluator(self, datasets) -> Optional[Any]:
        """Create evaluator if evaluation dataset exists."""
        if datasets.get('eval') is None:
            logger.info("æ²¡æœ‰è¯„ä¼°æ•°æ®é›†ï¼Œè·³è¿‡è¯„ä¼°å™¨åˆ›å»º")
            return None

        return self.create_evaluator(datasets['eval'])

    def _record_dataset_info(self, datasets: Dict[str, Any]):
        """Record dataset information to database (matching original train.py)."""
        task_id = self.raw_config.get('task_id')
        if not task_id:
            logger.warning("ç¼ºå°‘task_idï¼Œè·³è¿‡æ•°æ®é›†ä¿¡æ¯è®°å½•")
            return

        try:
            from bubble_rag.training.mysql_service.service.training_dataset_service import TrainingDatasetService

            # è·å–æ•°æ®é›†è·¯å¾„å’Œé…ç½®
            dataset_name_or_path = self.data_config.get('dataset_name_or_path', 'unknown')

            # è·å–ç›®æ ‡åˆ—åï¼ˆä½¿ç”¨ä¸train.pyç›¸åŒçš„é€»è¾‘ï¼‰
            target_column = self._get_target_column(datasets)

            # ç”Ÿæˆæ•°æ®æºæ˜ å°„ (å®Œå…¨åŒ¹é…åŸç‰ˆtrain.pyçš„é€»è¾‘)
            data_source_mapping = {}
            train_dataset = datasets.get('train')

            if self.data_loader.is_multi_dataset(train_dataset) and isinstance(train_dataset, dict):
                for idx, base_name in enumerate(train_dataset.keys()):
                    data_source_mapping[base_name] = self._generate_data_source_id(idx, base_name)
            else:
                # å•æ•°æ®é›†ï¼šä½¿ç”¨å›ºå®šçš„æ•°æ®æºIDå’ŒåŸºç¡€åç§°
                base_name = self.data_loader._extract_dataset_base_name(dataset_name_or_path)
                data_source_mapping[base_name] = "1"  # åŸç‰ˆtrain.pyçš„å›ºå®šå€¼

            # è®°å½•è®­ç»ƒæ•°æ®é›†
            if datasets.get('train'):
                self._record_split_dataset_info(
                    TrainingDatasetService, task_id, datasets['train'], 'train',
                    dataset_name_or_path, data_source_mapping, target_column
                )

            # è®°å½•éªŒè¯æ•°æ®é›†
            if datasets.get('eval'):
                self._record_split_dataset_info(
                    TrainingDatasetService, task_id, datasets['eval'], 'eval',
                    dataset_name_or_path, data_source_mapping, target_column
                )

            # è®°å½•æµ‹è¯•æ•°æ®é›†
            if datasets.get('test'):
                self._record_split_dataset_info(
                    TrainingDatasetService, task_id, datasets['test'], 'test',
                    dataset_name_or_path, data_source_mapping, target_column
                )

            logger.info("æ•°æ®é›†ä¿¡æ¯è®°å½•æˆåŠŸ")

        except Exception as e:
            logger.warning(f"è®°å½•æ•°æ®é›†ä¿¡æ¯å¤±è´¥ï¼ˆä¸å½±å“è®­ç»ƒï¼‰: {e}")

    def _record_split_dataset_info(self, TrainingDatasetService, task_id: str, dataset: Any,
                                   split_type: str, dataset_name_or_path: str,
                                   data_source_mapping: Dict[str, str], target_column: str):
        """Record dataset information for a specific split (matching original train.py)."""

        # è·å–æ ·æœ¬å¤§å°é…ç½®
        sample_size_key = f'{split_type}_sample_size'
        global_configured_sample_size = self.data_config.get(sample_size_key, -1)

        if self.data_loader.is_multi_dataset(dataset):
            # å¤šæ•°æ®é›†ï¼šæ¯ä¸ªæ•°æ®æºåˆ†é…ç‹¬ç«‹çš„ID
            for base_name, ds in dataset.items():
                data_source_id = data_source_mapping[base_name]  # ä½¿ç”¨ç»Ÿä¸€æ˜ å°„

                # æŒ‰ç…§åŸç‰ˆtrain.pyé€»è¾‘ï¼šç›´æ¥ä¿å­˜ç”¨æˆ·é…ç½®å€¼ï¼Œ0è¡¨ç¤ºæ— é™åˆ¶
                configured_sample_size = global_configured_sample_size

                TrainingDatasetService.record_dataset_info(
                    task_id=task_id,
                    data_source_id=data_source_id,
                    dataset_name=base_name,  # å»æ‰splitåç¼€ï¼Œåªä¿å­˜åŸºç¡€åç§°
                    dataset_base_name=base_name,
                    dataset_path=dataset_name_or_path,
                    dataset_type="auto",
                    split_type=split_type,
                    dataset=ds,
                    target_column="auto",  # è®©record_dataset_infoè‡ªåŠ¨è¯†åˆ«
                    loss_function=None,
                    evaluator=None,
                    hf_subset=self.data_config.get('HF_subset'),  # HF_subseté…ç½®
                    configured_sample_size=configured_sample_size  # ä¿®å¤ï¼šä½¿ç”¨å„æ•°æ®é›†çš„å®é™…å¤§å°
                )
        else:
            # å•æ•°æ®é›†ï¼šä½¿ç”¨å›ºå®šçš„æ•°æ®æºID
            base_name = self.data_loader._extract_dataset_base_name(dataset_name_or_path)
            data_source_id = self._generate_data_source_id(0, base_name)

            # æŒ‰ç…§åŸç‰ˆtrain.pyé€»è¾‘ï¼šç›´æ¥ä¿å­˜ç”¨æˆ·é…ç½®å€¼ï¼Œ0è¡¨ç¤ºæ— é™åˆ¶
            configured_sample_size = global_configured_sample_size

            TrainingDatasetService.record_dataset_info(
                task_id=task_id,
                data_source_id=data_source_id,
                dataset_name=base_name,  # å»æ‰splitåç¼€ï¼Œåªä¿å­˜åŸºç¡€åç§°
                dataset_base_name=base_name,
                dataset_path=dataset_name_or_path,
                dataset_type="auto",
                split_type=split_type,
                dataset=dataset,
                target_column="auto",  # è®©record_dataset_infoè‡ªåŠ¨è¯†åˆ«
                loss_function=None,
                evaluator=None,
                hf_subset=self.data_config.get('HF_subset'),  # HF_subseté…ç½®
                configured_sample_size=configured_sample_size  # ä¿®å¤ï¼šä½¿ç”¨å®é™…å¤§å°
            )

    def _generate_data_source_id(self, index: int, base_name: str) -> str:
        """Generate data source ID (matching original train.py logic)."""
        return str(index + 1)  # Original train.py uses simple "1", "2", "3"

    def _get_target_column(self, datasets: Dict[str, Any]) -> str:
        """Get target column for this training type (matching original train.py logic)."""
        # Use cached value if available
        if self._cached_target_column is not None:
            return self._cached_target_column

        train_dataset = datasets['train']

        # Use data_loader to get the initial target column
        target_column = self.data_loader.get_target_column(train_dataset)

        # Standardize based on data type (matching train.py logic)
        sample_value = None
        try:
            if self.data_loader.is_multi_dataset(train_dataset):
                # For multi-dataset, check first dataset
                first_dataset_name = next(iter(train_dataset.keys()))
                first_dataset = train_dataset[first_dataset_name]
                if target_column in first_dataset.column_names and len(first_dataset) > 0:
                    # Access underlying arrow table to avoid formatting issues
                    sample_value = first_dataset.data.column(target_column).to_pylist()[0] if hasattr(first_dataset, 'data') and first_dataset.data else None
            else:
                # For single dataset
                if target_column in train_dataset.column_names and len(train_dataset) > 0:
                    # Access underlying arrow table to avoid formatting issues
                    sample_value = train_dataset.data.column(target_column).to_pylist()[0] if hasattr(train_dataset, 'data') and train_dataset.data else None
        except Exception as e:
            logger.warning(f"è·å–æ ·æœ¬å€¼å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ—å: {e}")
            # Fall back to reranker/embedding type-based default
            if self.train_type == "embedding":
                target_column = "score"
            else:  # reranker
                target_column = "label"
            sample_value = None

        if sample_value is not None:
            if isinstance(sample_value, int) or (isinstance(sample_value, float) and sample_value.is_integer()):
                target_column = "label"
            else:
                target_column = "score"

        # Cache the result
        self._cached_target_column = target_column
        logger.info(f"ç›®æ ‡åˆ—åå·²æ›´æ–°ä¸º: {target_column}")
        return target_column

    def _update_loss_function_info(self, datasets: Dict[str, Any], loss):
        """Update loss function information in database (matching original train.py)."""
        task_id = self.raw_config.get('task_id')
        if not task_id:
            logger.warning("ç¼ºå°‘task_idï¼Œè·³è¿‡æŸå¤±å‡½æ•°ä¿¡æ¯æ›´æ–°")
            return

        try:
            from bubble_rag.training.mysql_service.service.training_dataset_service import TrainingDatasetService

            # è·å–æ•°æ®æºæ˜ å°„ï¼ˆå¤ç”¨ _record_dataset_info ä¸­çš„é€»è¾‘ï¼‰
            data_source_mapping = self._get_data_source_mapping(datasets['train'])

            # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ•°æ®é›†åœºæ™¯
            train_dataset = datasets['train']
            is_multi_dataset = self.data_loader.is_multi_dataset(train_dataset)

            if is_multi_dataset and isinstance(loss, dict):
                # å¤šæ•°æ®é›†æƒ…å†µï¼šä¸ºæ¯ä¸ªæ•°æ®æºæ›´æ–°å¯¹åº”çš„æŸå¤±å‡½æ•°
                for dataset_name, loss_func in loss.items():
                    actual_loss_name = type(loss_func).__name__
                    data_source_id = data_source_mapping.get(dataset_name, f"ds_{hash(dataset_name) % 1000:03d}")
                    try:
                        TrainingDatasetService.update_dataset_loss_function_by_source(
                            task_id=task_id,
                            data_source_id=data_source_id,
                            split_type="train",
                            loss_function=actual_loss_name
                        )
                        logger.info(f"æ›´æ–°å¤šæ•°æ®é›†æŸå¤±å‡½æ•°: {data_source_id}-train -> {actual_loss_name}")
                    except Exception as e:
                        logger.warning(f"æ›´æ–°æ•°æ®æº {data_source_id} è®­ç»ƒé›†æŸå¤±å‡½æ•°å¤±è´¥: {e}")

                # å¤šæ•°æ®é›†æƒ…å†µï¼šéªŒè¯é›†å’Œæµ‹è¯•é›†ä½¿ç”¨å¯¹åº”åŒåè®­ç»ƒæ•°æ®é›†çš„æŸå¤±å‡½æ•°
                if datasets.get('eval') and isinstance(datasets['eval'], dict):
                    for dataset_name in datasets['eval'].keys():
                        data_source_id = data_source_mapping.get(dataset_name, f"ds_{hash(dataset_name) % 1000:03d}")
                        if dataset_name in loss:
                            corresponding_loss_name = type(loss[dataset_name]).__name__
                            try:
                                TrainingDatasetService.update_dataset_loss_function_by_source(
                                    task_id=task_id,
                                    data_source_id=data_source_id,
                                    split_type="eval",
                                    loss_function=corresponding_loss_name
                                )
                                logger.info(f"æ›´æ–°å¤šéªŒè¯æ•°æ®é›†æŸå¤±å‡½æ•°: {data_source_id}-eval -> {corresponding_loss_name}")
                            except Exception as e:
                                logger.warning(f"æ›´æ–°éªŒè¯æ•°æ®æº {data_source_id} æŸå¤±å‡½æ•°å¤±è´¥: {e}")

                if datasets.get('test') and isinstance(datasets['test'], dict):
                    for dataset_name in datasets['test'].keys():
                        data_source_id = data_source_mapping.get(dataset_name, f"ds_{hash(dataset_name) % 1000:03d}")
                        if dataset_name in loss:
                            corresponding_loss_name = type(loss[dataset_name]).__name__
                            try:
                                TrainingDatasetService.update_dataset_loss_function_by_source(
                                    task_id=task_id,
                                    data_source_id=data_source_id,
                                    split_type="test",
                                    loss_function=corresponding_loss_name
                                )
                                logger.info(f"æ›´æ–°å¤šæµ‹è¯•æ•°æ®é›†æŸå¤±å‡½æ•°: {data_source_id}-test -> {corresponding_loss_name}")
                            except Exception as e:
                                logger.warning(f"æ›´æ–°æµ‹è¯•æ•°æ®æº {data_source_id} æŸå¤±å‡½æ•°å¤±è´¥: {e}")

            else:
                # å•æ•°æ®é›†æƒ…å†µï¼šç›´æ¥è·å–æŸå¤±å‡½æ•°ç±»å
                actual_loss_name = type(loss).__name__
                logger.info(f"æ›´æ–°å•æ•°æ®é›†æŸå¤±å‡½æ•°ä¿¡æ¯: {actual_loss_name}")

                # å•æ•°æ®é›†ä½¿ç”¨å›ºå®šçš„æ•°æ®æºID
                data_source_id = "1"

                # æ›´æ–°è®­ç»ƒæ•°æ®é›†çš„æŸå¤±å‡½æ•°
                if datasets.get('train'):
                    try:
                        TrainingDatasetService.update_dataset_loss_function_by_source(
                            task_id=task_id,
                            data_source_id=data_source_id,
                            split_type="train",
                            loss_function=actual_loss_name
                        )
                        logger.info(f"æ›´æ–°å•è®­ç»ƒæ•°æ®é›†æŸå¤±å‡½æ•°: {data_source_id}-train -> {actual_loss_name}")
                    except Exception as e:
                        logger.warning(f"æ›´æ–°è®­ç»ƒæ•°æ®é›†æŸå¤±å‡½æ•°å¤±è´¥: {e}")

                # å•æ•°æ®é›†æƒ…å†µï¼šéªŒè¯é›†å’Œæµ‹è¯•é›†ä¹Ÿä½¿ç”¨ç›¸åŒçš„æŸå¤±å‡½æ•°
                if datasets.get('eval'):
                    try:
                        TrainingDatasetService.update_dataset_loss_function_by_source(
                            task_id=task_id,
                            data_source_id=data_source_id,
                            split_type="eval",
                            loss_function=actual_loss_name
                        )
                        logger.info(f"æ›´æ–°å•éªŒè¯æ•°æ®é›†æŸå¤±å‡½æ•°: {data_source_id}-eval -> {actual_loss_name}")
                    except Exception as e:
                        logger.warning(f"æ›´æ–°éªŒè¯æ•°æ®é›†æŸå¤±å‡½æ•°å¤±è´¥: {e}")

                if datasets.get('test'):
                    try:
                        TrainingDatasetService.update_dataset_loss_function_by_source(
                            task_id=task_id,
                            data_source_id=data_source_id,
                            split_type="test",
                            loss_function=actual_loss_name
                        )
                        logger.info(f"æ›´æ–°å•æµ‹è¯•æ•°æ®é›†æŸå¤±å‡½æ•°: {data_source_id}-test -> {actual_loss_name}")
                    except Exception as e:
                        logger.warning(f"æ›´æ–°æµ‹è¯•æ•°æ®é›†æŸå¤±å‡½æ•°å¤±è´¥: {e}")

        except Exception as e:
            logger.warning(f"æ›´æ–°æ•°æ®é›†æŸå¤±å‡½æ•°ä¿¡æ¯å¤±è´¥: {e}")

    def _get_data_source_mapping(self, train_dataset) -> Dict[str, str]:
        """Get data source mapping (extracted from _record_dataset_info)."""
        data_source_mapping = {}
        dataset_name_or_path = self.data_config.get('dataset_name_or_path', 'unknown')

        if self.data_loader.is_multi_dataset(train_dataset) and isinstance(train_dataset, dict):
            for idx, base_name in enumerate(train_dataset.keys()):
                data_source_mapping[base_name] = self._generate_data_source_id(idx, base_name)
        else:
            # å•æ•°æ®é›†ï¼šä½¿ç”¨å›ºå®šçš„æ•°æ®æºIDå’ŒåŸºç¡€åç§°
            base_name = self.data_loader._extract_dataset_base_name(dataset_name_or_path)
            data_source_mapping[base_name] = self._generate_data_source_id(0, base_name)  # ä¿æŒä¸_record_dataset_infoä¸€è‡´

        return data_source_mapping

    def _update_model_info(self, model):
        """Update model information in database (matching original train.py)."""
        task_id = self.raw_config.get('task_id')
        if not task_id:
            logger.warning("ç¼ºå°‘task_idï¼Œè·³è¿‡æ¨¡å‹ä¿¡æ¯æ›´æ–°")
            return

        try:
            from bubble_rag.training.model_sft.services.task_manager import task_manager

            model_name = self.model_config.get('model_name_or_path', 'unknown')

            # æ„å»ºæ¨¡å‹ä¿¡æ¯ï¼ˆå®Œå…¨åŒ¹é…åŸç‰ˆtrain.pyï¼‰
            model_info_update = {
                "validation": {
                    "valid": True,
                    "message": "æ¨¡å‹åŠ è½½æˆåŠŸ",
                    "details": {
                        "type": "validated",
                        "name": model_name
                    }
                },
                "recommended_training_types": [self.train_type],
                "compatibility": {
                    "supported": True,
                    "model_type": "loaded",
                    "notes": ["æ¨¡å‹å·²æˆåŠŸåŠ è½½"]
                }
            }

            # è·å–æ¨¡å‹ç»´åº¦ï¼ˆæ”¯æŒembeddingå’Œrerankeræ¨¡å‹ï¼‰
            embedding_dimension = None
            if self.train_type == "embedding" and hasattr(model, 'get_sentence_embedding_dimension'):
                try:
                    embedding_dimension = model.get_sentence_embedding_dimension()
                    logger.info(f"è·å–åˆ°embeddingæ¨¡å‹ç»´åº¦: {embedding_dimension}")
                except Exception as dim_e:
                    logger.warning(f"è·å–embeddingæ¨¡å‹ç»´åº¦å¤±è´¥: {str(dim_e)}")
            elif self.train_type == "reranker":
                # å¯¹äºrerankeræ¨¡å‹ï¼Œå°è¯•å¤šç§æ–¹æ³•è·å–ç»´åº¦
                try:
                    # æ–¹æ³•1: é€šè¿‡æ¨¡å‹çš„tokenizerå’Œconfigè·å–hidden_size
                    if hasattr(model, 'model') and hasattr(model.model, 'config') and hasattr(model.model.config, 'hidden_size'):
                        embedding_dimension = model.model.config.hidden_size
                        logger.info(f"è·å–åˆ°rerankeræ¨¡å‹ç»´åº¦ (æ–¹æ³•1): {embedding_dimension}")
                    # æ–¹æ³•2: é€šè¿‡encodeæ–¹æ³•æµ‹è¯•è·å–ç»´åº¦
                    elif hasattr(model, 'encode'):
                        test_texts = ["test"]
                        try:
                            # æŸäº›rerankeræ¨¡å‹çš„encodeæ–¹æ³•è¿”å›embedding
                            test_embedding = model.encode(test_texts)
                            if hasattr(test_embedding, 'shape') and len(test_embedding.shape) > 1:
                                embedding_dimension = test_embedding.shape[1]
                                logger.info(f"è·å–åˆ°rerankeræ¨¡å‹ç»´åº¦ (æ–¹æ³•2): {embedding_dimension}")
                        except:
                            pass
                    # æ–¹æ³•3: æ£€æŸ¥æ˜¯å¦æœ‰classifierå±‚æ¥æ¨æ–­ç»´åº¦
                    elif hasattr(model, 'classifier') and hasattr(model.classifier, 'in_features'):
                        embedding_dimension = model.classifier.in_features
                        logger.info(f"è·å–åˆ°rerankeræ¨¡å‹ç»´åº¦ (æ–¹æ³•3): {embedding_dimension}")

                    if not embedding_dimension:
                        logger.info("æ— æ³•è‡ªåŠ¨è·å–rerankeræ¨¡å‹ç»´åº¦ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼æˆ–è·³è¿‡")

                except Exception as dim_e:
                    logger.warning(f"è·å–rerankeræ¨¡å‹ç»´åº¦å¤±è´¥: {str(dim_e)}")

            # å¦‚æœè·å–åˆ°äº†ç»´åº¦ï¼Œæ·»åŠ åˆ°æ¨¡å‹ä¿¡æ¯ä¸­
            if embedding_dimension:
                model_info_update["embedding_dimension"] = embedding_dimension

            # æ›´æ–°åˆ°æ•°æ®åº“
            task_manager.update_model_info_after_loading(task_id, model_info_update)
            logger.info("æ¨¡å‹ä¿¡æ¯æ›´æ–°æˆåŠŸ")

        except Exception as update_e:
            logger.warning(f"æ›´æ–°æ¨¡å‹ä¿¡æ¯åˆ°æ•°æ®åº“å¤±è´¥ï¼Œä¸å½±å“è®­ç»ƒç»§ç»­: {str(update_e)}")

    def _update_evaluator_info(self, datasets: Dict[str, Any], evaluators: Dict[str, Any]):
        """Update evaluator information in database (matching original train.py)."""
        task_id = self.raw_config.get('task_id')
        if not task_id:
            logger.warning("ç¼ºå°‘task_idï¼Œè·³è¿‡è¯„ä¼°å™¨ä¿¡æ¯æ›´æ–°")
            return

        try:
            from bubble_rag.training.mysql_service.service.training_dataset_service import TrainingDatasetService

            # è·å–æ•°æ®æºæ˜ å°„
            data_source_mapping = self._get_data_source_mapping(datasets['train'])

            # æ›´æ–°éªŒè¯æ•°æ®é›†çš„è¯„ä¼°å™¨ç±»å‹
            if datasets.get('eval') and evaluators.get('dev'):
                eval_dataset = datasets['eval']
                evaluator = evaluators['dev']

                # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ•°æ®é›†
                if self.data_loader.is_multi_dataset(eval_dataset):
                    # å¤šæ•°æ®é›†æƒ…å†µï¼šä»SequentialEvaluatorä¸­æå–å­è¯„ä¼°å™¨
                    if isinstance(eval_dataset, dict) and hasattr(evaluator, 'evaluators'):
                        # SequentialEvaluatoråŒ…å«å¤šä¸ªå­è¯„ä¼°å™¨
                        dataset_names = list(eval_dataset.keys())
                        sub_evaluators = evaluator.evaluators

                        for dataset_name, sub_evaluator in zip(dataset_names, sub_evaluators):
                            try:
                                if dataset_name in data_source_mapping:
                                    data_source_id = data_source_mapping[dataset_name]
                                    sub_evaluator_name = type(sub_evaluator).__name__

                                    TrainingDatasetService.update_dataset_evaluator_by_source(
                                        task_id=task_id,
                                        data_source_id=data_source_id,
                                        split_type="eval",
                                        evaluator=sub_evaluator_name
                                    )
                                    logger.info(f"æ›´æ–°å¤šæ•°æ®é›†è¯„ä¼°å™¨ç±»å‹: {dataset_name} (æºID: {data_source_id}) -> {sub_evaluator_name}")
                                else:
                                    logger.warning(f"æ‰¾ä¸åˆ°æ•°æ®é›† {dataset_name} çš„æ•°æ®æºæ˜ å°„")
                            except Exception as e:
                                logger.warning(f"æ›´æ–°æ•°æ®é›† {dataset_name} è¯„ä¼°å™¨ç±»å‹å¤±è´¥: {e}")
                    else:
                        # ä¸æ˜¯SequentialEvaluatorï¼Œæ‰€æœ‰æ•°æ®é›†ä½¿ç”¨åŒä¸€ä¸ªè¯„ä¼°å™¨
                        evaluator_name = type(evaluator).__name__
                        for dataset_name in eval_dataset.keys():
                            try:
                                if dataset_name in data_source_mapping:
                                    data_source_id = data_source_mapping[dataset_name]
                                    TrainingDatasetService.update_dataset_evaluator_by_source(
                                        task_id=task_id,
                                        data_source_id=data_source_id,
                                        split_type="eval",
                                        evaluator=evaluator_name
                                    )
                                    logger.info(f"æ›´æ–°å¤šæ•°æ®é›†è¯„ä¼°å™¨ç±»å‹: {dataset_name} (æºID: {data_source_id}) -> {evaluator_name}")
                            except Exception as e:
                                logger.warning(f"æ›´æ–°æ•°æ®é›† {dataset_name} è¯„ä¼°å™¨ç±»å‹å¤±è´¥: {e}")
                else:
                    # å•æ•°æ®é›†æƒ…å†µï¼šä¸ºè¯¥æ•°æ®æºæ›´æ–°è¯„ä¼°å™¨ç±»å‹
                    evaluator_name = type(evaluator).__name__
                    try:
                        # å•æ•°æ®é›†ä¹Ÿè¦é€šè¿‡data_source_idæ›´æ–°
                        for dataset_name, data_source_id in data_source_mapping.items():
                            TrainingDatasetService.update_dataset_evaluator_by_source(
                                task_id=task_id,
                                data_source_id=data_source_id,
                                split_type="eval",
                                evaluator=evaluator_name
                            )
                            logger.info(f"æ›´æ–°å•éªŒè¯æ•°æ®é›†è¯„ä¼°å™¨ç±»å‹: {dataset_name} (æºID: {data_source_id}) -> {evaluator_name}")
                    except Exception as e:
                        logger.warning(f"æ›´æ–°å•éªŒè¯æ•°æ®é›†è¯„ä¼°å™¨ç±»å‹å¤±è´¥: {e}")

            # æ›´æ–°æµ‹è¯•æ•°æ®é›†çš„è¯„ä¼°å™¨ç±»å‹
            if datasets.get('test') and evaluators.get('test'):
                test_dataset = datasets['test']
                evaluator = evaluators['test']

                # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ•°æ®é›†
                if self.data_loader.is_multi_dataset(test_dataset):
                    # å¤šæ•°æ®é›†æƒ…å†µï¼šä»SequentialEvaluatorä¸­æå–å­è¯„ä¼°å™¨
                    if isinstance(test_dataset, dict) and hasattr(evaluator, 'evaluators'):
                        # SequentialEvaluatoråŒ…å«å¤šä¸ªå­è¯„ä¼°å™¨
                        dataset_names = list(test_dataset.keys())
                        sub_evaluators = evaluator.evaluators

                        for dataset_name, sub_evaluator in zip(dataset_names, sub_evaluators):
                            try:
                                if dataset_name in data_source_mapping:
                                    data_source_id = data_source_mapping[dataset_name]
                                    sub_evaluator_name = type(sub_evaluator).__name__

                                    TrainingDatasetService.update_dataset_evaluator_by_source(
                                        task_id=task_id,
                                        data_source_id=data_source_id,
                                        split_type="test",
                                        evaluator=sub_evaluator_name
                                    )
                                    logger.info(f"æ›´æ–°å¤šæµ‹è¯•æ•°æ®é›†è¯„ä¼°å™¨ç±»å‹: {dataset_name} (æºID: {data_source_id}) -> {sub_evaluator_name}")
                                else:
                                    logger.warning(f"æ‰¾ä¸åˆ°æ•°æ®é›† {dataset_name} çš„æ•°æ®æºæ˜ å°„")
                            except Exception as e:
                                logger.warning(f"æ›´æ–°æ•°æ®é›† {dataset_name} è¯„ä¼°å™¨ç±»å‹å¤±è´¥: {e}")
                    else:
                        # ä¸æ˜¯SequentialEvaluatorï¼Œæ‰€æœ‰æ•°æ®é›†ä½¿ç”¨åŒä¸€ä¸ªè¯„ä¼°å™¨
                        evaluator_name = type(evaluator).__name__
                        for dataset_name in test_dataset.keys():
                            try:
                                if dataset_name in data_source_mapping:
                                    data_source_id = data_source_mapping[dataset_name]
                                    TrainingDatasetService.update_dataset_evaluator_by_source(
                                        task_id=task_id,
                                        data_source_id=data_source_id,
                                        split_type="test",
                                        evaluator=evaluator_name
                                    )
                                    logger.info(f"æ›´æ–°å¤šæµ‹è¯•æ•°æ®é›†è¯„ä¼°å™¨ç±»å‹: {dataset_name} (æºID: {data_source_id}) -> {evaluator_name}")
                            except Exception as e:
                                logger.warning(f"æ›´æ–°æ•°æ®é›† {dataset_name} è¯„ä¼°å™¨ç±»å‹å¤±è´¥: {e}")
                else:
                    # å•æ•°æ®é›†æƒ…å†µï¼šä¸ºè¯¥æ•°æ®æºæ›´æ–°è¯„ä¼°å™¨ç±»å‹
                    evaluator_name = type(evaluator).__name__
                    try:
                        # å•æ•°æ®é›†ä¹Ÿè¦é€šè¿‡data_source_idæ›´æ–°
                        for dataset_name, data_source_id in data_source_mapping.items():
                            TrainingDatasetService.update_dataset_evaluator_by_source(
                                task_id=task_id,
                                data_source_id=data_source_id,
                                split_type="test",
                                evaluator=evaluator_name
                            )
                            logger.info(f"æ›´æ–°å•æµ‹è¯•æ•°æ®é›†è¯„ä¼°å™¨ç±»å‹: {dataset_name} (æºID: {data_source_id}) -> {evaluator_name}")
                    except Exception as e:
                        logger.warning(f"æ›´æ–°å•æµ‹è¯•æ•°æ®é›†è¯„ä¼°å™¨ç±»å‹å¤±è´¥: {e}")

        except Exception as e:
            logger.warning(f"æ›´æ–°è¯„ä¼°å™¨ä¿¡æ¯å¤±è´¥: {e}")

    def _execute_training(self, trainer, model, progress_callback) -> TrainingResult:
        """Execute the actual training."""
        # æ€»æ˜¯è®¾ç½®åŸºç¡€çš„task_managerè¿›åº¦è·Ÿè¸ªï¼ˆåŒ¹é…åŸç‰ˆtrain.pyï¼‰
        task_id = self.raw_config.get('task_id')
        if task_id:
            try:
                from bubble_rag.training.model_sft.services.task_manager import task_manager
                task_manager.update_task_progress(task_id, 0.0, "å¼€å§‹è®­ç»ƒ")
            except Exception as tm_e:
                logger.warning(f"åˆå§‹åŒ–task_managerè¿›åº¦å¤±è´¥: {tm_e}")

        # Setup progress callback if provided
        if progress_callback:
            # ä¿®å¤ï¼šæœ‰å¤–å±‚è¿›åº¦å›è°ƒæ—¶ï¼Œåªè®¾ç½®å¤–å±‚å›è°ƒï¼Œé¿å…åŒé‡æ›´æ–°
            self._setup_progress_callback(trainer, progress_callback)
            # ä½†ä»éœ€è¦è®¾ç½®lossæ•°æ®æ”¶é›†
            self._setup_loss_collection(trainer)
        else:
            # å³ä½¿æ²¡æœ‰è¿›åº¦å›è°ƒï¼Œä¹Ÿè¦è®¾ç½®lossæ•°æ®æ”¶é›†å’ŒåŸºç¡€è¿›åº¦è·Ÿè¸ª (matching original train.py)
            self._setup_loss_collection(trainer)
            # è®¾ç½®åŸºç¡€çš„progressè·Ÿè¸ªï¼ˆä»…åœ¨æ— å¤–å±‚å›è°ƒæ—¶ä½¿ç”¨ï¼‰
            self._setup_basic_progress_tracking(trainer)

        # ä¿å­˜è®­ç»ƒå…ƒæ•°æ®åˆ°lossæ–‡ä»¶ï¼ˆåŒ…å«æ•°æ®æºæ˜ å°„ç­‰ä¿¡æ¯ï¼‰
        self._save_training_metadata(trainer)

        # ç¡®ä¿è¯„ä¼°å™¨ç›®å½•å­˜åœ¨ï¼ˆåŒ¹é…åŸç‰ˆtrain.pyé€»è¾‘ï¼‰
        self._ensure_eval_directories(trainer)

        # Run training
        trainer.train()

        # ä¿®å¤ï¼šè®­ç»ƒå®Œæˆæ—¶ä¸ä½¿ç”¨ç¡¬ç¼–ç çš„(1,1)ï¼Œè€Œæ˜¯ä½¿ç”¨æ­£ç¡®çš„æ­¥æ•°
        # è®­ç»ƒçœŸæ­£å®Œæˆæ—¶ï¼Œè¿›åº¦åº”è¯¥æ¥è¿‘100%ï¼Œä¸éœ€è¦é¢å¤–çš„è¿›åº¦å›è°ƒ
        # æœ€ç»ˆçš„100%è¿›åº¦ç”±ä»»åŠ¡å®ŒæˆçŠ¶æ€æ›´æ–°æ—¶è®¾ç½®
        logger.info("è®­ç»ƒæ‰§è¡Œå®Œæˆï¼Œè·³è¿‡ç¡¬ç¼–ç çš„è®­ç»ƒå®Œæˆè¿›åº¦å›è°ƒ")

        # æ€»æ˜¯æ›´æ–°task_managerä¸­çš„è¿›åº¦åˆ°99%ï¼ˆåŒ¹é…åŸç‰ˆtrain.pyï¼Œ100%ç”±æˆåŠŸçŠ¶æ€æ›´æ–°å¤„ç†ï¼‰
        task_id = self.raw_config.get('task_id')
        if task_id:
            try:
                from bubble_rag.training.model_sft.services.task_manager import task_manager
                task_manager.update_task_progress(task_id, 99.0, "ä¿å­˜æ¨¡å‹")
                logger.info("è®­ç»ƒå®Œæˆï¼Œè¿›åº¦æ›´æ–°åˆ°99%")
            except Exception as tm_e:
                logger.warning(f"æ›´æ–°å®Œæˆæ—¶task_managerè¿›åº¦å¤±è´¥: {tm_e}")

        # Save model
        save_dir = trainer.args.output_dir
        try:
            logger.info(f"å¼€å§‹ä¿å­˜æ¨¡å‹åˆ°: {save_dir}")
            trainer.save_model(save_dir)

            # éªŒè¯ä¿å­˜æ˜¯å¦æˆåŠŸ
            if os.path.exists(save_dir) and os.listdir(save_dir):
                logger.info(f"æ¨¡å‹å·²æˆåŠŸä¿å­˜åˆ°: {save_dir}")

                # åˆ—å‡ºä¿å­˜çš„æ–‡ä»¶
                saved_files = os.listdir(save_dir)
                logger.info(f"ä¿å­˜çš„æ–‡ä»¶: {saved_files}")
            else:
                logger.error(f"æ¨¡å‹ä¿å­˜éªŒè¯å¤±è´¥: ç›®å½•ä¸å­˜åœ¨æˆ–ä¸ºç©º - {save_dir}")

        except Exception as save_e:
            logger.error(f"æ¨¡å‹ä¿å­˜å¤±è´¥: {save_e}")
            # ä¸è¦æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­æ‰§è¡Œå…¶ä»–æ¸…ç†å·¥ä½œ
            import traceback
            logger.error(f"ä¿å­˜é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

        return TrainingResult(
            model=model,
            save_dir=save_dir
        )

    def _perform_baseline_evaluation(self, model, datasets, evaluators):
        """Perform baseline evaluation before training (matching original train.py)."""
        logger.info("å¼€å§‹è®­ç»ƒå‰åŸºçº¿è¯„ä¼°")

        # Validation set baseline evaluation (matching train.py lines 1455-1489)
        dev_evaluator = None
        if datasets.get('eval') is not None:
            # Check if multi-dataset (same logic as original)
            if self.data_loader.is_multi_dataset(datasets['eval']):
                # Multi-dataset: create SequentialEvaluator
                target_column = self._get_target_column(datasets)
                model_name = self.model_config.get('model_name_or_path', 'unknown')
                short_model_name = model_name.split("/")[-1] if "/" in model_name else model_name
                run_name = f"{self.train_type}-{short_model_name}"

                # è·å–æ•°æ®æºæ˜ å°„
                data_source_mapping = self._get_data_source_mapping(datasets['train'])
                dev_evaluator = self.evaluator_factory.create_multi_evaluator(
                    datasets['eval'], target_column, run_name, data_source_mapping
                )
            elif 'dev' in evaluators:
                # Single dataset: use single evaluator
                dev_evaluator = evaluators['dev']

            # Evaluate baseline model
            if dev_evaluator is not None:
                try:
                    dev_results = self.evaluator_factory.evaluate_model(model, dev_evaluator)
                    logger.info(f"éªŒè¯é›†åŸºçº¿è¯„ä¼°ç»“æœ: {dev_results}")

                    task_id = self.raw_config.get('task_id')
                    if task_id:
                        # æ£€æµ‹æ˜¯å¦ä¸ºSequentialEvaluatorï¼Œä»ç»“æœä¸­æŒ‰å‰ç¼€åˆ†åˆ«ä¿å­˜
                        from sentence_transformers.evaluation import SequentialEvaluator
                        if isinstance(dev_evaluator, SequentialEvaluator):
                            logger.info(f"æ£€æµ‹åˆ°SequentialEvaluatorï¼Œä»ç»“æœä¸­æŒ‰å‰ç¼€æå–{len(dev_evaluator.evaluators)}ä¸ªå­è¯„ä¼°å™¨çš„åŸºçº¿ç»“æœ")

                            # ä»SequentialEvaluatorç»“æœä¸­æŒ‰å‰ç¼€æå–å„source_idçš„ç»“æœ
                            self._save_sequential_evaluator_results_by_prefix(task_id, dev_results, 'eval', dev_evaluator, is_baseline=True)
                        else:
                            # éSequentialEvaluatorï¼Œå•æ•°æ®é›†ä½¿ç”¨source_id="1"
                            if dev_results:
                                self._save_baseline_results_by_source_id(task_id, dev_results, 'eval', "1")

                except Exception as e:
                    logger.warning(f"éªŒè¯é›†åŸºçº¿è¯„ä¼°å¤±è´¥: {e}")
            else:
                logger.info("æ²¡æœ‰æœ‰æ•ˆçš„éªŒè¯è¯„ä¼°å™¨ï¼Œè·³è¿‡åŸºçº¿æ¨¡å‹éªŒè¯é›†è¯„ä¼°")

        # Test set baseline evaluation (matching train.py lines 1491-1526)
        base_test_evaluator = None
        if datasets.get('test') is not None:
            # Check if multi-dataset (same logic as original)
            if self.data_loader.is_multi_dataset(datasets['test']):
                # Multi-dataset: create SequentialEvaluator
                target_column = self._get_target_column(datasets)
                model_name = self.model_config.get('model_name_or_path', 'unknown')
                short_model_name = model_name.split("/")[-1] if "/" in model_name else model_name
                run_name = f"{self.train_type}-{short_model_name}"

                # è·å–æ•°æ®æºæ˜ å°„
                data_source_mapping = self._get_data_source_mapping(datasets['train'])
                base_test_evaluator = self.evaluator_factory.create_multi_evaluator(
                    datasets['test'], target_column, run_name, data_source_mapping
                )
            elif 'test' in evaluators:
                # Single dataset: use single evaluator
                base_test_evaluator = evaluators['test']

            # Evaluate baseline model
            if base_test_evaluator is not None:
                try:
                    base_test_results = self.evaluator_factory.evaluate_model(model, base_test_evaluator)
                    logger.info(f"æµ‹è¯•é›†åŸºçº¿è¯„ä¼°ç»“æœ: {base_test_results}")

                    task_id = self.raw_config.get('task_id')
                    if task_id:
                        # æ£€æµ‹æ˜¯å¦ä¸ºSequentialEvaluatorï¼Œä»ç»“æœä¸­æŒ‰å‰ç¼€åˆ†åˆ«ä¿å­˜
                        from sentence_transformers.evaluation import SequentialEvaluator
                        if isinstance(base_test_evaluator, SequentialEvaluator):
                            logger.info(f"æ£€æµ‹åˆ°SequentialEvaluatorï¼Œä»ç»“æœä¸­æŒ‰å‰ç¼€æå–{len(base_test_evaluator.evaluators)}ä¸ªå­è¯„ä¼°å™¨çš„åŸºçº¿ç»“æœ")

                            # ä»SequentialEvaluatorç»“æœä¸­æŒ‰å‰ç¼€æå–å„source_idçš„ç»“æœ
                            self._save_sequential_evaluator_results_by_prefix(task_id, base_test_results, 'test', base_test_evaluator, is_baseline=True)
                        else:
                            # éSequentialEvaluatorï¼Œå•æ•°æ®é›†ä½¿ç”¨source_id="1"
                            if base_test_results:
                                self._save_baseline_results_by_source_id(task_id, base_test_results, 'test', "1")

                except Exception as e:
                    logger.warning(f"æµ‹è¯•é›†åŸºçº¿è¯„ä¼°å¤±è´¥: {e}")
            else:
                logger.info("æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•è¯„ä¼°å™¨ï¼Œè·³è¿‡åŸºçº¿æ¨¡å‹æµ‹è¯•é›†è¯„ä¼°")

    def _perform_final_evaluation(self, model, datasets, evaluators):
        """Perform final evaluation after training (matching original train.py)."""
        logger.info("å¼€å§‹è®­ç»ƒåæœ€ç»ˆè¯„ä¼°")

        # Final validation set evaluation (matching train.py lines 1942-1978)
        if datasets.get('eval') is not None:
            # Check if multi-dataset (same logic as original)
            if self.data_loader.is_multi_dataset(datasets['eval']):
                # Multi-dataset: create SequentialEvaluator
                target_column = self._get_target_column(datasets)
                model_name = self.model_config.get('model_name_or_path', 'unknown')
                short_model_name = model_name.split("/")[-1] if "/" in model_name else model_name
                run_name = f"{self.train_type}-{short_model_name}"

                # è·å–æ•°æ®æºæ˜ å°„
                data_source_mapping = self._get_data_source_mapping(datasets['train'])
                final_eval_evaluator = self.evaluator_factory.create_multi_evaluator(
                    datasets['eval'], target_column, run_name, data_source_mapping
                )
            elif 'dev' in evaluators:
                # Single dataset: use single evaluator
                final_eval_evaluator = evaluators['dev']
            else:
                final_eval_evaluator = None

            # Evaluate trained validation set
            if final_eval_evaluator is not None:
                try:
                    # è¿è¡Œè¯„ä¼°å™¨ï¼ˆåŒ…æ‹¬SequentialEvaluatorä¼šè‡ªåŠ¨è¿è¡Œæ‰€æœ‰å­è¯„ä¼°å™¨ï¼‰
                    final_eval_results = self.evaluator_factory.evaluate_model(model, final_eval_evaluator)
                    logger.info(f"éªŒè¯é›†æœ€ç»ˆè¯„ä¼°ç»“æœ: {final_eval_results}")

                    task_id = self.raw_config.get('task_id')
                    if task_id:
                        # æ£€æµ‹æ˜¯å¦ä¸ºSequentialEvaluatorï¼Œä»ç»“æœä¸­æŒ‰å‰ç¼€åˆ†åˆ«ä¿å­˜
                        from sentence_transformers.evaluation import SequentialEvaluator
                        if isinstance(final_eval_evaluator, SequentialEvaluator):
                            logger.info(f"æ£€æµ‹åˆ°SequentialEvaluatorï¼Œä»ç»“æœä¸­æŒ‰å‰ç¼€æå–{len(final_eval_evaluator.evaluators)}ä¸ªå­è¯„ä¼°å™¨çš„æœ€ç»ˆç»“æœ")

                            # ä»SequentialEvaluatorç»“æœä¸­æŒ‰å‰ç¼€æå–å„source_idçš„ç»“æœ
                            self._save_sequential_evaluator_results_by_prefix(task_id, final_eval_results, 'eval', final_eval_evaluator, is_baseline=False)
                        else:
                            # éSequentialEvaluatorï¼Œå•æ•°æ®é›†ä½¿ç”¨source_id="1"
                            if final_eval_results:
                                self._save_final_results_by_source_id(task_id, final_eval_results, 'eval', "1")

                except Exception as e:
                    logger.warning(f"éªŒè¯é›†æœ€ç»ˆè¯„ä¼°å¤±è´¥: {e}")
            else:
                logger.info("æ²¡æœ‰æœ‰æ•ˆçš„éªŒè¯è¯„ä¼°å™¨ï¼Œè·³è¿‡éªŒè¯é›†æœ€ç»ˆè¯„ä¼°")

        # Final test set evaluation (matching train.py lines 1980+)
        if datasets.get('test') is not None:
            # Check if multi-dataset (same logic as original)
            if self.data_loader.is_multi_dataset(datasets['test']):
                # Multi-dataset: create SequentialEvaluator
                target_column = self._get_target_column(datasets)
                model_name = self.model_config.get('model_name_or_path', 'unknown')
                short_model_name = model_name.split("/")[-1] if "/" in model_name else model_name
                run_name = f"{self.train_type}-{short_model_name}"

                # è·å–æ•°æ®æºæ˜ å°„
                data_source_mapping = self._get_data_source_mapping(datasets['train'])
                test_evaluator = self.evaluator_factory.create_multi_evaluator(
                    datasets['test'], target_column, run_name, data_source_mapping
                )
            elif 'test' in evaluators:
                # Single dataset: use single evaluator
                test_evaluator = evaluators['test']
            else:
                test_evaluator = None

            # Evaluate trained test set
            if test_evaluator is not None:
                try:
                    # è¿è¡Œè¯„ä¼°å™¨ï¼ˆåŒ…æ‹¬SequentialEvaluatorä¼šè‡ªåŠ¨è¿è¡Œæ‰€æœ‰å­è¯„ä¼°å™¨ï¼‰
                    test_results = self.evaluator_factory.evaluate_model(model, test_evaluator)
                    logger.info(f"æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°ç»“æœ: {test_results}")

                    task_id = self.raw_config.get('task_id')
                    if task_id:
                        # æ£€æµ‹æ˜¯å¦ä¸ºSequentialEvaluatorï¼Œä»ç»“æœä¸­æŒ‰å‰ç¼€åˆ†åˆ«ä¿å­˜
                        from sentence_transformers.evaluation import SequentialEvaluator
                        if isinstance(test_evaluator, SequentialEvaluator):
                            logger.info(f"æ£€æµ‹åˆ°SequentialEvaluatorï¼Œä»ç»“æœä¸­æŒ‰å‰ç¼€æå–{len(test_evaluator.evaluators)}ä¸ªå­è¯„ä¼°å™¨çš„æœ€ç»ˆç»“æœ")

                            # ä»SequentialEvaluatorç»“æœä¸­æŒ‰å‰ç¼€æå–å„source_idçš„ç»“æœ
                            self._save_sequential_evaluator_results_by_prefix(task_id, test_results, 'test', test_evaluator, is_baseline=False)
                        else:
                            # éSequentialEvaluatorï¼Œå•æ•°æ®é›†ä½¿ç”¨source_id="1"
                            if test_results:
                                self._save_final_results_by_source_id(task_id, test_results, 'test', "1")

                except Exception as e:
                    logger.warning(f"æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°å¤±è´¥: {e}")
            else:
                logger.info("æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•è¯„ä¼°å™¨ï¼Œè·³è¿‡æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°")

    def _save_baseline_results_by_source_id(self, task_id: str, results: dict, split_type: str, source_id: str):
        """Save baseline evaluation results to database by source_id."""
        try:
            from bubble_rag.training.mysql_service.service.training_dataset_service import TrainingDatasetService
            from bubble_rag.training.mysql_service.entity.training_task_models import safe_get_session
            from bubble_rag.training.mysql_service.entity.training_dataset_models import DatasetInfo
            from sqlmodel import select

            # åœ¨åŒä¸€ä¸ªä¼šè¯ä¸­å®ŒæˆæŸ¥è¯¢å’Œæ›´æ–°
            with safe_get_session() as session:
                statement = select(DatasetInfo).where(
                    DatasetInfo.task_id == task_id,
                    DatasetInfo.data_source_id == source_id,
                    DatasetInfo.split_type == split_type
                )
                dataset_info = session.exec(statement).first()

                if dataset_info:
                    dataset_id = dataset_info.id
                    dataset_name = dataset_info.dataset_name

                    # åœ¨åŒä¸€ä¸ªä¼šè¯å¤–è°ƒç”¨æ›´æ–°æ–¹æ³•
                    TrainingDatasetService.update_eval_results(
                        dataset_id=dataset_id,
                        base_results=results
                    )
                    logger.info(f"{split_type}é›†åŸºçº¿ç»“æœå·²ä¿å­˜åˆ°æ•°æ®é›† '{dataset_name}' (source_id: {source_id})")
                else:
                    logger.warning(f"æœªæ‰¾åˆ°source_id '{source_id}' å’Œsplit_type '{split_type}' å¯¹åº”çš„æ•°æ®é›†è®°å½•")

        except Exception as e:
            logger.warning(f"æŒ‰source_idä¿å­˜{split_type}é›†åŸºçº¿ç»“æœå¤±è´¥: {e}")

    def _save_final_results_by_source_id(self, task_id: str, results: dict, split_type: str, source_id: str):
        """Save final evaluation results to database by source_id."""
        try:
            from bubble_rag.training.mysql_service.service.training_dataset_service import TrainingDatasetService
            from bubble_rag.training.mysql_service.entity.training_task_models import safe_get_session
            from bubble_rag.training.mysql_service.entity.training_dataset_models import DatasetInfo
            from sqlmodel import select

            # åœ¨åŒä¸€ä¸ªä¼šè¯ä¸­å®ŒæˆæŸ¥è¯¢å’Œæ›´æ–°
            with safe_get_session() as session:
                statement = select(DatasetInfo).where(
                    DatasetInfo.task_id == task_id,
                    DatasetInfo.data_source_id == source_id,
                    DatasetInfo.split_type == split_type
                )
                dataset_info = session.exec(statement).first()

                if dataset_info:
                    dataset_id = dataset_info.id
                    dataset_name = dataset_info.dataset_name

                    # åœ¨åŒä¸€ä¸ªä¼šè¯å¤–è°ƒç”¨æ›´æ–°æ–¹æ³•
                    TrainingDatasetService.update_eval_results(
                        dataset_id=dataset_id,
                        final_results=results
                    )
                    logger.info(f"{split_type}é›†æœ€ç»ˆç»“æœå·²ä¿å­˜åˆ°æ•°æ®é›† '{dataset_name}' (source_id: {source_id})")
                else:
                    logger.warning(f"æœªæ‰¾åˆ°source_id '{source_id}' å’Œsplit_type '{split_type}' å¯¹åº”çš„æ•°æ®é›†è®°å½•")

        except Exception as e:
            logger.warning(f"æŒ‰source_idä¿å­˜{split_type}é›†æœ€ç»ˆç»“æœå¤±è´¥: {e}")

    def _save_sequential_evaluator_results_by_prefix(self, task_id: str, sequential_results: dict, split_type: str,
                                                   sequential_evaluator, is_baseline: bool = True):
        """ä»SequentialEvaluatorç»“æœä¸­æŒ‰å‰ç¼€æå–å„source_idçš„ç»“æœå¹¶åˆ†åˆ«ä¿å­˜."""
        try:
            # è·å–æ‰€æœ‰source_idï¼ˆå³evaluator namesï¼‰
            source_ids = [getattr(evaluator, 'name', 'unknown') for evaluator in sequential_evaluator.evaluators]
            logger.info(f"ä»SequentialEvaluatorç»“æœä¸­æå–source_ids: {source_ids}")

            # ä¸ºæ¯ä¸ªsource_idæå–å¯¹åº”çš„metrics
            for source_id in source_ids:
                try:
                    # æå–å¸¦æœ‰è¯¥source_idå‰ç¼€çš„æ‰€æœ‰metrics
                    prefix = f"{source_id}_"
                    source_results = {}

                    for key, value in sequential_results.items():
                        if key.startswith(prefix):
                            # å»æ‰å‰ç¼€ï¼Œä¿ç•™åŸå§‹metricåç§°
                            original_key = key[len(prefix):]
                            source_results[original_key] = value

                    if source_results:
                        logger.info(f"source_id '{source_id}' çš„{'åŸºçº¿' if is_baseline else 'æœ€ç»ˆ'}ç»“æœ: {source_results}")

                        # æŒ‰source_idä¿å­˜ç»“æœ
                        if is_baseline:
                            self._save_baseline_results_by_source_id(task_id, source_results, split_type, source_id)
                        else:
                            self._save_final_results_by_source_id(task_id, source_results, split_type, source_id)
                    else:
                        logger.warning(f"æœªæ‰¾åˆ°source_id '{source_id}' å¯¹åº”çš„metricsï¼ˆå‰ç¼€: {prefix}ï¼‰")

                except Exception as e:
                    logger.warning(f"å¤„ç†source_id '{source_id}' çš„ç»“æœå¤±è´¥: {e}")

        except Exception as e:
            result_type = "åŸºçº¿" if is_baseline else "æœ€ç»ˆ"
            logger.warning(f"ä»SequentialEvaluatorç»“æœæŒ‰å‰ç¼€æå–{result_type}ç»“æœå¤±è´¥: {e}")
    
    def _setup_progress_callback(self, trainer, progress_callback):
        """Setup progress callback for trainer (matching original train.py logic)."""
        if not progress_callback:
            return

        try:
            # åˆå§‹åŒ–æ­¥éª¤è®¡æ•°å™¨å’Œè¿›åº¦è·Ÿè¸ªï¼ˆåŒ¹é…åŸtrain.pyé€»è¾‘ï¼‰
            step_count = 0
            last_reported_progress = -1  # ä¸Šæ¬¡æŠ¥å‘Šçš„è¿›åº¦ç™¾åˆ†æ¯”ï¼Œç”¨äº1%èŠ‚æµ
            max_steps = getattr(trainer.args, 'max_steps', None) if hasattr(trainer, 'args') else None
            logger.info(f"è¿›åº¦å›è°ƒåˆå§‹åŒ– - é¢„è®¾max_steps: {max_steps}")

            # å°è¯•ä»trainer.stateè·å–å®é™…çš„max_stepsï¼ˆè®­ç»ƒå¼€å§‹åå¯ç”¨ï¼‰
            if hasattr(trainer, 'state') and hasattr(trainer.state, 'max_steps') and trainer.state.max_steps > 0:
                actual_max_steps = trainer.state.max_steps
                logger.info(f"ä»trainer.stateè·å–å®é™…max_steps: {actual_max_steps}")
                max_steps = actual_max_steps

            # ä¿®å¤ï¼šmax_steps=-1 æ˜¯æœ‰æ•ˆå€¼ï¼Œè¡¨ç¤ºepoch-basedè®­ç»ƒï¼Œä¸éœ€è¦ä¼°ç®—
            if (max_steps is None or (max_steps <= 0 and max_steps != -1)) and hasattr(trainer, 'args') and hasattr(trainer.args, 'num_train_epochs'):
                logger.info("éœ€è¦ä¼°ç®—max_stepsï¼Œå¼€å§‹è®¡ç®—...")
                # ä¼°ç®—æœ€å¤§æ­¥æ•°ï¼ˆåŒ¹é…åŸtrain.pyé€»è¾‘ï¼‰
                try:
                    if hasattr(trainer, 'train_dataset') and hasattr(trainer.train_dataset, '__len__'):
                        dataset_size = len(trainer.train_dataset)
                    elif hasattr(trainer, '_train_dataset') and hasattr(trainer._train_dataset, '__len__'):
                        dataset_size = len(trainer._train_dataset)
                    else:
                        dataset_size = 1000  # é»˜è®¤ä¼°ç®—

                    batch_size = getattr(trainer.args, 'per_device_train_batch_size', 16) if hasattr(trainer, 'args') else 16
                    gradient_accumulation_steps = getattr(trainer.args, 'gradient_accumulation_steps', 1) if hasattr(trainer, 'args') else 1
                    num_epochs = getattr(trainer.args, 'num_train_epochs', 3) if hasattr(trainer, 'args') else 3

                    # ä¿®å¤max_stepsè®¡ç®—ï¼šåŒ…å«GPUæ•°é‡ï¼ˆåŒ¹é…åŸtrain.pyé€»è¾‘ï¼‰
                    import torch
                    num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1
                    effective_batch_size = batch_size * gradient_accumulation_steps * num_gpus
                    steps_per_epoch = max(1, dataset_size // effective_batch_size)
                    max_steps = int(steps_per_epoch * num_epochs)

                    logger.info(f"è¯¦ç»†è®¡ç®—å‚æ•°: æ•°æ®é›†å¤§å°={dataset_size}, GPUæ•°é‡={num_gpus}, æ¯è®¾å¤‡æ‰¹æ¬¡={batch_size}, æ¢¯åº¦ç´¯ç§¯={gradient_accumulation_steps}")
                    logger.info(f"è®¡ç®—ç»“æœ: æœ‰æ•ˆæ‰¹æ¬¡å¤§å°={effective_batch_size}, æ¯è½®æ­¥æ•°={steps_per_epoch}, ä¼°ç®—æœ€å¤§æ­¥æ•°={max_steps}")
                except Exception as e:
                    logger.warning(f"ä¼°ç®—æœ€å¤§æ­¥æ•°å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼1000")
                    max_steps = 1000

            # ç›´æ¥åŒ…è£…trainerçš„training_stepæ–¹æ³•ï¼ˆåŒ¹é…åŸtrain.pyé€»è¾‘ï¼‰
            if hasattr(trainer, 'training_step'):
                original_training_step = trainer.training_step

                def wrapped_training_step(*args, **kwargs):
                    nonlocal step_count, last_reported_progress
                    result = original_training_step(*args, **kwargs)

                    # ä½¿ç”¨trainer.state.global_stepè€Œä¸æ˜¯è‡ªå·±è®¡æ•°ï¼Œç¡®ä¿ä¸å®é™…è®­ç»ƒæ­¥æ•°åŒæ­¥
                    if hasattr(trainer, 'state') and hasattr(trainer.state, 'global_step'):
                        step_count = trainer.state.global_step
                    else:
                        step_count += 1  # å›é€€æ–¹æ¡ˆ

                    # æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼ˆåŒ¹é…åŸtrain.pyï¼‰
                    if step_count % 10 == 0:
                        logger.info(f"training_stepè¢«è°ƒç”¨: ç¬¬{step_count}æ­¥")

                    # å®ç°1%è¿›åº¦èŠ‚æµæ›´æ–°ç­–ç•¥
                    try:
                        # åŠ¨æ€è·å–trainer.state.max_stepsï¼ˆå¦‚æœå¯ç”¨ä¸”åˆç†ï¼‰
                        runtime_max_steps = None
                        if hasattr(trainer, 'state') and hasattr(trainer.state, 'max_steps') and trainer.state.max_steps > 0:
                            runtime_max_steps = trainer.state.max_steps

                        # ä¼˜å…ˆä½¿ç”¨è¿è¡Œæ—¶max_stepsï¼Œç„¶åæ˜¯ä¼°ç®—å€¼ï¼Œæœ€åæ˜¯é»˜è®¤å€¼
                        if runtime_max_steps and runtime_max_steps > 0:
                            effective_max_steps = runtime_max_steps
                        elif max_steps and max_steps > 0:
                            effective_max_steps = max_steps
                        else:
                            effective_max_steps = 1000


                        # è®¡ç®—å½“å‰è¿›åº¦ç™¾åˆ†æ¯”
                        current_progress = min(99.0, (step_count / effective_max_steps) * 100)

                        # åªæœ‰è¿›åº¦å¢åŠ 1%ä»¥ä¸Šæ—¶æ‰æ›´æ–°ï¼ˆèŠ‚æµç­–ç•¥ï¼‰
                        progress_change = current_progress - last_reported_progress
                        should_update = (
                            progress_change >= 1.0 or  # è¿›åº¦å˜åŒ–1%ä»¥ä¸Š
                            last_reported_progress == -1 or  # é¦–æ¬¡æ›´æ–°
                            step_count % 100 == 0  # æ¯100æ­¥å¼ºåˆ¶æ›´æ–°ä¸€æ¬¡ï¼ˆé˜²æ­¢é•¿æ—¶é—´æ— æ›´æ–°ï¼‰
                        )

                        if should_update:
                            # è°ƒç”¨å¤–éƒ¨è¿›åº¦å›è°ƒ
                            progress_callback(step_count, effective_max_steps, "è®­ç»ƒä¸­")

                            # æ›´æ–°ä¸Šæ¬¡æŠ¥å‘Šçš„è¿›åº¦
                            last_reported_progress = current_progress

                            logger.info(f"è¿›åº¦æ›´æ–°(1%èŠ‚æµ): {current_progress:.1f}% (æ­¥æ•°: {step_count}/{effective_max_steps}, å˜åŒ–: +{progress_change:.1f}%)")

                            # æ³¨æ„ï¼šä¸å†ç›´æ¥è°ƒç”¨task_manager.update_task_progressï¼Œé¿å…ä¸å¤–å±‚progress_callbacké‡å¤
                        elif step_count % 50 == 0:
                            # å®šæœŸæ˜¾ç¤ºå½“å‰è¿›åº¦ï¼ˆä½†ä¸æ›´æ–°æ•°æ®åº“ï¼‰
                            logger.debug(f"å½“å‰è¿›åº¦: {current_progress:.1f}% (æ­¥æ•°: {step_count}/{effective_max_steps}, æœªè¾¾1%æ›´æ–°é˜ˆå€¼)")
                    except KeyboardInterrupt:
                        logger.info("æ£€æµ‹åˆ°è®­ç»ƒåœæ­¢ä¿¡å·ï¼Œä¸­æ–­è®­ç»ƒ")
                        if hasattr(trainer, 'state'):
                            trainer.state.should_epoch_stop = True
                            trainer.state.should_training_stop = True
                        raise
                    except Exception as e:
                        logger.error(f"progress_callbackè°ƒç”¨å¤±è´¥: {e}")

                    return result

                trainer.training_step = wrapped_training_step
                logger.info("æˆåŠŸåŒ…è£…training_stepæ–¹æ³•è¿›è¡Œè¿›åº¦è¿½è¸ª")

            # åŒ…è£…trainerçš„logæ–¹æ³•æ¥æ”¶é›†lossæ•°æ®ï¼ˆåŒ¹é…åŸtrain.pyé€»è¾‘ï¼‰
            if hasattr(trainer, 'log'):
                original_log = trainer.log
                task_id = self.raw_config.get('task_id')
                output_dir = getattr(trainer.args, 'output_dir', './output') if hasattr(trainer, 'args') else './output'

                def wrapped_log(logs, start_time=None):
                    try:
                        # è°ƒç”¨åŸå§‹logæ–¹æ³• - ç»Ÿä¸€ä½¿ç”¨åŸtrain.pyçš„è°ƒç”¨æ–¹å¼
                        if original_log and callable(original_log):
                            result = original_log(logs, start_time)
                        else:
                            result = None
                    except Exception as e:
                        logger.error(f"åŸå§‹logæ–¹æ³•è°ƒç”¨å¤±è´¥: {e}")
                        result = None


                    # æ·»åŠ åŸºäºlogçš„è¿›åº¦æ›´æ–°ä½œä¸ºfallback (åŒ¹é…åŸtrain.py)
                    try:
                        if logs and task_id:
                            # å°è¯•ä»logsä¸­è·å–è¿›åº¦ä¿¡æ¯
                            if 'step' in logs and hasattr(trainer, 'state') and hasattr(trainer.state, 'max_steps'):
                                current_step = logs['step']
                                max_steps = trainer.state.max_steps

                                if max_steps and max_steps > 0:
                                    from bubble_rag.training.model_sft.services.task_manager import task_manager
                                    progress_percentage = min(99.0, (current_step / max_steps) * 100)
                                    task_manager.update_task_progress(task_id, progress_percentage, f"è®­ç»ƒä¸­: {current_step}/{max_steps}")

                                    if current_step % 10 == 0:
                                        logger.info(f"åŸºäºlogçš„è¿›åº¦æ›´æ–°: {progress_percentage:.1f}% (æ­¥æ•°: {current_step}/{max_steps})")
                    except Exception as e:
                        if logs and logs.get('step', 0) % 20 == 0:  # å‡å°‘é”™è¯¯æ—¥å¿—é¢‘ç‡
                            logger.warning(f"åŸºäºlogçš„è¿›åº¦æ›´æ–°å¤±è´¥: {e}")

                    return result

                trainer.log = wrapped_log
                logger.info("æˆåŠŸåŒ…è£…logæ–¹æ³•è¿›è¡Œlossæ•°æ®æ”¶é›†")

            # åˆå§‹è¿›åº¦å›è°ƒï¼ˆåŒ¹é…åŸtrain.pyï¼‰
            try:
                # ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨å·²ç»æ­£ç¡®è®¡ç®—çš„æ­¥æ•°ï¼Œä¸ä¾èµ–ä¸å¯é çš„åˆå§‹max_steps
                # ç­‰è®­ç»ƒå¼€å§‹åé€šè¿‡training_stepå›è°ƒæ¥æ›´æ–°è¿›åº¦ï¼Œé¿å…é”™è¯¯çš„åˆå§‹å›è°ƒ
                logger.info("â­ï¸ è·³è¿‡åˆå§‹progress_callbackï¼Œç­‰å¾…è®­ç»ƒå¼€å§‹åé€šè¿‡training_stepå›è°ƒæ›´æ–°è¿›åº¦")
                # ä¸è°ƒç”¨åˆå§‹å›è°ƒï¼Œè®©ç¬¬ä¸€æ¬¡training_stepå›è°ƒæ¥è®¾ç½®æ­£ç¡®çš„total_steps
            except Exception as e:
                logger.error(f"åˆå§‹progress_callbackè®¾ç½®å¤±è´¥: {e}")

        except Exception as e:
            logger.warning(f"è®¾ç½®è¿›åº¦å›è°ƒå¤±è´¥: {e}")

    def _setup_loss_collection(self, trainer):
        """Setup loss data collection without progress callback (matching original train.py)."""
        try:
            logger.info(f"[DEBUG] å¼€å§‹è®¾ç½®lossæ”¶é›†ï¼Œtrainerç±»å‹: {type(trainer)}")
            # åŒ…è£…trainerçš„logæ–¹æ³•æ¥æ”¶é›†lossæ•°æ®ï¼ˆåŒ¹é…åŸtrain.pyé€»è¾‘ï¼‰
            if hasattr(trainer, 'log'):
                logger.info(f"[DEBUG] traineræœ‰logæ–¹æ³•: {type(trainer.log)}")
                original_log = trainer.log
                task_id = self.raw_config.get('task_id')
                output_dir = getattr(trainer.args, 'output_dir', './output') if hasattr(trainer, 'args') else './output'
                step_count = 0

                def wrapped_log(logs, start_time=None):
                    nonlocal step_count
                    try:
                        # LogåŒ…è£…æ­£å¸¸å·¥ä½œ

                        # è°ƒç”¨åŸå§‹logæ–¹æ³• - ç»Ÿä¸€ä½¿ç”¨åŸtrain.pyçš„è°ƒç”¨æ–¹å¼
                        if original_log and callable(original_log):
                            result = original_log(logs, start_time)
                        else:
                            result = None
                    except Exception as e:
                        logger.error(f"åŸå§‹logæ–¹æ³•è°ƒç”¨å¤±è´¥: {e}")
                        result = None

                    # å¯ç”¨lossæœ¬åœ°æ–‡ä»¶ä¿å­˜åŠŸèƒ½ - ä¸train.pyé€»è¾‘ä¿æŒä¸€è‡´
                    try:
                        # åªè¦æœ‰lossæˆ–evalå­—æ®µå°±è®°å½•ï¼Œæ‰©å±•ä»¥åŒ…å«æ‰€æœ‰evalæŒ‡æ ‡
                        has_loss = any(key in logs for key in ['train_loss', 'eval_loss', 'loss'])
                        has_eval = any('eval' in key.lower() for key in logs.keys())
                        if logs and task_id and (has_loss or has_eval):
                            from bubble_rag.training.model_sft.utils.loss_manager import get_loss_manager

                            # åŒºåˆ†trainingå’Œevaluation logçš„stepè®¡ç®—
                            if 'step' in logs:
                                # å¦‚æœlogsä¸­æœ‰stepï¼Œç›´æ¥ä½¿ç”¨ï¼ˆè¿™é€šå¸¸æ˜¯æ­£ç¡®çš„ï¼‰
                                current_step = logs['step']
                                logger.debug(f"ä½¿ç”¨logsä¸­çš„step: {current_step}")
                            else:
                                # å½“logsä¸­æ²¡æœ‰stepæ—¶ï¼Œéœ€è¦æ ¹æ®logç±»å‹ç¡®å®šstep
                                is_eval_log = any('eval' in key.lower() for key in logs.keys())

                                if is_eval_log:
                                    # evaluation log: æ ¹æ®eval_stepsè®¡ç®—
                                    # evalé€šå¸¸åœ¨ç‰¹å®šæ­¥æ•°è§¦å‘ï¼Œæˆ‘ä»¬éœ€è¦ä»trainerè·å–å½“å‰çœŸå®æ­¥æ•°
                                    try:
                                        if hasattr(trainer, 'state') and hasattr(trainer.state, 'global_step'):
                                            current_step = trainer.state.global_step
                                            logger.debug(f"Eval log - ä½¿ç”¨trainer.state.global_step: {current_step}")
                                        else:
                                            # å›é€€ï¼šä¼°ç®—evalæ­¥æ•°
                                            eval_steps = 100  # é»˜è®¤å€¼
                                            if hasattr(trainer, 'args') and hasattr(trainer.args, 'eval_steps'):
                                                eval_steps = trainer.args.eval_steps
                                            current_step = eval_steps * (step_count // 2 + 1)  # ç²—ç•¥ä¼°ç®—
                                            logger.debug(f"Eval log - ä¼°ç®—æ­¥æ•°: {current_step}")
                                    except:
                                        current_step = step_count * 10  # æœ€åçš„å›é€€
                                else:
                                    # training log: æ ¹æ®logging_stepsè®¡ç®—
                                    step_count += 1
                                    logging_steps = 10  # é»˜è®¤å€¼
                                    if hasattr(trainer, 'args') and hasattr(trainer.args, 'logging_steps'):
                                        logging_steps = trainer.args.logging_steps

                                    current_step = step_count * logging_steps
                                    logger.debug(f"Training log - è®¡ç®—æ­¥æ•°: å›è°ƒ#{step_count} Ã— {logging_steps} = {current_step}")

                            current_epoch = logs.get('epoch', None)

                            # æ„å»ºlossæŒ‡æ ‡å­—å…¸ - åŒ…å«æ‰€æœ‰æ•°å€¼å‹æŒ‡æ ‡
                            loss_metrics = {}
                            for key, value in logs.items():
                                if isinstance(value, (int, float)):
                                    # åŒ…å«lossã€evalæŒ‡æ ‡ï¼Œä»¥åŠå¸¸è§çš„è¯„ä¼°æŒ‡æ ‡
                                    if any(keyword in key.lower() for keyword in ['loss', 'eval', 'accuracy', 'f1', 'precision', 'recall', 'pearson', 'spearman']):
                                        loss_metrics[key] = value

                            if loss_metrics:
                                loss_manager = get_loss_manager(output_dir, task_id)

                                # æ£€æŸ¥æ˜¯å¦æ˜¯æ··åˆçš„evalè®°å½•ï¼Œéœ€è¦æŒ‰æ•°æ®æºåˆ†ç¦»
                                is_mixed_eval = self._is_mixed_eval_record(loss_metrics)
                                logger.info(f"[DEBUG] æ··åˆevalæ£€æµ‹: {is_mixed_eval}, æŒ‡æ ‡: {list(loss_metrics.keys())}")
                                logger.info(f"[DEBUG] _current_data_source_mappingå­˜åœ¨: {hasattr(self, '_current_data_source_mapping')}")
                                if hasattr(self, '_current_data_source_mapping'):
                                    logger.info(f"[DEBUG] _current_data_source_mapping: {self._current_data_source_mapping}")

                                if is_mixed_eval:
                                    # æŒ‰æ•°æ®æºåˆ†ç¦»å¹¶ä¿å­˜å¤šæ¡è®°å½•
                                    logger.info(f"è°ƒç”¨_save_separated_eval_recordsè¿›è¡Œlossåç§°è½¬æ¢")
                                    self._save_separated_eval_records(loss_manager, current_step, loss_metrics, current_epoch)
                                else:
                                    # æ™®é€šè®°å½•ï¼Œç›´æ¥ä¿å­˜
                                    logger.info(f"ä¿å­˜å•æ¡è®°å½•: step={current_step}, metrics={list(loss_metrics.keys())}")
                                    data_source_mapping = getattr(self, '_current_data_source_mapping', {})
                                    loss_manager.save_loss_record(current_step, loss_metrics, current_epoch, True, data_source_mapping)

                                logger.info(f"Losså·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶: step={current_step}, metrics={list(loss_metrics.keys())}")

                                # ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ•°æ®åº“ (åŒ…å«eval_losså’Œå…¶ä»–evalæŒ‡æ ‡)
                                eval_metrics = {k: v for k, v in loss_metrics.items() if k.startswith('eval_')}
                                if eval_metrics:
                                    try:
                                        # ä¼ é€’data_source_mappingç»™è¯„ä¼°ç»“æœä¿å­˜å‡½æ•°
                                        data_source_mapping = getattr(self, '_current_data_source_mapping', {})
                                        self._save_training_evaluation_results(task_id, eval_metrics, current_step, current_epoch, data_source_mapping)
                                        logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“: step={current_step}, metrics={list(eval_metrics.keys())}")
                                    except Exception as e:
                                        logger.warning(f"ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ•°æ®åº“å¤±è´¥: {e}")

                            else:
                                logger.debug(f"â­ï¸ æ²¡æœ‰æœ‰æ•ˆçš„lossæŒ‡æ ‡å¯ä¿å­˜: {list(logs.keys())}")

                    except Exception as e:
                        logger.warning(f"ä¿å­˜lossåˆ°æœ¬åœ°æ–‡ä»¶å¤±è´¥: {e}")

                    return result

                trainer.log = wrapped_log
                logger.info(f"[DEBUG] æˆåŠŸåŒ…è£…logæ–¹æ³•è¿›è¡Œlossæ•°æ®æ”¶é›†ï¼Œæ–°logæ–¹æ³•: {type(trainer.log)}")
            else:
                logger.warning("[DEBUG] traineræ²¡æœ‰logæ–¹æ³•ï¼Œæ— æ³•åŒ…è£…")

        except Exception as e:
            logger.warning(f"è®¾ç½®lossæ•°æ®æ”¶é›†å¤±è´¥: {e}")

    def _setup_basic_progress_tracking(self, trainer):
        """Setup basic progress tracking without external progress callback (matching original train.py)."""
        try:
            task_id = self.raw_config.get('task_id')
            if not task_id:
                return

            # åˆå§‹åŒ–æ­¥éª¤è®¡æ•°å™¨å’Œè¿›åº¦è·Ÿè¸ªï¼ˆåŒ¹é…åŸtrain.pyé€»è¾‘ï¼‰
            step_count = 0
            last_reported_progress = -1  # ä¸Šæ¬¡æŠ¥å‘Šçš„è¿›åº¦ç™¾åˆ†æ¯”ï¼Œç”¨äº1%èŠ‚æµ
            max_steps = getattr(trainer.args, 'max_steps', None) if hasattr(trainer, 'args') else None
            logger.info(f"åŸºç¡€è¿›åº¦è·Ÿè¸ªåˆå§‹åŒ– - é¢„è®¾max_steps: {max_steps}")

            # ä¿®å¤ï¼šmax_steps=-1 æ˜¯æœ‰æ•ˆå€¼ï¼Œè¡¨ç¤ºepoch-basedè®­ç»ƒï¼Œä¸éœ€è¦ä¼°ç®—
            if (max_steps is None or (max_steps <= 0 and max_steps != -1)) and hasattr(trainer, 'args') and hasattr(trainer.args, 'num_train_epochs'):
                logger.info("åŸºç¡€è¿›åº¦è·Ÿè¸ªéœ€è¦ä¼°ç®—max_stepsï¼Œå¼€å§‹è®¡ç®—...")
                # ä¼°ç®—æœ€å¤§æ­¥æ•°ï¼ˆåŒ¹é…åŸtrain.pyé€»è¾‘ï¼‰
                try:
                    if hasattr(trainer, 'train_dataset') and hasattr(trainer.train_dataset, '__len__'):
                        dataset_size = len(trainer.train_dataset)
                    elif hasattr(trainer, '_train_dataset') and hasattr(trainer._train_dataset, '__len__'):
                        dataset_size = len(trainer._train_dataset)
                    else:
                        dataset_size = 1000  # é»˜è®¤ä¼°ç®—

                    batch_size = getattr(trainer.args, 'per_device_train_batch_size', 16) if hasattr(trainer, 'args') else 16
                    gradient_accumulation_steps = getattr(trainer.args, 'gradient_accumulation_steps', 1) if hasattr(trainer, 'args') else 1
                    num_epochs = getattr(trainer.args, 'num_train_epochs', 3) if hasattr(trainer, 'args') else 3

                    # ä¿®å¤max_stepsè®¡ç®—ï¼šåŒ…å«GPUæ•°é‡ï¼ˆåŒ¹é…åŸtrain.pyé€»è¾‘ï¼‰
                    import torch
                    num_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 1
                    effective_batch_size = batch_size * gradient_accumulation_steps * num_gpus
                    steps_per_epoch = max(1, dataset_size // effective_batch_size)
                    max_steps = int(steps_per_epoch * num_epochs)

                    logger.info(f"åŸºç¡€è¿›åº¦è·Ÿè¸ª - è¯¦ç»†è®¡ç®—å‚æ•°: æ•°æ®é›†å¤§å°={dataset_size}, GPUæ•°é‡={num_gpus}, æ¯è®¾å¤‡æ‰¹æ¬¡={batch_size}, æ¢¯åº¦ç´¯ç§¯={gradient_accumulation_steps}")
                    logger.info(f"åŸºç¡€è¿›åº¦è·Ÿè¸ª - è®¡ç®—ç»“æœ: æœ‰æ•ˆæ‰¹æ¬¡å¤§å°={effective_batch_size}, æ¯è½®æ­¥æ•°={steps_per_epoch}, ä¼°ç®—æœ€å¤§æ­¥æ•°={max_steps}")
                except Exception as e:
                    logger.warning(f"ä¼°ç®—æœ€å¤§æ­¥æ•°å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼1000")
                    max_steps = 1000

            # åŒ…è£…trainerçš„training_stepæ–¹æ³•è¿›è¡Œè¿›åº¦è·Ÿè¸ª
            if hasattr(trainer, 'training_step'):
                original_training_step = trainer.training_step

                def wrapped_training_step(*args, **kwargs):
                    nonlocal step_count, last_reported_progress
                    result = original_training_step(*args, **kwargs)
                    step_count += 1

                    # å®ç°1%è¿›åº¦èŠ‚æµæ›´æ–°ç­–ç•¥ï¼ˆåŸºç¡€è¿›åº¦è·Ÿè¸ªï¼‰
                    try:
                        from bubble_rag.training.model_sft.services.task_manager import task_manager

                        # åŠ¨æ€è·å–trainer.state.max_stepsï¼ˆå¦‚æœå¯ç”¨ä¸”åˆç†ï¼‰
                        runtime_max_steps = None
                        if hasattr(trainer, 'state') and hasattr(trainer.state, 'max_steps') and trainer.state.max_steps > 0:
                            runtime_max_steps = trainer.state.max_steps

                        # ä¼˜å…ˆä½¿ç”¨è¿è¡Œæ—¶max_stepsï¼Œç„¶åæ˜¯ä¼°ç®—å€¼ï¼Œæœ€åæ˜¯é»˜è®¤å€¼
                        if runtime_max_steps and runtime_max_steps > 0:
                            effective_max_steps = runtime_max_steps
                        elif max_steps and max_steps > 0:
                            effective_max_steps = max_steps
                        else:
                            effective_max_steps = 1000

                        # è®¡ç®—å½“å‰è¿›åº¦ç™¾åˆ†æ¯”
                        current_progress = min(99.0, (step_count / effective_max_steps) * 100)

                        # åªæœ‰è¿›åº¦å¢åŠ 1%ä»¥ä¸Šæ—¶æ‰æ›´æ–°ï¼ˆèŠ‚æµç­–ç•¥ï¼‰
                        progress_change = current_progress - last_reported_progress
                        should_update = (
                            progress_change >= 1.0 or  # è¿›åº¦å˜åŒ–1%ä»¥ä¸Š
                            last_reported_progress == -1 or  # é¦–æ¬¡æ›´æ–°
                            step_count % 100 == 0  # æ¯100æ­¥å¼ºåˆ¶æ›´æ–°ä¸€æ¬¡ï¼ˆé˜²æ­¢é•¿æ—¶é—´æ— æ›´æ–°ï¼‰
                        )

                        if should_update:
                            task_manager.update_task_progress(task_id, current_progress, f"è®­ç»ƒä¸­: {step_count}/{effective_max_steps}")

                            # æ›´æ–°ä¸Šæ¬¡æŠ¥å‘Šçš„è¿›åº¦
                            last_reported_progress = current_progress

                            logger.info(f"åŸºç¡€è¿›åº¦è·Ÿè¸ª(1%èŠ‚æµ): {current_progress:.1f}% (æ­¥æ•°: {step_count}/{effective_max_steps}, å˜åŒ–: +{progress_change:.1f}%)")
                        elif step_count % 50 == 0:
                            # å®šæœŸæ˜¾ç¤ºå½“å‰è¿›åº¦ï¼ˆä½†ä¸æ›´æ–°æ•°æ®åº“ï¼‰
                            logger.debug(f"åŸºç¡€è¿›åº¦: {current_progress:.1f}% (æ­¥æ•°: {step_count}/{effective_max_steps}, æœªè¾¾1%æ›´æ–°é˜ˆå€¼)")
                    except Exception as tm_e:
                        if step_count % 100 == 0:  # å‡å°‘é”™è¯¯æ—¥å¿—é¢‘ç‡
                            logger.warning(f"åŸºç¡€è¿›åº¦è·Ÿè¸ªå¤±è´¥: {tm_e}")

                    return result

                trainer.training_step = wrapped_training_step
                logger.info("æˆåŠŸè®¾ç½®åŸºç¡€è¿›åº¦è·Ÿè¸ª")

        except Exception as e:
            logger.warning(f"è®¾ç½®åŸºç¡€è¿›åº¦è·Ÿè¸ªå¤±è´¥: {e}")

    def _is_mixed_eval_record(self, loss_metrics: dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯åŒ…å«å¤šä¸ªæ•°æ®æºçš„æ··åˆevalè®°å½•"""
        eval_keys = [key for key in loss_metrics.keys() if 'eval' in key.lower()]
        if len(eval_keys) < 2:
            return False

        # åŠ¨æ€æ£€æµ‹æ•°æ®æºï¼šåˆ©ç”¨ç°æœ‰çš„æ˜ å°„æ„å»ºå‡½æ•°
        source_ids, dataset_to_source_mapping = self._extract_data_sources_and_build_mapping(eval_keys)

        # å¦‚æœæ£€æµ‹åˆ°å¤šä¸ªæ•°æ®æºï¼Œå°±æ˜¯æ··åˆè®°å½•
        total_sources = len(source_ids) + len(dataset_to_source_mapping)
        return total_sources > 1

    def _extract_data_sources_and_build_mapping(self, eval_keys: list) -> tuple:
        """
        ä»evalæŒ‡æ ‡é”®ä¸­æå–æ•°æ®æºä¿¡æ¯å¹¶å»ºç«‹æ˜ å°„å…³ç³»

        Returns:
            tuple: (source_ids, dataset_to_source_mapping)
            - source_ids: set of source_ids
            - dataset_to_source_mapping: {dataset_name: source_id} çš„åŠ¨æ€æ˜ å°„
        """
        source_ids = set()
        dataset_to_source_mapping = {}

        # ç›´æ¥ä½¿ç”¨å·²å»ºç«‹çš„data_source_mappingï¼Œé¿å…é‡å¤è§£æå’Œç¡¬ç¼–ç 
        if hasattr(self, '_current_data_source_mapping'):
            # æ”¶é›†æ‰€æœ‰source_ids
            for source_id in self._current_data_source_mapping.values():
                source_ids.add(source_id)

            # å»ºç«‹æ•°æ®é›†åç§°åˆ°source_idçš„æ˜ å°„
            for full_name, source_id in self._current_data_source_mapping.items():
                dataset_to_source_mapping[full_name] = source_id
                # åŒæ—¶ä¸ºç®€åŒ–åç§°å»ºç«‹æ˜ å°„ï¼ˆå¦‚æœæ˜¯è·¯å¾„æ ¼å¼ï¼‰
                if '/' in full_name:
                    simple_name = full_name.split('/')[-1]
                    dataset_to_source_mapping[simple_name] = source_id

        return source_ids, dataset_to_source_mapping

    def _save_separated_eval_records(self, loss_manager, current_step: int, loss_metrics: dict, current_epoch):
        """å°†æ··åˆçš„evalè®°å½•æŒ‰æ•°æ®æºåˆ†ç¦»ä¿å­˜å¹¶ç»Ÿä¸€å‘½å"""
        logger.info(f"åˆ†ç¦»ä¿å­˜evalè®°å½•: step={current_step}, åŸå§‹æŒ‡æ ‡æ•°={len(loss_metrics)}")

        # è·å–æ‰€æœ‰evalé”®ï¼Œæå–æ•°æ®æºå’Œæ˜ å°„å…³ç³»
        eval_keys = [key for key in loss_metrics.keys() if 'eval' in key.lower()]
        source_ids, dataset_to_source_mapping = self._extract_data_sources_and_build_mapping(eval_keys)
        logger.info(f"æ£€æµ‹åˆ°æ•°æ®æº: {source_ids}")
        logger.info(f"ğŸ”— æ•°æ®é›†æ˜ å°„: {dataset_to_source_mapping}")

        # æŒ‰æ•°æ®æºåˆ†ç¦»æŒ‡æ ‡
        source_metrics = {}
        common_metrics = {}

        # åˆå§‹åŒ–æ¯ä¸ªæ•°æ®æºçš„æŒ‡æ ‡å­—å…¸
        for source_id in source_ids:
            source_metrics[source_id] = {}

        # åˆ†ç±»å’Œè½¬æ¢æŒ‡æ ‡
        for key, value in loss_metrics.items():
            assigned_to_source = False

            # æ£€æŸ¥æ ¼å¼1: eval_{source_id}_* (å·²ç»æ˜¯æ ‡å‡†æ ¼å¼)
            for source_id in source_ids:
                if key.startswith(f'eval_{source_id}_'):
                    source_metrics[source_id][key] = value
                    assigned_to_source = True
                    break

            # æ£€æŸ¥æ ¼å¼2: å…¶ä»–æ ¼å¼ï¼Œé€šè¿‡æ•°æ®é›†åç§°åŒ¹é…è½¬æ¢
            if not assigned_to_source:
                for dataset_name, source_id in dataset_to_source_mapping.items():
                    if dataset_name in key:
                        # è½¬æ¢æŒ‡æ ‡åç§°ä¸ºç»Ÿä¸€æ ¼å¼
                        metric_name = key.split('_')[-1]  # æå–æœ€åçš„æŒ‡æ ‡å(loss, runtimeç­‰)
                        new_key = f'eval_{source_id}_{metric_name}'
                        source_metrics[source_id][new_key] = value
                        assigned_to_source = True
                        logger.debug(f"è½¬æ¢æŒ‡æ ‡åç§°: {key} â†’ {new_key}")
                        break

            # å¦‚æœä¸å±äºä»»ä½•ç‰¹å®šæ•°æ®æºï¼Œåˆ™å½’ä¸ºé€šç”¨æŒ‡æ ‡
            if not assigned_to_source:
                # æ’é™¤å·²çŸ¥çš„æ•°æ®æºç‰¹å®šæŒ‡æ ‡ï¼Œä¿ç•™é€šç”¨æŒ‡æ ‡ï¼ˆå¦‚sequential_scoreç­‰ï¼‰
                is_source_specific = (
                    any(key.startswith(f'eval_{source_id}_') for source_id in source_ids) or
                    any(dataset_name in key for dataset_name in dataset_to_source_mapping.keys())
                )
                if not is_source_specific:
                    common_metrics[key] = value

        # ä¸ºæ¯ä¸ªæ•°æ®æºä¿å­˜è®°å½•ï¼Œæ”¯æŒåŒstepå†…çš„åˆå¹¶
        for source_id, metrics in source_metrics.items():
            if metrics:  # åªæœ‰å½“è¯¥æ•°æ®æºæœ‰æŒ‡æ ‡æ—¶æ‰ä¿å­˜
                source_record = {**metrics, **common_metrics}
                # ä½¿ç”¨step+source_idä½œä¸ºåˆå¹¶é”®
                merge_key = f"step_{current_step}_source_{source_id}"
                loss_manager.save_or_merge_loss_record(current_step, source_record, current_epoch, merge_key)
                logger.info(f"source_id {source_id} è®°å½•å·²ä¿å­˜/åˆå¹¶: {list(metrics.keys())}")

        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°æ•°æ®æºä½†æœ‰æŒ‡æ ‡ï¼Œä¿å­˜ä¸ºå•æ¡è®°å½•
        if not source_ids and loss_metrics:
            logger.warning(f"æœªæ£€æµ‹åˆ°æ•°æ®æºï¼Œä¿å­˜ä¸ºå•æ¡è®°å½•")
            data_source_mapping = getattr(self, '_current_data_source_mapping', {})
            loss_manager.save_loss_record(current_step, loss_metrics, current_epoch, True, data_source_mapping)


    def _perform_training_evaluation(self, trainer, current_step):
        """Perform evaluation during training (matching train.py behavior)."""
        try:
            # This matches the periodic evaluation logic from original train.py
            if hasattr(trainer, 'evaluate'):
                eval_results = trainer.evaluate()
                if eval_results:
                    logger.info(f"è®­ç»ƒä¸­è¯„ä¼° (Step {current_step}): {eval_results}")
        except Exception as e:
            logger.warning(f"è®­ç»ƒä¸­è¯„ä¼°å¤±è´¥: {e}")

    def _save_training_evaluation_results(self, task_id: str, eval_logs: dict, step: int, epoch: float, data_source_mapping: Dict[str, str] = None):
        """Save evaluation results during training to database."""
        try:
            from bubble_rag.training.mysql_service.service.training_dataset_service import TrainingDatasetService

            # ä½¿ç”¨evaluation_resultå·¥å…·ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ•°æ®åº“
            from bubble_rag.training.model_sft.utils.evaluation_result import save_evaluation_results_to_database
            save_evaluation_results_to_database(task_id, eval_logs, step, epoch, data_source_mapping)

        except Exception as e:
            logger.warning(f"ä¿å­˜è®­ç»ƒè¯„ä¼°ç»“æœå¤±è´¥: {e}")
    
    # Abstract methods that must be implemented by subclasses

    def _ensure_eval_directories(self, trainer):
        """ç¡®ä¿è¯„ä¼°å™¨æ‰€éœ€çš„ç›®å½•å­˜åœ¨"""
        import os

        # ç¡®ä¿ä¸»è¾“å‡ºç›®å½•å­˜åœ¨
        if not os.path.exists(trainer.args.output_dir):
            os.makedirs(trainer.args.output_dir, exist_ok=True)
            logger.info(f"åˆ›å»ºè¾“å‡ºç›®å½•: {trainer.args.output_dir}")

        # åˆå§‹åŒ–eval_dirå˜é‡
        eval_dir = None

        # ä½¿ç”¨é…ç½®ä¸­å®šä¹‰çš„user_eval_dirï¼Œè€Œä¸æ˜¯è¿è¡Œæ—¶åˆ›å»º
        try:
            # ä»é…ç½®ä¸­è·å–è‡ªå®šä¹‰å‚æ•°ï¼ŒåŒ…æ‹¬user_eval_dir
            param_manager = TrainingParametersManager()
            param_manager.load_from_config(self.raw_config)
            custom_params = param_manager.get_custom_params_dict()

            eval_dir = custom_params.get('user_eval_dir')
            if eval_dir:
                if not os.path.exists(eval_dir):
                    os.makedirs(eval_dir, exist_ok=True)
                    logger.info(f"åˆ›å»ºè¯„ä¼°è¾“å‡ºç›®å½•: {eval_dir}")

                logger.info(f"ä½¿ç”¨output_dir: {trainer.args.output_dir}")
                logger.info(f"ä½¿ç”¨user_eval_dir: {eval_dir}")
            else:
                logger.warning("æœªæ‰¾åˆ°user_eval_diré…ç½®ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„")
                eval_dir = os.path.join(trainer.args.output_dir, "eval")

        except Exception as e:
            logger.error(f"è·å–user_eval_dirå¤±è´¥: {e}")
            # å›é€€åˆ°åŸæ¥çš„æ–¹å¼
            eval_dir = os.path.join(trainer.args.output_dir, "eval")

        # ç¡®ä¿eval_dirå­˜åœ¨
        if eval_dir and not os.path.exists(eval_dir):
            os.makedirs(eval_dir, exist_ok=True)
            logger.info(f"åˆ›å»ºè¯„ä¼°è¾“å‡ºç›®å½•: {eval_dir}")

        # æ³¨ï¼šè¯„ä¼°å™¨çš„ __call__ æ–¹æ³•æœ‰ output_path å‚æ•°ï¼Œç›´æ¥æŒ‡å®šè¾“å‡ºç›®å½•å³å¯

        # # ä¸ºè¯„ä¼°å™¨é¢„åˆ›å»ºå¯èƒ½éœ€è¦çš„å­ç›®å½•ç»“æ„
        # # CrossEncoderCorrelationEvaluator ä¼šåˆ›å»ºä»¥è¯„ä¼°å™¨åç§°å‘½åçš„å­ç›®å½•
        # try:
        #     # è·å–æ¨¡å‹åç§°ï¼Œç”¨äºæ„å»ºè¯„ä¼°å™¨ç›®å½•å - éœ€è¦ä¸è¯„ä¼°å™¨å®é™…ä½¿ç”¨çš„åç§°ä¸€è‡´
        #     model_short_name = os.path.basename(trainer.model.config.name_or_path) if hasattr(trainer.model, 'config') and hasattr(trainer.model.config, 'name_or_path') else "model"
        #     logger.info(f"æ¨¡å‹åç§°è°ƒè¯•: {model_short_name}")

        #     # å¯èƒ½çš„è¯„ä¼°å™¨ç›®å½•åå˜ä½“ - è¦†ç›–ä¸åŒçš„å‘½åè§„åˆ™
        #     potential_eval_subdirs = [
        #         # åŸå§‹æ ¼å¼
        #         f"CrossEncoderCorrelationEvaluator_{model_short_name}",
        #         f"CrossEncoderClassificationEvaluator_{model_short_name}",
        #         # å¸¦rerankerå‰ç¼€çš„æ ¼å¼
        #         f"CrossEncoderCorrelationEvaluator_reranker-{model_short_name}",
        #         f"CrossEncoderClassificationEvaluator_reranker-{model_short_name}",
        #         # å¸¦sentence-transformersåç¼€çš„æ ¼å¼
        #         f"CrossEncoderCorrelationEvaluator_{model_short_name}-sentence-transformers",
        #         f"CrossEncoderCorrelationEvaluator_reranker-{model_short_name}-sentence-transformers",
        #         # ç‰¹æ®Šçš„evalæ ¼å¼ï¼ˆæ ¹æ®é”™è¯¯ä¿¡æ¯ï¼‰
        #         f"CrossEncoderCorrelationEvaluator_reranker-{model_short_name}-eval-sentence-transformers",
        #     ]

        #     for subdir in potential_eval_subdirs:
        #         eval_subdir_path = os.path.join(eval_dir, subdir)
        #         os.makedirs(eval_subdir_path, exist_ok=True)
        #         logger.info(f"é¢„åˆ›å»ºè¯„ä¼°å™¨ç›®å½•: {subdir}")
        #     logger.info(f"é¢„åˆ›å»ºè¯„ä¼°å™¨å­ç›®å½•å®Œæˆ")
        # except Exception as e:
        #     logger.warning(f"é¢„åˆ›å»ºè¯„ä¼°å™¨å­ç›®å½•å¤±è´¥ï¼Œä½†ç»§ç»­è®­ç»ƒ: {e}")

        logger.info(f"è¯„ä¼°ç›®å½•å‡†å¤‡å®Œæˆï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ï¼ˆè¯„ä¼°å™¨å°†é€šè¿‡output_pathå‚æ•°è‡ªè¡Œåˆ›å»ºæ–‡ä»¶ï¼‰")

    @abstractmethod
    def initialize_model(self, model_name: str) -> Any:
        """
        Initialize the model for this training type.
        
        Args:
            model_name: Name or path of the model
            
        Returns:
            Initialized model instance
        """
        pass
    
    @abstractmethod
    def create_loss_function(self, model: Any, train_dataset: Any) -> Any:
        """
        Create loss function for this training type.
        
        Args:
            model: The initialized model
            train_dataset: Training dataset
            
        Returns:
            Loss function instance
        """
        pass
    
    @abstractmethod
    def create_training_args(self, config: Dict[str, Any]) -> Any:
        """
        Create training arguments for this training type.
        
        Args:
            config: Training configuration dictionary
            
        Returns:
            Training arguments instance
        """
        pass
    
    @abstractmethod
    def create_trainer_instance(self, model, args, train_dataset, eval_dataset, loss, evaluator) -> Any:
        """
        Create trainer instance for this training type.
        
        Args:
            model: The initialized model
            args: Training arguments
            train_dataset: Training dataset
            eval_dataset: Evaluation dataset (optional)
            loss: Loss function
            evaluator: Evaluator (optional)
            
        Returns:
            Trainer instance
        """
        pass
    
    @abstractmethod
    def create_evaluator(self, eval_dataset: Any) -> Any:
        """
        Create evaluator for this training type.
        
        Args:
            eval_dataset: Evaluation dataset
            
        Returns:
            Evaluator instance
        """
        pass

    # Task Status Management Methods (matching original train.py)

    def _update_task_status_running(self, task_id: str):
        """Update task status to RUNNING (matching original train.py)."""
        if not task_id:
            logger.warning("task_idä¸ºç©ºï¼Œè·³è¿‡çŠ¶æ€æ›´æ–°")
            return

        logger.info(f"å¼€å§‹æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºRUNNING: {task_id}")

        try:
            from bubble_rag.training.mysql_service.service.training_task_service import training_task_service
            from ..enums import TrainingStatus

            # å¢å¼ºé”™è¯¯å¤„ç†ï¼šè®°å½•è¯¦ç»†çš„æ›´æ–°è¿‡ç¨‹
            logger.info(f"æ­£åœ¨è°ƒç”¨ training_task_service.update_task_status(task_id={task_id}, status={TrainingStatus.RUNNING.value}, progress=0.0)")

            success = training_task_service.update_task_status(task_id, TrainingStatus.RUNNING.value, progress=0.0)

            if success:
                logger.info(f"æ•°æ®åº“çŠ¶æ€æ›´æ–°æˆåŠŸ: RUNNING, è¿›åº¦=0.0%")
            else:
                logger.error(f"æ•°æ®åº“çŠ¶æ€æ›´æ–°å¤±è´¥: update_task_statusè¿”å›False")
                # å³ä½¿æ•°æ®åº“æ›´æ–°å¤±è´¥ï¼Œä¹Ÿè¦æ›´æ–°å†…å­˜çŠ¶æ€

            # åŒæ—¶æ›´æ–°task_managerä¸­çš„çŠ¶æ€å’Œè¿›åº¦ï¼Œç¡®ä¿å†…å­˜å’Œæ•°æ®åº“åŒæ­¥
            try:
                from ..services.task_manager import task_manager

                # å¼ºåˆ¶æ›´æ–°å†…å­˜ä¸­çš„ä»»åŠ¡çŠ¶æ€ä¸ºRUNNING
                task = task_manager.get_task(task_id)
                if task:
                    logger.info(f"æ›´æ–°å†…å­˜ä»»åŠ¡çŠ¶æ€: {task.status} -> RUNNING")
                    task.status = TrainingStatus.RUNNING.value
                    task.started_at = task.started_at or datetime.now()

                    # é‡ç½® _last_db_progress å±æ€§ï¼Œç¡®ä¿è¿›åº¦åŒæ­¥æ­£å¸¸å·¥ä½œ
                    if hasattr(task, '_last_db_progress'):
                        task._last_db_progress = -1
                        logger.info("å·²é‡ç½®ä»»åŠ¡çš„æ•°æ®åº“è¿›åº¦è·Ÿè¸ªçŠ¶æ€")

                    # å¼ºåˆ¶æ›´æ–°ä»»åŠ¡è¿›åº¦ä¸º0%ï¼Œç¡®ä¿æ•°æ®åº“åŒæ­¥
                    task_manager.update_task_progress(task_id, 0.0, "è®­ç»ƒå¼€å§‹", force_db_update=True)
                    logger.info("å†…å­˜çŠ¶æ€å’Œè¿›åº¦å·²å¼ºåˆ¶é‡ç½®: RUNNING, 0%")
                else:
                    logger.warning(f"åœ¨task_managerä¸­æœªæ‰¾åˆ°ä»»åŠ¡: {task_id}")

            except Exception as tm_e:
                logger.error(f"é‡ç½®task_managerçŠ¶æ€å¤±è´¥: {tm_e}")
                import traceback
                logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

            logger.info(f"è®­ç»ƒçŠ¶æ€æ›´æ–°å®Œæˆ: {task_id}")

        except Exception as e:
            logger.error(f"æ›´æ–°è®­ç»ƒçŠ¶æ€å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

            # å³ä½¿å‡ºç°å¼‚å¸¸ï¼Œä¹Ÿè¦å°è¯•æ›´æ–°å†…å­˜çŠ¶æ€ï¼Œç¡®ä¿è®­ç»ƒå¯ä»¥ç»§ç»­
            try:
                from ..services.task_manager import task_manager
                task = task_manager.get_task(task_id)
                if task:
                    task.status = TrainingStatus.RUNNING.value
                    logger.warning(f"å¼‚å¸¸æƒ…å†µä¸‹å¼ºåˆ¶æ›´æ–°å†…å­˜çŠ¶æ€ä¸ºRUNNING: {task_id}")
            except Exception as fallback_e:
                logger.error(f"å¼‚å¸¸æƒ…å†µä¸‹å†…å­˜çŠ¶æ€æ›´æ–°ä¹Ÿå¤±è´¥: {fallback_e}")

    def _update_task_status_post_training_evaluation(self, task_id: str):
        """è®­ç»ƒå®Œæˆåï¼Œè®¾ç½®PENDINGçŠ¶æ€è¿›è¡Œæœ€ç»ˆè¯„ä¼°ï¼Œè¿›åº¦100%"""
        if not task_id:
            return
        try:
            from bubble_rag.training.mysql_service.service.training_task_service import training_task_service
            from bubble_rag.training.model_sft.services.task_manager import task_manager
            from ..enums import TrainingStatus

            # æ›´æ–°ä»»åŠ¡ç®¡ç†å™¨ï¼šè¿›åº¦100%ï¼ŒçŠ¶æ€è¯´æ˜æ­£åœ¨æœ€ç»ˆè¯„ä¼°
            task_manager.update_task_progress(task_id, 100.0, "è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨è¿›è¡Œæœ€ç»ˆè¯„ä¼°...")

            # æ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸ºPENDINGï¼ˆè¡¨ç¤ºæ­£åœ¨è¿›è¡Œæœ€ç»ˆè¯„ä¼°ï¼‰
            training_task_service.update_task_status(task_id, TrainingStatus.PENDING.value)
            logger.info(f"è®­ç»ƒå®Œæˆï¼ŒçŠ¶æ€è®¾ä¸ºPENDINGè¿›è¡Œæœ€ç»ˆè¯„ä¼°: {task_id}, è¿›åº¦100%")

        except Exception as e:
            logger.warning(f"æ›´æ–°æœ€ç»ˆè¯„ä¼°çŠ¶æ€å¤±è´¥ï¼ˆä¸å½±å“è®­ç»ƒï¼‰: {e}")

    def _update_task_status_succeeded(self, task_id: str, save_dir: str):
        """Update task status to SUCCEEDED (matching original train.py)."""
        if not task_id:
            return

        try:
            from ..services.task_manager import task_manager
            from bubble_rag.training.mysql_service.service.training_task_service import training_task_service
            from ..enums import TrainingStatus
            from ..enums.training_task_enums import ProcessStatus

            # Update task manager (this sets status to SUCCEEDED and progress to 100%)
            task_manager.complete_task(task_id, save_dir)
            task_manager.update_task_progress(task_id, 100.0, "è®­ç»ƒå®Œæˆ")

            # Update database task status to SUCCEEDED (matching unified_training_service)
            training_task_service.update_task_status(task_id, TrainingStatus.SUCCEEDED.value)
            training_task_service.update_task_result(task_id, final_model_path=save_dir)
            logger.info(f"æ•°æ®åº“ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸ºSUCCEEDED: {task_id}")

            # ç«‹å³é‡Šæ”¾GPUèµ„æº - ç¡®ä¿ä¸æ•°æ®åº“çŠ¶æ€åŒæ­¥
            try:
                from ..utils.gpu_resource_manager import gpu_resource_manager
                success = gpu_resource_manager.release_gpus_for_task(task_id)
                if success:
                    logger.info(f"ä»»åŠ¡æˆåŠŸå®Œæˆï¼Œç«‹å³é‡Šæ”¾GPUèµ„æº: {task_id}")
                else:
                    logger.warning(f"å¸¸è§„GPUé‡Šæ”¾å¤±è´¥ï¼Œå°è¯•å¼ºåˆ¶é‡Šæ”¾")
                    gpu_resource_manager.force_release_gpu_for_task(task_id)
            except Exception as gpu_error:
                logger.critical(f"ä¸¥é‡é”™è¯¯ï¼šè®­ç»ƒæˆåŠŸåGPUèµ„æºé‡Šæ”¾å¤±è´¥ï¼å°è¯•å¼ºåˆ¶æ¢å¤ã€‚ä»»åŠ¡: {task_id}, é”™è¯¯: {gpu_error}")
                try:
                    gpu_resource_manager.force_release_gpu_for_task(task_id)
                    logger.warning(f"å¼ºåˆ¶GPUé‡Šæ”¾å·²æ‰§è¡Œ")
                except Exception as force_error:
                    logger.critical(f"å¼ºåˆ¶GPUé‡Šæ”¾ä¹Ÿå¤±è´¥ï¼ä»»åŠ¡ {task_id} èµ„æºå¯èƒ½æ°¸ä¹…æ³„æ¼: {force_error}")

            # Update process status to TERMINATED
            training_task_service.update_process_info(task_id, None, ProcessStatus.TERMINATED.value)

            logger.info(f"è®­ç»ƒçŠ¶æ€å·²æ›´æ–°ä¸ºå·²å®Œæˆï¼Œæ¨¡å‹ä¿å­˜åœ¨: {save_dir}")

            # å®Œæˆlossç®¡ç†å™¨çš„æœ€ç»ˆåŒ–å¤„ç†å¹¶ä¿å­˜æ±‡æ€»åˆ°æ•°æ®åº“ (matching original train.py)
            try:
                from bubble_rag.training.model_sft.utils.loss_manager import cleanup_loss_manager

                # è·å–æœ€ç»ˆè®­ç»ƒæŒ‡æ ‡
                final_metrics = {
                    "final_model_path": save_dir,
                    "training_completed": True
                }

                # æ¸…ç†lossç®¡ç†å™¨å¹¶è·å–æ•°æ®åº“æ±‡æ€»ä¿¡æ¯
                loss_summary = cleanup_loss_manager(task_id, final_metrics)

                if loss_summary:
                    # å°†lossæ±‡æ€»ä¿¡æ¯ä¿å­˜åˆ°æ•°æ®åº“
                    try:
                        import json
                        loss_data_json = json.dumps(loss_summary, ensure_ascii=False)
                        training_task_service.update_task_result(task_id, loss_data=loss_data_json)
                        logger.info(f"Lossæ±‡æ€»ä¿¡æ¯å·²ä¿å­˜åˆ°æ•°æ®åº“: {len(loss_summary)} é¡¹æŒ‡æ ‡")
                    except Exception as db_e:
                        logger.warning(f"ä¿å­˜lossæ±‡æ€»åˆ°æ•°æ®åº“å¤±è´¥: {db_e}")

                logger.info("Lossç®¡ç†å™¨å·²å®Œæˆæœ€ç»ˆåŒ–å¤„ç†")
            except Exception as loss_e:
                logger.warning(f"Lossç®¡ç†å™¨æ¸…ç†å¤±è´¥: {loss_e}")

        except Exception as e:
            logger.warning(f"æ›´æ–°è®­ç»ƒæˆåŠŸçŠ¶æ€å¤±è´¥: {e}")

    def _update_task_status_failed(self, task_id: str, error_msg: str):
        """Update task status to FAILED (matching original train.py)."""
        if not task_id:
            return

        try:
            from bubble_rag.training.mysql_service.service.training_task_service import training_task_service
            from ..enums import TrainingStatus
            from ..services.task_manager import task_manager

            # Update task manager
            task_manager.fail_task(task_id, error_msg, None)

            # Update database
            training_task_service.update_task_status(task_id, TrainingStatus.FAILED.value)
            training_task_service.update_task_result(task_id, error_message=error_msg)
            logger.info(f"å…¨å±€å¼‚å¸¸å¤„ç†ï¼šä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸ºFAILED: {task_id}")

            # ç«‹å³é‡Šæ”¾GPUèµ„æº - ç¡®ä¿ä¸æ•°æ®åº“çŠ¶æ€åŒæ­¥
            try:
                from ..utils.gpu_resource_manager import gpu_resource_manager
                success = gpu_resource_manager.release_gpus_for_task(task_id)
                if success:
                    logger.info(f"ä»»åŠ¡å¤±è´¥å®Œæˆï¼Œç«‹å³é‡Šæ”¾GPUèµ„æº: {task_id}")
                else:
                    logger.warning(f"å¸¸è§„GPUé‡Šæ”¾å¤±è´¥ï¼Œå°è¯•å¼ºåˆ¶é‡Šæ”¾")
                    gpu_resource_manager.force_release_gpu_for_task(task_id)
            except Exception as gpu_error:
                logger.critical(f"ä¸¥é‡é”™è¯¯ï¼šè®­ç»ƒå¤±è´¥æ—¶GPUèµ„æºé‡Šæ”¾å¤±è´¥ï¼å°è¯•å¼ºåˆ¶æ¢å¤ã€‚ä»»åŠ¡: {task_id}, é”™è¯¯: {gpu_error}")
                try:
                    gpu_resource_manager.force_release_gpu_for_task(task_id)
                    logger.warning(f"å¼ºåˆ¶GPUé‡Šæ”¾å·²æ‰§è¡Œ")
                except Exception as force_error:
                    logger.critical(f"å¼ºåˆ¶GPUé‡Šæ”¾ä¹Ÿå¤±è´¥ï¼ä»»åŠ¡ {task_id} èµ„æºå¯èƒ½æ°¸ä¹…æ³„æ¼: {force_error}")

        except Exception as status_e:
            logger.warning(f"å…¨å±€å¼‚å¸¸å¤„ç†ï¼šæ›´æ–°è®­ç»ƒå¤±è´¥çŠ¶æ€å¤±è´¥: {status_e}")

    def _save_training_metadata(self, trainer):
        """ä¿å­˜è®­ç»ƒå…ƒæ•°æ®åˆ°lossæ–‡ä»¶ï¼ŒåŒ…å«æ•°æ®æºæ˜ å°„ç­‰ä¿¡æ¯"""
        try:
            task_id = self.raw_config.get('task_id')
            if not task_id:
                logger.warning("ç¼ºå°‘task_idï¼Œè·³è¿‡å…ƒæ•°æ®ä¿å­˜")
                return

            output_dir = getattr(trainer.args, 'output_dir', './output') if hasattr(trainer, 'args') else './output'

            from bubble_rag.training.model_sft.utils.loss_manager import get_loss_manager
            loss_manager = get_loss_manager(output_dir, task_id)

            # æ„å»ºå…ƒæ•°æ®
            metadata = {
                "train_type": self.train_type,
                "model_config": self.model_config,
                "data_config": self.data_config,
                "data_source_mapping": getattr(self, '_current_data_source_mapping', {}),
            }

            # ä¿å­˜å…ƒæ•°æ®
            loss_manager.save_metadata(metadata)
            logger.info(f"è®­ç»ƒå…ƒæ•°æ®å·²ä¿å­˜ï¼ŒåŒ…å«æ•°æ®æºæ˜ å°„: {metadata.get('data_source_mapping', {})}")

        except Exception as e:
            logger.warning(f"ä¿å­˜è®­ç»ƒå…ƒæ•°æ®å¤±è´¥: {e}")