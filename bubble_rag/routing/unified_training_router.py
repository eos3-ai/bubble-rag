"""
ç»Ÿä¸€è®­ç»ƒAPIè·¯ç”±
æ”¯æŒä¸²è¡Œ(serial)å’Œå¹¶è¡Œ(parallel)è®­ç»ƒæ¨¡å¼çš„ç»Ÿä¸€æ¥å£
"""
from fastapi import APIRouter, Query
from pydantic import BaseModel, Field, ValidationError, ConfigDict
from typing import Optional, Dict, Any, List
from datetime import datetime, timedelta
import json
import uuid

from bubble_rag.entity.query.response_model import SrvResult
from bubble_rag.training.model_sft.services.unified_training_service import unified_training_service
from bubble_rag.training.model_sft.services.dataset_service import dataset_service
from bubble_rag.training.model_sft.services.model_service import model_service
from bubble_rag.training.model_sft.services.config_service import config_service
from bubble_rag.training.model_sft.models.training_task import TrainingTaskCreateRequest
from bubble_rag.training.model_sft.models.unified_config import UnifiedTrainingConfig
from bubble_rag.training.mysql_service.service.training_task_service import training_task_service
from bubble_rag.training.mysql_service.service.training_dataset_service import TrainingDatasetService
from bubble_rag.training.model_sft.enums import TrainingStatus
from bubble_rag.training.model_sft.enums.training_task_enums import ProcessStatus
from bubble_rag.training.model_sft.utils.error_handler import handle_api_error
from bubble_rag.training.model_sft.utils.gpu_resource_manager import gpu_resource_manager
from loguru import logger

# æƒé™æ£€æŸ¥åŠ©æ‰‹å‡½æ•°
def can_access_task(task_username: str, current_user: dict) -> bool:
    """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å¯ä»¥è®¿é—®æŒ‡å®šä»»åŠ¡"""
    # ç®¡ç†å‘˜å¯ä»¥è®¿é—®æ‰€æœ‰ä»»åŠ¡
    if current_user.get('is_admin', False):
        return True
    # æ™®é€šç”¨æˆ·åªèƒ½è®¿é—®è‡ªå·±çš„ä»»åŠ¡
    return task_username == current_user.get('username')

def is_admin_user(current_user: dict) -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºç®¡ç†å‘˜ç”¨æˆ·"""
    return current_user.get('is_admin', False)

router = APIRouter()

def validate_task_access(task_id: str, username: Optional[str] = None) -> tuple:
    """
    ç»Ÿä¸€çš„ä»»åŠ¡è®¿é—®éªŒè¯å‡½æ•°

    Args:
        task_id: ä»»åŠ¡ID
        username: ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰

    Returns:
        tuple: (current_user, task_db)

    Raises:
        HTTPException: å½“ä»»åŠ¡ä¸å­˜åœ¨æˆ–æƒé™ä¸è¶³æ—¶
    """
    from bubble_rag.utils.user_manager import validate_user
    from fastapi import HTTPException

    # éªŒè¯ç”¨æˆ·èº«ä»½
    current_user = validate_user(username)

    # è·å–ä»»åŠ¡ä¿¡æ¯
    task_db = training_task_service.get_training_task(task_id)
    if not task_db:
        raise HTTPException(status_code=404, detail=f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")

    # æƒé™æ£€æŸ¥ï¼šç®¡ç†å‘˜å¯ä»¥è®¿é—®æ‰€æœ‰ä»»åŠ¡ï¼Œæ™®é€šç”¨æˆ·åªèƒ½è®¿é—®è‡ªå·±çš„ä»»åŠ¡
    if (current_user['user_role'] != 'admin' and
        task_db.username != current_user['username']):
        raise HTTPException(
            status_code=403,
            detail=f"æƒé™ä¸è¶³ï¼šæ— æ³•è®¿é—®å…¶ä»–ç”¨æˆ·çš„ä»»åŠ¡ï¼ˆä»»åŠ¡å±äºç”¨æˆ·: {task_db.username}ï¼‰"
        )

    return current_user, task_db

class StartTrainingRequest(UnifiedTrainingConfig):
    """å¯åŠ¨è®­ç»ƒè¯·æ±‚æ¨¡å‹ï¼ˆç»§æ‰¿ç»Ÿä¸€é…ç½®ï¼‰"""
    training_mode: Optional[str] = Field(default="parallel", description="è®­ç»ƒæ¨¡å¼: serial(ä¸²è¡Œ) æˆ– parallel(å¹¶è¡Œ)")
    base_task_id: Optional[str] = Field(default=None, description="é‡å¯æºä»»åŠ¡IDï¼ˆå¯é€‰ï¼Œç”¨äºè®°å½•é‡å¯å…³ç³»ï¼‰")
    username: Optional[str] = Field(default=None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")


class StopTrainingRequest(BaseModel):
    """åœæ­¢è®­ç»ƒè¯·æ±‚æ¨¡å‹"""
    task_id: str = Field(description="è¦åœæ­¢çš„ä»»åŠ¡ID")
    username: Optional[str] = Field(default=None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")


# å…¬å…±æ•°æ®è½¬æ¢å‡½æ•°
def convert_task_to_dict(task) -> Dict[str, Any]:
    """å°†è®­ç»ƒä»»åŠ¡å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
    task_data = {
        "task_id": task.task_id,
        "task_name": task.task_name,
        "description": task.description,
        "train_type": task.train_type,
        "model_name_or_path": task.model_name_or_path,  # åŸºç¡€æ¨¡å‹è·¯å¾„
        "dataset_name_or_path": task.dataset_name_or_path,
        "HF_subset": getattr(task, 'HF_subset', None),  # HuggingFaceå­é›†
        "output_dir": task.output_dir,
        "device": task.device,
        "embedding_dim": getattr(task, 'embedding_dim', None),  # æ¨¡å‹ç»´åº¦
        "status": task.status,
        "progress": task.progress,
        "username": getattr(task, 'username', None),
        "created_at": task.created_at.isoformat() if task.created_at else None,
        "updated_at": task.updated_at.isoformat() if task.updated_at else None,  # æ›´æ–°æ—¶é—´
        "started_at": task.started_at.isoformat() if task.started_at else None,
        "completed_at": task.completed_at.isoformat() if task.completed_at else None,
        "duration_seconds": _calculate_duration_seconds(task),
        "final_model_path": task.final_model_path,
        "error_message": task.error_message,
        "loss_data": getattr(task, 'loss_data', None),  # è®­ç»ƒæŸå¤±æ•°æ®
        # è®­ç»ƒå‚æ•°ï¼ˆå…¼å®¹å†…å­˜å¯¹è±¡å’Œæ•°æ®åº“å¯¹è±¡ï¼‰
        "training_params": _parse_training_params(task.training_params),
        # è¿›ç¨‹ç®¡ç†å­—æ®µ
        "process_pid": getattr(task, 'process_pid', None),
        "process_status": getattr(task, 'process_status', None),
        # æœåŠ¡ç®¡ç†å­—æ®µ
        "service_instance_id": getattr(task, 'service_instance_id', None),
        "service_startup_time": getattr(task, 'service_startup_time', None),
        # é‡å¯å…³ç³»å­—æ®µ
        "base_task_id": getattr(task, 'base_task_id', None),
        "restart_count": getattr(task, 'restart_count', 0)
    }
    return task_data

def _calculate_duration_seconds(task) -> Optional[float]:
    """è®¡ç®—è®­ç»ƒæ—¶é•¿ï¼ˆç§’ï¼‰- é€šç”¨åŠ¨æ€è®¡ç®—å‡½æ•°"""
    if not task.started_at:
        return None

    if task.completed_at:
        # å·²å®Œæˆçš„ä»»åŠ¡ï¼šä½¿ç”¨å®Œæˆæ—¶é—´ - å¼€å§‹æ—¶é—´
        return (task.completed_at - task.started_at).total_seconds()
    else:
        # æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ï¼šä½¿ç”¨å½“å‰æ—¶é—´ - å¼€å§‹æ—¶é—´
        return (datetime.now() - task.started_at).total_seconds()

def _calculate_duration_from_dict(task_dict: Dict[str, Any]) -> Optional[float]:
    """ä»å­—å…¸æ ¼å¼çš„ä»»åŠ¡è®¡ç®—è®­ç»ƒæ—¶é•¿ï¼ˆç§’ï¼‰"""
    started_at_str = task_dict.get("started_at")
    if not started_at_str:
        return None

    try:
        # å°†ISOæ ¼å¼å­—ç¬¦ä¸²è½¬æ¢ä¸ºdatetimeå¯¹è±¡
        if isinstance(started_at_str, str):
            started_at = datetime.fromisoformat(started_at_str.replace('Z', '+00:00'))
        else:
            started_at = started_at_str

        completed_at_str = task_dict.get("completed_at")
        if completed_at_str:
            if isinstance(completed_at_str, str):
                completed_at = datetime.fromisoformat(completed_at_str.replace('Z', '+00:00'))
            else:
                completed_at = completed_at_str
            # å·²å®Œæˆçš„ä»»åŠ¡ï¼šä½¿ç”¨å®Œæˆæ—¶é—´ - å¼€å§‹æ—¶é—´
            return (completed_at - started_at).total_seconds()
        else:
            # æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ï¼šä½¿ç”¨å½“å‰æ—¶é—´ - å¼€å§‹æ—¶é—´
            return (datetime.now() - started_at).total_seconds()
    except Exception as e:
        logger.warning(f"è®¡ç®—ä»»åŠ¡æ—¶é•¿å¤±è´¥: {e}")
        return None

def _get_task_duration_fields(task_db) -> Dict[str, Any]:
    """è·å–ä»»åŠ¡è®­ç»ƒæ—¶é•¿å­—æ®µ - ç®€å•åŠ¨æ€è®¡ç®—"""
    duration_seconds = _calculate_duration_seconds(task_db)
    duration_formatted = _format_duration(duration_seconds)

    return {
        "training_duration_seconds": duration_seconds,
        "duration_formatted": duration_formatted
    }

def _format_duration(seconds: Optional[float]) -> Optional[str]:
    """å°†ç§’æ•°è½¬æ¢ä¸ºäººç±»å¯è¯»æ ¼å¼"""
    if seconds is None:
        return None

    total_seconds = int(seconds)
    days = total_seconds // 86400
    hours = (total_seconds % 86400) // 3600
    minutes = (total_seconds % 3600) // 60
    secs = total_seconds % 60

    if days > 0:
        return f"{days}å¤©{hours}æ—¶{minutes}åˆ†{secs}ç§’"
    elif hours > 0:
        return f"{hours}æ—¶{minutes}åˆ†{secs}ç§’"
    elif minutes > 0:
        return f"{minutes}åˆ†{secs}ç§’"
    else:
        return f"{secs}ç§’"

def _check_process_running(pid: int) -> bool:
    """è·¨æœåŠ¡æ£€æŸ¥æŒ‡å®šPIDçš„è¿›ç¨‹æ˜¯å¦æ­£åœ¨è¿è¡Œ"""
    try:
        import psutil
        return psutil.pid_exists(pid)
    except ImportError:
        # å¦‚æœæ²¡æœ‰psutilï¼Œä½¿ç”¨ç³»ç»Ÿè°ƒç”¨
        try:
            import os
            import signal
            os.kill(pid, 0)  # å‘é€ä¿¡å·0æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜åœ¨
            return True
        except (OSError, ProcessLookupError):
            return False
    except Exception as e:
        logger.warning(f"æ£€æŸ¥è¿›ç¨‹ {pid} çŠ¶æ€å¤±è´¥: {e}")
        return False

def _parse_training_params(training_params) -> Dict[str, Any]:
    """
    è§£æè®­ç»ƒå‚æ•°ï¼Œå…¼å®¹ä¸åŒçš„è¾“å…¥æ ¼å¼

    Args:
        training_params: è®­ç»ƒå‚æ•°ï¼Œå¯èƒ½æ˜¯å­—å…¸ã€JSONå­—ç¬¦ä¸²æˆ–None

    Returns:
        Dict[str, Any]: è§£æåçš„å‚æ•°å­—å…¸
    """
    if not training_params:
        return {}

    if isinstance(training_params, dict):
        # å†…å­˜ä¸­çš„TrainingTaskå¯¹è±¡ï¼Œtraining_paramså·²ç»æ˜¯å­—å…¸
        return training_params

    if isinstance(training_params, str):
        # æ•°æ®åº“çš„TrainingTaskDBå¯¹è±¡ï¼Œtraining_paramsæ˜¯JSONå­—ç¬¦ä¸²
        try:
            return json.loads(training_params)
        except (json.JSONDecodeError, TypeError) as e:
            logger.warning(f"è§£æè®­ç»ƒå‚æ•°JSONå¤±è´¥: {e}")
            return {}

    # å…¶ä»–æƒ…å†µï¼Œè¿”å›ç©ºå­—å…¸
    logger.warning(f"æœªçŸ¥çš„è®­ç»ƒå‚æ•°æ ¼å¼: {type(training_params)}")
    return {}

def _calculate_estimated_completion_time_from_dict(task_dict: Dict[str, Any]) -> Optional[str]:
    """ä»å­—å…¸æ ¼å¼çš„ä»»åŠ¡è®¡ç®—é¢„ä¼°å®Œæˆæ—¶é—´"""
    started_at_str = task_dict.get("started_at")
    progress = task_dict.get("progress", 0)
    status = task_dict.get("status")

    # å¯¹äºå·²å®Œæˆçš„ä»»åŠ¡ï¼Œè¿”å›ç‰¹æ®Šæ ‡è¯†
    if status in ["SUCCEEDED", "FAILED", "STOPPED"]:
        if status == "SUCCEEDED":
            return "å·²å®Œæˆ"
        elif status == "FAILED":
            return "å·²å¤±è´¥"
        else:
            return "å·²åœæ­¢"

    # åªå¯¹æœ‰å¼€å§‹æ—¶é—´ä¸”è¿›åº¦åœ¨åˆç†èŒƒå›´å†…çš„è¿è¡Œä¸­ä»»åŠ¡è®¡ç®—å‰©ä½™æ—¶é—´
    if not started_at_str or progress <= 0 or progress >= 100:
        return None

    try:
        # å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºdatetimeå¯¹è±¡
        if isinstance(started_at_str, datetime):
            # å¦‚æœå·²ç»æ˜¯datetimeå¯¹è±¡ï¼Œç›´æ¥ä½¿ç”¨
            started_at = started_at_str
        elif isinstance(started_at_str, str):
            try:
                # å°è¯•æ ‡å‡†ISOæ ¼å¼
                started_at = datetime.fromisoformat(started_at_str.replace('Z', '+00:00'))
            except ValueError:
                # å°è¯•å…¶ä»–å¸¸è§æ ¼å¼
                try:
                    started_at = datetime.strptime(started_at_str, '%Y-%m-%d %H:%M:%S')
                except ValueError:
                    logger.warning(f"æ— æ³•è§£ææ—¶é—´æ ¼å¼: {started_at_str}")
                    return None
        else:
            logger.warning(f"started_atç±»å‹ä¸æ”¯æŒ: {type(started_at_str)}")
            return None

        # è®¡ç®—å·²è¿è¡Œæ—¶é—´
        elapsed = (datetime.now() - started_at).total_seconds()

        # æ ¹æ®è¿›åº¦ä¼°ç®—æ€»æ—¶é—´å’Œå‰©ä½™æ—¶é—´
        estimated_total = elapsed / (progress / 100)
        estimated_remaining = max(0, estimated_total - elapsed)

        # è®¡ç®—é¢„ä¼°å®Œæˆæ—¶é—´
        estimated_completion_time = datetime.now() + timedelta(seconds=estimated_remaining)

        # æ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„æ—¶é—´å­—ç¬¦ä¸²
        return estimated_completion_time.strftime("%Y-%m-%d %H:%M:%S")
    except Exception as e:
        logger.warning(f"è®¡ç®—é¢„ä¼°å®Œæˆæ—¶é—´å¤±è´¥: task_id={task_dict.get('task_id')}, error={e}")
        return None


@router.post("/start_training")
def start_training(request: StartTrainingRequest):
    """
    å¯åŠ¨è®­ç»ƒä»»åŠ¡ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
    
    æ”¯æŒä¸¤ç§è®­ç»ƒæ¨¡å¼ï¼š
    - serial: ä¸²è¡Œè®­ç»ƒï¼Œä¸€æ¬¡åªèƒ½è¿è¡Œä¸€ä¸ªä»»åŠ¡
    - parallel: å¹¶è¡Œè®­ç»ƒï¼Œå¯åŒæ—¶è¿è¡Œå¤šä¸ªä»»åŠ¡
    """
    try:
        # ğŸ” éªŒè¯å¹¶è·å–ç”¨æˆ·ä¿¡æ¯
        from bubble_rag.utils.user_manager import validate_user
        current_user = validate_user(request.username)
        logger.info(f"ç”¨æˆ· {current_user['username']} (è§’è‰²: {current_user['user_role']}) è¯·æ±‚å¯åŠ¨è®­ç»ƒä»»åŠ¡")

        # æ£€æŸ¥æœåŠ¡å®ä¾‹ID - ç¡®ä¿æœåŠ¡éš”ç¦»åŠŸèƒ½æ­£å¸¸
        if not unified_training_service.service_instance_id:
            return SrvResult(
                code=500,
                msg="âŒ æœåŠ¡éš”ç¦»åŠŸèƒ½å¼‚å¸¸ï¼šæœåŠ¡å®ä¾‹IDä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºè®­ç»ƒä»»åŠ¡ï¼"
            )
        # æå–è®­ç»ƒæ¨¡å¼
        training_mode = request.training_mode or "parallel"
        
        # åˆ†ç¦»è®­ç»ƒæ¨¡å¼å’Œå…¶ä»–å‚æ•°ï¼Œæ’é™¤ç”¨æˆ·èº«ä»½å­—æ®µå’Œæ§åˆ¶å­—æ®µ
        request_data = request.model_dump(exclude={"training_mode", "username", "base_task_id"}, exclude_none=True)

        # åˆ†ç¦»æ ¸å¿ƒä»»åŠ¡å‚æ•°å’Œè®­ç»ƒå‚æ•°
        core_task_fields = {
            "task_name", "description", "train_type", "model_name_or_path",
            "dataset_name_or_path", "HF_subset", "output_dir", "device"
        }

        # æ’é™¤çš„å­—æ®µï¼šæ ¸å¿ƒä»»åŠ¡å‚æ•° + è·¯ç”±æ§åˆ¶å‚æ•° + ç”¨æˆ·èº«ä»½å­—æ®µ
        excluded_fields = core_task_fields | {"training_mode", "username", "base_task_id"}

        core_params = {k: v for k, v in request_data.items() if k in core_task_fields}
        training_params = {k: v for k, v in request_data.items() if k not in excluded_fields}
        
        # åˆå¹¶ç”¨æˆ·æä¾›çš„training_params
        if request.training_params:
            training_params.update(request.training_params)
        
        # ç›´æ¥ä½¿ç”¨Pydanticçš„TrainingParametersè¿›è¡ŒéªŒè¯
        try:
            from bubble_rag.training.model_sft.models.training_parameters import TrainingParameters
            validated_training_params = TrainingParameters(**training_params)
            training_params_dict = validated_training_params.model_dump(exclude_none=True)
            
            logger.info(f"è®­ç»ƒå‚æ•°éªŒè¯æˆåŠŸï¼Œå…± {len(training_params_dict)} ä¸ªå‚æ•°")
            
        except Exception as e:
            logger.error(f"è®­ç»ƒå‚æ•°éªŒè¯å¤±è´¥: {e}")
            # å¦‚æœæ˜¯ Pydantic éªŒè¯é”™è¯¯ï¼Œæä¾›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
            if hasattr(e, 'errors'):
                error_details = []
                for error in e.errors():
                    field = '.'.join(str(loc) for loc in error['loc'])
                    error_details.append(f"{field}: {error['msg']}")
                error_msg = "å‚æ•°éªŒè¯å¤±è´¥: " + "; ".join(error_details)
            else:
                error_msg = f"è®­ç»ƒå‚æ•°éªŒè¯å¤±è´¥: {e}"
            
            return SrvResult(code=400, msg=error_msg)
        
        # åˆ›å»ºè®­ç»ƒä»»åŠ¡è¯·æ±‚
        training_request = TrainingTaskCreateRequest(
            task_name=core_params.get("task_name"),
            description=core_params.get("description"),
            train_type=core_params["train_type"],
            model_name_or_path=core_params["model_name_or_path"],
            dataset_name_or_path=core_params["dataset_name_or_path"],
            HF_subset=core_params.get("HF_subset"),
            output_dir=core_params.get("output_dir"),
            device=core_params.get("device", "auto"),
            training_params=training_params_dict
        )
        
        logger.info(f"å¯åŠ¨è®­ç»ƒä»»åŠ¡è¯·æ±‚ï¼Œæ¨¡å¼: {training_mode}")
        logger.info(f"æ ¸å¿ƒå‚æ•°: model={training_request.model_name_or_path}, dataset={training_request.dataset_name_or_path}, train_type={training_request.train_type}")
        logger.info(f"è®­ç»ƒè¶…å‚æ•° ({len(training_request.training_params)}ä¸ª): {list(training_request.training_params.keys())}")

        # å¤„ç†é‡å¯å…³ç³»å’Œæ™ºèƒ½å‘½å
        base_task = None
        if request.base_task_id:
            logger.info(f"æ£€æµ‹åˆ°é‡å¯å…³ç³»ï¼Œæºä»»åŠ¡: {request.base_task_id}")
            # éªŒè¯æºä»»åŠ¡å­˜åœ¨å’Œæƒé™
            base_task = training_task_service.get_training_task(request.base_task_id)
            if base_task:
                if can_access_task(base_task.username, current_user):
                    logger.info(f"é‡å¯å…³ç³»éªŒè¯é€šè¿‡ï¼Œå°†è®°å½•é‡å¯å…³ç³»")
                else:
                    logger.warning(f"ç”¨æˆ· {current_user['username']} æ— æƒè®¿é—®æºä»»åŠ¡ {request.base_task_id}")
                    request.base_task_id = None  # æ¸…é™¤æ— æƒé™çš„é‡å¯å…³ç³»
                    base_task = None
            else:
                logger.warning(f"æºä»»åŠ¡ {request.base_task_id} ä¸å­˜åœ¨ï¼Œæ¸…é™¤é‡å¯å…³ç³»")
                request.base_task_id = None
                base_task = None

        # æ™ºèƒ½ç”Ÿæˆä»»åŠ¡åç§°
        if not request.task_name or not request.task_name.strip():
            if base_task:
                # é‡å¯ä»»åŠ¡å‘½åï¼šæºä»»åŠ¡å + é‡å¯æ¬¡æ•°
                restart_count = getattr(base_task, 'restart_count', 0) + 1
                generated_name = f"{base_task.task_name}_restart_{restart_count}"
                logger.info(f"ä¸ºé‡å¯ä»»åŠ¡ç”Ÿæˆåç§°: {generated_name}")
            else:
                # æ™®é€šä»»åŠ¡å‘½åï¼šä½¿ç”¨é»˜è®¤é€»è¾‘
                from datetime import datetime
                timestamp = datetime.now().strftime("%m%d_%H%M%S")
                generated_name = f"training_task_{timestamp}"
                logger.info(f"ä¸ºæ–°ä»»åŠ¡ç”Ÿæˆåç§°: {generated_name}")

            # æ›´æ–°è®­ç»ƒè¯·æ±‚ä¸­çš„ä»»åŠ¡åç§°
            training_request.task_name = generated_name

        # ä½¿ç”¨ç»Ÿä¸€è®­ç»ƒæœåŠ¡å¯åŠ¨ä»»åŠ¡
        task = unified_training_service.start_training(training_request, training_mode=training_mode)

        # è®¾ç½®é‡å¯å…³ç³»ä¿¡æ¯
        if request.base_task_id:
            task.base_task_id = request.base_task_id
            # æ›´æ–°ä»»åŠ¡æè¿°ï¼Œæ·»åŠ é‡å¯æ ‡è®°
            if task.description:
                task.description = f"{task.description} | ğŸ”„ é‡å¯è‡ªä»»åŠ¡ {request.base_task_id}"
            else:
                task.description = f"ğŸ”„ é‡å¯è‡ªä»»åŠ¡ {request.base_task_id}"
            logger.info(f"å·²è®¾ç½®é‡å¯å…³ç³»: {task.task_id} -> é‡å¯è‡ª {request.base_task_id}")

        # ğŸ” ä»»åŠ¡åˆ›å»ºåç«‹å³æ›´æ–°ç”¨æˆ·ä¿¡æ¯åˆ°æ•°æ®åº“
        try:
            training_task_service.save_training_task(
                task,
                task.training_params,
                service_instance_id=unified_training_service.service_instance_id,
                username=current_user['username']
            )
            logger.info(f"ä»»åŠ¡ {task.task_id} ç”¨æˆ·ä¿¡æ¯å·²ä¿å­˜: {current_user['username']} ({current_user['user_role']})")

            # æ›´æ–°æºä»»åŠ¡çš„é‡å¯è®¡æ•°
            if request.base_task_id:
                try:
                    base_task = training_task_service.get_training_task(request.base_task_id)
                    if base_task:
                        base_task.restart_count = getattr(base_task, 'restart_count', 0) + 1
                        training_task_service.save_training_task(base_task)
                        logger.info(f"æºä»»åŠ¡ {request.base_task_id} é‡å¯è®¡æ•°å·²æ›´æ–°: {base_task.restart_count}")
                except Exception as restart_count_error:
                    logger.warning(f"æ›´æ–°æºä»»åŠ¡é‡å¯è®¡æ•°å¤±è´¥: {str(restart_count_error)}")

        except Exception as user_save_error:
            logger.warning(f"ä¿å­˜ä»»åŠ¡ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {str(user_save_error)}")
            # ä¸å½±å“ä»»åŠ¡åˆ›å»ºæµç¨‹ï¼Œç»§ç»­æ‰§è¡Œ
        
        # æ„å»ºå“åº”æ•°æ®
        response_data = convert_task_to_dict(task)
        response_data["training_mode"] = training_mode
        
        return SrvResult(
            code=200,
            msg=f"è®­ç»ƒä»»åŠ¡å¯åŠ¨æˆåŠŸ (æ¨¡å¼: {training_mode})",
            data=response_data
        )
        
    except ValidationError as ve:
        logger.error(f"è¯·æ±‚å‚æ•°éªŒè¯å¤±è´¥: {ve}")
        error_details = []
        for error in ve.errors():
            field = '.'.join(str(loc) for loc in error['loc']) 
            error_details.append(f"{field}: {error['msg']}")
        return SrvResult(
            code=422,
            msg=f"è¯·æ±‚å‚æ•°éªŒè¯å¤±è´¥: {'; '.join(error_details)}"
        )
    except Exception as e:
        logger.error(f"å¯åŠ¨è®­ç»ƒä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "start_training")  # è®°å½•é”™è¯¯åˆ°æ—¥å¿—
        return SrvResult(
            code=500,
            msg=f"å¯åŠ¨è®­ç»ƒä»»åŠ¡å¤±è´¥: {str(e)}"
        )

@router.post("/stop_training")
def stop_training(request: StopTrainingRequest):
    """åœæ­¢è®­ç»ƒä»»åŠ¡"""
    try:
        # ğŸ” éªŒè¯ä»»åŠ¡è®¿é—®æƒé™
        current_user, task_db = validate_task_access(request.task_id, request.username)

        success = unified_training_service.stop_training(request.task_id)

        if success:
            return SrvResult(
                code=200,
                msg="è®­ç»ƒä»»åŠ¡å·²åœæ­¢",
                data={"task_id": request.task_id, "stopped": True, "user": current_user['username']}
            )
        else:
            return SrvResult(code=500, msg=f"åœæ­¢è®­ç»ƒä»»åŠ¡å¤±è´¥: {request.task_id}")
            
    except Exception as e:
        logger.error(f"åœæ­¢è®­ç»ƒä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "stop_training")
        return SrvResult(code=500, msg=f"åœæ­¢è®­ç»ƒä»»åŠ¡å¤±è´¥: {str(e)}")

@router.get("/tasks/{task_id}")
def get_task_detail(
    task_id: str,
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """è·å–ä»»åŠ¡è¯¦æƒ…ï¼ˆä»æ•°æ®åº“è·å–å®Œæ•´ä¿¡æ¯ï¼ŒåŒ…å«æŒä¹…åŒ–çš„è¿›åº¦çŠ¶æ€ï¼‰"""
    try:
        # ğŸ” éªŒè¯ä»»åŠ¡è®¿é—®æƒé™
        current_user, task_db = validate_task_access(task_id, username)
        
        # ğŸŒ è·¨æœåŠ¡è·å–è¿è¡Œè¿›ç¨‹ä¿¡æ¯ï¼ˆå®æ—¶çŠ¶æ€ï¼‰
        # é€šè¿‡æ•°æ®åº“PIDæ£€æŸ¥å®é™…è¿è¡ŒçŠ¶æ€ï¼Œè€Œä¸æ˜¯ä¾èµ–æœ¬åœ°æœåŠ¡å®ä¾‹
        is_running = False
        process_info = None
        if task_db.process_pid:
            is_running = _check_process_running(task_db.process_pid)
            if is_running:
                process_info = {
                    'pid': task_db.process_pid,
                    'status': task_db.process_status,
                    'cross_service': True
                }
        
        # ğŸ”§ ä¿®å¤è¿›åº¦æ˜¾ç¤ºé—®é¢˜ï¼šä½¿ç”¨ä¸progressæ¥å£ç›¸åŒçš„æ··åˆæ•°æ®æºç­–ç•¥
        # ç¡®ä¿ä¸¤ä¸ªæ¥å£è¿”å›ä¸€è‡´çš„è¿›åº¦æ•°æ®
        real_time_progress = task_db.progress or 0.0  # ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“è¿›åº¦ï¼ˆå¯é ï¼‰
        memory_progress = 0.0
        sync_status = "unknown"
        
        try:
            from bubble_rag.training.model_sft.services.task_manager import task_manager
            memory_task = task_manager.get_task(task_id)
            if memory_task:
                memory_progress = memory_task.progress
                # è®¡ç®—åŒæ­¥çŠ¶æ€
                sync_diff = abs(real_time_progress - memory_progress)
                sync_status = "synced" if sync_diff < 1 else "out_of_sync"
                logger.debug(f"ä»»åŠ¡è¯¦æƒ…API: æ•°æ®åº“è¿›åº¦={real_time_progress}%, å†…å­˜è¿›åº¦={memory_progress}%, åŒæ­¥çŠ¶æ€={sync_status}")
            else:
                logger.debug(f"å†…å­˜ä¸­æœªæ‰¾åˆ°ä»»åŠ¡ï¼Œä½¿ç”¨æ•°æ®åº“è¿›åº¦: {real_time_progress}%")
                sync_status = "memory_not_found"
        except Exception as e:
            logger.warning(f"è·å–å†…å­˜ä»»åŠ¡å¤±è´¥ï¼Œä½¿ç”¨æ•°æ®åº“è¿›åº¦: {e}")
            sync_status = "memory_error"
            
        # ç¡®ä¿è¿›åº¦åœ¨åˆç†èŒƒå›´å†…
        real_time_progress = max(0.0, min(100.0, real_time_progress))

        # ğŸ”§ ä¿®å¤ï¼šçŠ¶æ€ä¸è¿›åº¦ä¸€è‡´æ€§æ£€æŸ¥
        if task_db.status in ["SUCCEEDED", "FAILED", "STOPPED"] and real_time_progress == 0:
            # å¦‚æœä»»åŠ¡å·²ç»ç»“æŸä½†è¿›åº¦ä¸º0ï¼Œè®¾ç½®è¿›åº¦ä¸º100%ï¼ˆé’ˆå¯¹SUCCESSï¼‰æˆ–ä¿æŒ0ï¼ˆé’ˆå¯¹FAILED/STOPPEDï¼‰
            if task_db.status == "SUCCEEDED":
                real_time_progress = 100.0
                logger.info(f"ğŸ”§ ä¿®å¤ä»»åŠ¡è¿›åº¦: {task_id} çŠ¶æ€={task_db.status}, è¿›åº¦ä»0%ä¿®æ­£ä¸º100%")

        # ğŸ”§ è°ƒè¯•æ—¥å¿—ï¼šè¾“å‡ºæœ€ç»ˆä½¿ç”¨çš„è¿›åº¦å€¼
        logger.info(f"ğŸ” ä»»åŠ¡è¯¦æƒ…APIè°ƒè¯•: task_id={task_id}, æ•°æ®åº“åŸå§‹è¿›åº¦={task_db.progress}, æœ€ç»ˆè¿›åº¦={real_time_progress}, åŒæ­¥çŠ¶æ€={sync_status}")
        
        # è®¡ç®—é¢„ä¼°å‰©ä½™æ—¶é—´ï¼ˆä½¿ç”¨å®æ—¶è¿›åº¦ï¼‰
        estimated_time = None
        estimated_completion_time = None
        if task_db.started_at and real_time_progress > 0 and real_time_progress < 100:
            elapsed = (datetime.now() - task_db.started_at).total_seconds()
            estimated_total = elapsed / (real_time_progress / 100)
            estimated_time = max(0, estimated_total - elapsed)
            # è®¡ç®—é¢„è®¡å®Œæˆæ—¶é—´ç‚¹
            from datetime import timedelta
            estimated_completion_time = (datetime.now() + timedelta(seconds=estimated_time)).isoformat()
        
        # ğŸ“Š è¯¦ç»†ä»»åŠ¡ä¿¡æ¯
        task_detail = {
            # åŸºç¡€ä¿¡æ¯
            "task_id": task_db.task_id,
            "task_name": task_db.task_name,
            "description": task_db.description,
            "train_type": task_db.train_type,
            "model_name_or_path": task_db.model_name_or_path,
            "dataset_name_or_path": task_db.dataset_name_or_path,
            "HF_subset": getattr(task_db, 'HF_subset', None),  # æ–°å¢å­—æ®µ
            "output_dir": task_db.output_dir,
            "device": task_db.device,
            
            # çŠ¶æ€ä¿¡æ¯
            "status": task_db.status,
            "progress": real_time_progress,
            "is_running": is_running,
            
            # æ—¶é—´ä¿¡æ¯
            "created_at": task_db.created_at.isoformat() if task_db.created_at else None,
            "started_at": task_db.started_at.isoformat() if task_db.started_at else None,
            "completed_at": task_db.completed_at.isoformat() if task_db.completed_at else None,
            "duration_seconds": _calculate_duration_seconds(task_db),
            "estimated_completion_time": estimated_completion_time,
            
            # ç»“æœä¿¡æ¯
            "final_model_path": task_db.final_model_path,
            "error_message": task_db.error_message,
            
            # è¿›ç¨‹ä¿¡æ¯ï¼ˆå¦‚æœæ­£åœ¨è¿è¡Œï¼‰
            "process_info": process_info,
            
            # è®­ç»ƒå‚æ•°ï¼ˆè§£æä¸ºå¯¹è±¡ä¾¿äºå‰ç«¯ä½¿ç”¨ï¼‰
            "training_params": json.loads(task_db.training_params) if task_db.training_params else {}
        }
        
        return SrvResult(code=200, msg="è·å–ä»»åŠ¡è¯¦æƒ…æˆåŠŸ", data=task_detail)
            
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_task_detail")
        return SrvResult(code=500, msg=f"è·å–ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {str(e)}")

@router.get("/tasks/{task_id}/datasets")
def get_task_datasets(
    task_id: str,
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """è·å–ä»»åŠ¡çš„è®­ç»ƒæ•°æ®é›†ä¿¡æ¯ - åŸºäºç”¨æˆ·æƒé™"""
    try:
        # ğŸ” éªŒè¯ä»»åŠ¡è®¿é—®æƒé™
        current_user, task_db = validate_task_access(task_id, username)
        
        # ğŸ“Š è·å–ä»»åŠ¡çš„æ•°æ®é›†ä¿¡æ¯
        try:
            # è·å–æ‰€æœ‰æ•°æ®æº
            data_sources = TrainingDatasetService.get_data_sources_by_task(task_id)
            
            if not data_sources:
                return SrvResult(
                    code=200,
                    msg="ä»»åŠ¡æš‚æ— æ•°æ®é›†ä¿¡æ¯",
                    data={
                        "task_id": task_id,
                        "data_sources": [],
                        "summary": {
                            "total_sources": 0,
                            "total_samples": 0,
                            "actual_total_samples": 0,
                            "available_splits": []
                        }
                    }
                )
            
            # è·å–è¯¦ç»†æ•°æ®é›†ä¿¡æ¯
            datasets_info = []
            total_samples = 0
            actual_total_samples = 0
            all_splits = set()
            
            for source_id in data_sources:
                source_info = TrainingDatasetService.get_splits_by_source(task_id, source_id)
                datasets_info.append({
                    "data_source_id": source_id,
                    "dataset_base_name": source_info["base_name"],
                    "dataset_path": source_info["path"],
                    "splits": source_info["splits"]
                })
                
                # ç»Ÿè®¡ä¿¡æ¯
                for split_info in source_info["splits"].values():
                    total_samples += split_info.get("samples", 0)
                    actual_total_samples += split_info.get("actual_samples", 0)
                    all_splits.add(split_info.get("split_type", "unknown"))
            
            # è·å–æ€§èƒ½æ‘˜è¦
            performance_summary = TrainingDatasetService.get_source_performance_summary(task_id)
            
            datasets_data = {
                "task_id": task_id,
                "data_sources": datasets_info,
                "performance_summary": performance_summary,
                "summary": {
                    "total_sources": len(data_sources),
                    "total_samples": total_samples,  # åŸå§‹æ•°æ®é›†æ€»æ ·æœ¬æ•°
                    "actual_total_samples": actual_total_samples,  # å®é™…è®­ç»ƒä½¿ç”¨æ€»æ ·æœ¬æ•°
                    "available_splits": list(all_splits),
                    "timestamp": datetime.now().isoformat()
                }
            }
            
            return SrvResult(
                code=200,
                msg="è·å–ä»»åŠ¡æ•°æ®é›†ä¿¡æ¯æˆåŠŸ",
                data=datasets_data
            )
            
        except Exception as dataset_error:
            logger.error(f"è·å–æ•°æ®é›†ä¿¡æ¯å¤±è´¥: {dataset_error}")
            return SrvResult(
                code=500,
                msg=f"è·å–æ•°æ®é›†ä¿¡æ¯å¤±è´¥: {str(dataset_error)}"
            )
            
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡æ•°æ®é›†å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_task_datasets")
        return SrvResult(code=500, msg=f"è·å–ä»»åŠ¡æ•°æ®é›†å¤±è´¥: {str(e)}")

@router.get("/tasks/{task_id}/training_metrics")
def get_task_training_metrics(
    task_id: str,
    limit: Optional[int] = Query(None, description="é™åˆ¶è¿”å›çš„lossè®°å½•æ•°é‡ï¼Œä¸ä¼ åˆ™è·å–å…¨éƒ¨"),
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """è·å–ä»»åŠ¡çš„è®­ç»ƒæŒ‡æ ‡å’Œlosså†å² - ä»…å½“å‰æœåŠ¡å®ä¾‹"""
    try:
        # ğŸ” éªŒè¯ä»»åŠ¡è®¿é—®æƒé™
        current_user, task_db = validate_task_access(task_id, username)
        
        # ğŸŒ æ”¯æŒè·¨æœåŠ¡æŸ¥è¯¢ï¼šå·²é€šè¿‡ç”¨æˆ·æƒé™éªŒè¯ï¼Œå…è®¸æŸ¥è¯¢è®­ç»ƒæ—¥å¿—
        # æœåŠ¡éš”ç¦»ä»…ç”¨äºè¿›ç¨‹ç®¡ç†ï¼Œä¸é™åˆ¶æ•°æ®æŸ¥è¯¢
        
        # ğŸ“Š è·å–è®­ç»ƒæŒ‡æ ‡å’Œlosså†å²
        try:
            # æ£€æŸ¥æœ¬åœ°lossæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            output_dir = task_db.output_dir
            if not output_dir:
                return SrvResult(
                    code=404,
                    msg="ä»»åŠ¡è¾“å‡ºç›®å½•æœªé…ç½®ï¼Œæ— æ³•è·å–è®­ç»ƒæŒ‡æ ‡"
                )
            
            from bubble_rag.training.model_sft.utils.loss_manager import get_loss_manager
            
            # åˆ›å»ºæˆ–è·å–lossç®¡ç†å™¨
            loss_manager = get_loss_manager(output_dir, task_id)
            
            # è·å–è®­ç»ƒæŒ‡æ ‡æ±‡æ€»
            training_metrics = loss_manager.get_training_metrics()
            
            # è·å–losså†å²è®°å½•
            loss_history = loss_manager.get_loss_history(limit=limit)
            
            # æ„å»ºå“åº”æ•°æ®
            metrics_data = {
                "task_id": task_id,
                "training_metrics": training_metrics,
                "loss_history": loss_history,
                "loss_history_count": len(loss_history),
                "total_loss_records": training_metrics.get("loss_records_count", 0),
                "files_info": {
                    "loss_history_file": str(loss_manager.loss_history_file),
                    "training_metrics_file": str(loss_manager.training_metrics_file),
                    "loss_history_exists": loss_manager.loss_history_file.exists(),
                    "training_metrics_exists": loss_manager.training_metrics_file.exists()
                }
            }
            
            return SrvResult(
                code=200,
                msg="è·å–è®­ç»ƒæŒ‡æ ‡æˆåŠŸ",
                data=metrics_data
            )
            
        except Exception as metrics_error:
            logger.error(f"è·å–è®­ç»ƒæŒ‡æ ‡å¤±è´¥: {metrics_error}")
            return SrvResult(
                code=500,
                msg=f"è·å–è®­ç»ƒæŒ‡æ ‡å¤±è´¥: {str(metrics_error)}"
            )
            
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è®­ç»ƒæŒ‡æ ‡å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_task_training_metrics")
        return SrvResult(code=500, msg=f"è·å–ä»»åŠ¡è®­ç»ƒæŒ‡æ ‡å¤±è´¥: {str(e)}")

@router.get("/tasks/{task_id}/progress")
def get_task_progress(
    task_id: str,
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """è·å–ä»»åŠ¡å®æ—¶è¿›åº¦ï¼ˆä»å†…å­˜è·å–ï¼Œé«˜é¢‘è½®è¯¢ä¼˜åŒ–ï¼‰"""
    try:
        # ğŸ” éªŒè¯ä»»åŠ¡è®¿é—®æƒé™
        current_user, task_db = validate_task_access(task_id, username)

        # ğŸš€ ç›´æ¥ä»å†…å­˜è·å–å®æ—¶è¿›åº¦ï¼ˆé¿å…æ•°æ®åº“æŸ¥è¯¢å»¶è¿Ÿï¼‰
        from bubble_rag.training.model_sft.services.task_manager import task_manager
        task = task_manager.get_task(task_id)

        if not task:
            # ä»»åŠ¡å­˜åœ¨ä½†ä¸åœ¨å†…å­˜ä¸­ï¼ˆå¯èƒ½å·²å®Œæˆï¼‰ï¼Œè¿”å›æ•°æ®åº“ä¸­çš„çŠ¶æ€
            progress_data = {
                "task_id": task_db.task_id,
                "status": task_db.status,
                "progress": task_db.progress,
                "is_running": False,
                "timestamp": datetime.now().isoformat(),
                "source": "database"  # æ ‡è¯†æ•°æ®æ¥æº
            }
            return SrvResult(code=200, msg="è·å–ä»»åŠ¡è¿›åº¦æˆåŠŸï¼ˆæ¥æºï¼šæ•°æ®åº“ï¼‰", data=progress_data)
        
        # ğŸŒ æ”¯æŒè·¨æœåŠ¡æŸ¥è¯¢ï¼šå·²é€šè¿‡ç”¨æˆ·æƒé™éªŒè¯ï¼Œå…è®¸æŸ¥è¯¢ä»»åŠ¡è¿›åº¦
        # æœåŠ¡éš”ç¦»ä»…ç”¨äºè¿›ç¨‹ç®¡ç†ï¼Œä¸é™åˆ¶æ•°æ®æŸ¥è¯¢
        
        # ğŸŒ è·¨æœåŠ¡è·å–è¿è¡Œè¿›ç¨‹ä¿¡æ¯
        # é€šè¿‡æ•°æ®åº“PIDæ£€æŸ¥å®é™…è¿è¡ŒçŠ¶æ€ï¼Œè€Œä¸æ˜¯ä¾èµ–æœ¬åœ°æœåŠ¡å®ä¾‹
        task_db = training_task_service.get_training_task(task_id)
        is_running = False
        if task_db and task_db.process_pid:
            is_running = _check_process_running(task_db.process_pid)
        
        # ğŸš€ å®æ—¶è¿›åº¦ä¿¡æ¯ï¼ˆä»å†…å­˜è·å–ï¼Œé«˜é¢‘è½®è¯¢ä¼˜åŒ–ï¼‰
        # ğŸ”§ ä¿®å¤è¿›åº¦åŒæ­¥é—®é¢˜ï¼šä¼˜å…ˆä»æ•°æ®åº“è·å–æœ€æ–°è¿›åº¦ï¼Œç¡®ä¿å‡†ç¡®æ€§
        try:
            task_db = training_task_service.get_training_task(task_id)
            if task_db and task_db.service_instance_id == unified_training_service.service_instance_id:
                # ä½¿ç”¨æ•°æ®åº“ä¸­çš„æœ€æ–°è¿›åº¦æ•°æ®ï¼Œå› ä¸ºå®ƒæ›´å¯é 
                db_progress = task_db.progress or 0
                db_status = task_db.status

                # ğŸ”§ ä¿®å¤ï¼šçŠ¶æ€ä¸è¿›åº¦ä¸€è‡´æ€§æ£€æŸ¥
                if db_status in ["SUCCEEDED", "FAILED", "STOPPED"] and db_progress == 0:
                    if db_status == "SUCCEEDED":
                        db_progress = 100.0
                        logger.info(f"ğŸ”§ ä¿®å¤ä»»åŠ¡è¿›åº¦: {task_id} çŠ¶æ€={db_status}, è¿›åº¦ä»0%ä¿®æ­£ä¸º100%")

                # ä½†æ˜¯ä½¿ç”¨å†…å­˜ä¸­çš„is_runningçŠ¶æ€ï¼ˆæ›´å®æ—¶ï¼‰
                memory_is_running = is_running
                
                progress_data = {
                    "task_id": task.task_id,
                    "status": db_status,  # ä½¿ç”¨æ•°æ®åº“çŠ¶æ€
                    "progress": db_progress,  # ä½¿ç”¨æ•°æ®åº“è¿›åº¦
                    "progress_percentage": db_progress,  # å…¼å®¹å­—æ®µ
                    "is_running": memory_is_running,  # ä½¿ç”¨å†…å­˜è¿è¡ŒçŠ¶æ€
                    "timestamp": datetime.now().isoformat(),
                    "source": "hybrid",  # æ··åˆæ•°æ®æº
                    # æ·»åŠ è®­ç»ƒè¯¦æƒ…
                    "training_details": {
                        "memory_progress": task.progress,
                        "database_progress": db_progress,
                        "sync_status": "synced" if abs(task.progress - db_progress) < 1 else "out_of_sync",
                        "task_started_at": task.started_at.isoformat() if task.started_at else None,
                        "process_running": memory_is_running
                    }
                }
                
                logger.info(f"è¿›åº¦APIæ··åˆæ¨¡å¼: ä»»åŠ¡{task_id} DBè¿›åº¦={db_progress}%, å†…å­˜è¿›åº¦={task.progress}%, è¿è¡ŒçŠ¶æ€={memory_is_running}")
            else:
                # å›é€€åˆ°çº¯å†…å­˜æ¨¡å¼
                progress_data = {
                    "task_id": task.task_id,
                    "status": task.status,
                    "progress": task.progress,
                    "progress_percentage": task.progress,
                    "is_running": is_running,
                    "timestamp": datetime.now().isoformat(),
                    "source": "memory_fallback"
                }
        except Exception as e:
            logger.warning(f"è·å–æ•°æ®åº“è¿›åº¦å¤±è´¥ï¼Œä½¿ç”¨å†…å­˜æ•°æ®: {e}")
            # å¦‚æœæ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œä½¿ç”¨åŸæœ‰å†…å­˜æ•°æ®
            progress_data = {
                "task_id": task.task_id,
                "status": task.status,
                "progress": task.progress,
                "progress_percentage": task.progress,
                "is_running": is_running,
                "timestamp": datetime.now().isoformat(),
                "source": "memory_emergency"
            }
        
        return SrvResult(code=200, msg="è·å–ä»»åŠ¡è¿›åº¦æˆåŠŸï¼ˆæ¥æºï¼šå†…å­˜ï¼‰", data=progress_data)

    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è¿›åº¦å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_task_progress")
        return SrvResult(code=500, msg=f"è·å–ä»»åŠ¡è¿›åº¦å¤±è´¥: {str(e)}")


def _aggregate_local_cache_data(task_id: str, raw_loss_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    å°†æœ¬åœ°ç¼“å­˜çš„åŸå§‹æ•°æ®èšåˆä¸ºè§„èŒƒåŒ–æ ¼å¼
    Args:
        task_id: ä»»åŠ¡ID
        raw_loss_data: åŸå§‹æœ¬åœ°ç¼“å­˜æ•°æ®ï¼ˆæ¯æ¡è®°å½•ç‹¬ç«‹ï¼‰
    Returns:
        è§„èŒƒåŒ–çš„èšåˆæ•°æ®ï¼ˆç›¸åŒstepçš„æ•°æ®åˆå¹¶ï¼‰
    """
    from collections import defaultdict

    if not raw_loss_data:
        return []

    # æŒ‰stepèšåˆæ•°æ®
    step_data = defaultdict(dict)
    data_sources = {}
    all_metric_names = set()

    for record in raw_loss_data:
        step = record.get('step')
        if step is None:
            continue

        # åˆå§‹åŒ–stepæ•°æ®
        if step not in step_data:
            step_data[step] = {
                'step': step,
                'epoch': record.get('epoch'),
                'timestamp': record.get('timestamp')
            }

        # èšåˆæ‰€æœ‰æŒ‡æ ‡åˆ°åŒä¸€æ¡è®°å½•ä¸­
        for key, value in record.items():
            if key not in ['step', 'epoch', 'timestamp']:
                step_data[step][key] = value

                # æ”¶é›†è¯„ä¼°æŒ‡æ ‡ä¿¡æ¯ç”¨äºç”Ÿæˆå…ƒæ•°æ®
                if key.startswith('eval_') and '_' in key[5:]:
                    parts = key[5:].split('_', 1)
                    if len(parts) >= 2:
                        source_id, metric_name = parts[0], parts[1]
                        if not metric_name.endswith('_loss') and metric_name not in ['runtime', 'second', 'steps_per_second', 'samples_per_second']:
                            all_metric_names.add(metric_name)

    # è·å–æ•°æ®æºæ˜ å°„ä¿¡æ¯
    try:
        from bubble_rag.training.mysql_service.service.training_dataset_service import TrainingDatasetService
        datasets = []
        try:
            # å°è¯•è·å–æ•°æ®åº“ä¸­çš„æ•°æ®æºä¿¡æ¯
            with TrainingDatasetService.safe_get_session() as session:
                from bubble_rag.training.mysql_service.entity.training_dataset_models import DatasetInfo
                from sqlmodel import select
                statement = select(DatasetInfo).where(DatasetInfo.task_id == task_id, DatasetInfo.split_type == 'eval')
                datasets = session.exec(statement).all()
        except Exception as db_e:
            logger.debug(f"ä»æ•°æ®åº“è·å–æ•°æ®æºä¿¡æ¯å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤æ˜ å°„: {db_e}")

        # æ„å»ºæ•°æ®æºæ˜ å°„
        for dataset in datasets:
            data_sources[dataset.data_source_id] = {
                "name": dataset.dataset_name,
                "source_id": dataset.data_source_id
            }
    except Exception as e:
        logger.debug(f"è·å–æ•°æ®æºæ˜ å°„å¤±è´¥: {e}")

    # ç”Ÿæˆè§„èŒƒåŒ–ç»“æœ
    result = []
    for step in sorted(step_data.keys()):
        record = step_data[step]

        # æ·»åŠ evaluation_metadataï¼ˆä»…å¯¹åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„è®°å½•ï¼‰
        eval_metrics = [k for k in record.keys() if k.startswith('eval_') and not k.endswith('_loss')]
        if eval_metrics and all_metric_names:
            try:
                from bubble_rag.training.model_sft.utils.evaluation_result import get_evaluation_result_processor
                processor = get_evaluation_result_processor()
                frontend_metadata = processor.registry.get_frontend_metadata(list(all_metric_names))

                record['evaluation_metadata'] = {
                    **frontend_metadata,
                    "data_sources": data_sources
                }
            except Exception as meta_e:
                logger.warning(f"è·å–è¯„ä¼°å…ƒæ•°æ®å¤±è´¥: {meta_e}")

        result.append(record)

    logger.debug(f"æœ¬åœ°ç¼“å­˜æ•°æ®èšåˆå®Œæˆ: {len(raw_loss_data)} -> {len(result)} æ¡è®°å½•")
    return result


@router.get("/training_logs")
def get_training_logs(
    task_id: str = Query(..., description="ä»»åŠ¡ID"),
    lines: int = Query(None, description="è·å–æ—¥å¿—è¡Œæ•°ï¼Œä¸ä¼ åˆ™è·å–å…¨éƒ¨"),
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """
    è·å–è®­ç»ƒæ—¥å¿—å’Œlossæ•°æ®

    åŠŸèƒ½ï¼š
    1. è·å–è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ–‡æœ¬æ—¥å¿—ä¿¡æ¯
    2. è·å–å®Œæ•´çš„losså†å²æ•°æ®ï¼ˆtrain_loss, eval_lossç­‰ï¼‰
    3. æä¾›ç»Ÿä¸€çš„è®­ç»ƒç›‘æ§æ•°æ®æ¥å£

    Args:
        task_id: è®­ç»ƒä»»åŠ¡ID
        lines: è·å–çš„æ—¥å¿—è¡Œæ•°ï¼ˆ1-1000ï¼‰
        username: ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œç”¨äºæƒé™éªŒè¯ï¼‰

    Returns:
        åŒ…å«æ—¥å¿—å’Œlossæ•°æ®çš„ç»¼åˆä¿¡æ¯
    """
    try:
        # éªŒè¯ä»»åŠ¡è®¿é—®æƒé™ï¼ˆåŒ…å«ç”¨æˆ·æƒé™å’ŒæœåŠ¡å®ä¾‹æƒé™ï¼‰
        current_user, task_db = validate_task_access(task_id, username)
        
        # ä»ä»»åŠ¡ç®¡ç†å™¨è·å–ä»»åŠ¡ä¿¡æ¯
        from bubble_rag.training.model_sft.services.task_manager import task_manager
        task = task_manager.get_task(task_id)

        # è·å–æ—¥å¿—æ•°æ®
        recent_logs = []
        if task:
            # ä»»åŠ¡åœ¨å†…å­˜ä¸­ï¼Œä½¿ç”¨å†…å­˜ä¸­çš„æ—¥å¿—
            if lines is None:
                recent_logs = task.logs if task.logs else []
            else:
                recent_logs = task.logs[-lines:] if task.logs else []
        else:
            # ä»»åŠ¡ä¸åœ¨å†…å­˜ä¸­ï¼Œä»»åŠ¡å·²å®Œæˆæˆ–æœåŠ¡é‡å¯
            # ç›®å‰ä»æ•°æ®åº“è·å–æ—¥å¿—åŠŸèƒ½æœªå®ç°ï¼Œè¿”å›ç©ºæ—¥å¿—
            pass
        
        # ğŸ†• è·å–lossæ•°æ®ï¼ˆä¼˜å…ˆä»æ•°æ®åº“ï¼Œå¤±è´¥æ—¶å›é€€åˆ°æœ¬åœ°æ–‡ä»¶ï¼‰
        loss_data = []
        data_source = "unknown"

        # ä¼˜å…ˆä»æ•°æ®åº“è·å–
        try:
            from bubble_rag.training.mysql_service.service.training_dataset_service import TrainingDatasetService
            db_loss_data = TrainingDatasetService.get_loss_data_by_task(task_id)
            logger.info(f"ğŸ” æ•°æ®åº“æŸ¥è¯¢ç»“æœ: ä»»åŠ¡{task_id}, è®°å½•æ•°={len(db_loss_data) if db_loss_data else 0}")
            if db_loss_data:
                loss_data = db_loss_data
                data_source = "database"
                logger.info(f"âœ… ä»æ•°æ®åº“è·å–lossæ•°æ®æˆåŠŸ: {len(loss_data)} æ¡è®°å½•")
            else:
                logger.info("æ•°æ®åº“ä¸­æš‚æ— lossæ•°æ®ï¼Œå°è¯•æœ¬åœ°æ–‡ä»¶")
                raise Exception("æ•°æ®åº“ä¸­æ— æ•°æ®")
        except Exception as db_e:
            logger.warning(f"ä»æ•°æ®åº“è·å–lossæ•°æ®å¤±è´¥: {db_e}")

            # å›é€€åˆ°æœ¬åœ°æ–‡ä»¶
            try:
                from bubble_rag.training.model_sft.utils.loss_manager import get_loss_manager
                output_dir = task_db.output_dir or "/tmp/training_output"
                loss_manager = get_loss_manager(output_dir, task_id)
                raw_loss_data = loss_manager.get_loss_history()

                # ğŸ”„ èšåˆæœ¬åœ°ç¼“å­˜æ•°æ®ä¸ºè§„èŒƒåŒ–æ ¼å¼
                loss_data = _aggregate_local_cache_data(task_id, raw_loss_data)
                data_source = "local_file"
                logger.info(f"âœ… ä»æœ¬åœ°æ–‡ä»¶è·å–å¹¶èšåˆlossæ•°æ®æˆåŠŸ: {len(raw_loss_data)} -> {len(loss_data)} æ¡è®°å½•")
            except Exception as file_e:
                logger.warning(f"ä»æœ¬åœ°æ–‡ä»¶è·å–lossæ•°æ®å¤±è´¥: {file_e}")
                data_source = "failed"
        
        return SrvResult(
            code=200,
            msg="è·å–è®­ç»ƒæ—¥å¿—æˆåŠŸ",
            data={
                "task_id": task.task_id if task else task_id,
                "logs": recent_logs,
                "total_logs": len(task.logs) if task and task.logs else 0,
                "requested_lines": lines,
                "loss_data": loss_data,
                "total_loss_records": len(loss_data),
                "data_source": data_source  # æ ‡è¯†æ•°æ®æ¥æºï¼šdatabase, local_file, failed
            }
        )
        
    except Exception as e:
        logger.error(f"è·å–è®­ç»ƒæ—¥å¿—å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_training_logs")
        return SrvResult(code=500, msg=f"è·å–è®­ç»ƒæ—¥å¿—å¤±è´¥: {str(e)}")

@router.get("/running_tasks")
def get_running_tasks(
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """è·å–æ­£åœ¨è¿è¡Œçš„è®­ç»ƒä»»åŠ¡åˆ—è¡¨ - è·¨æœåŠ¡æŸ¥è¯¢å®é™…è¿è¡Œçš„è¿›ç¨‹"""
    try:
        # ğŸ” éªŒè¯å¹¶è·å–ç”¨æˆ·ä¿¡æ¯
        from bubble_rag.utils.user_manager import validate_user
        current_user = validate_user(username)

        # ğŸŒ è·¨æœåŠ¡æŸ¥è¯¢ï¼šä»æ•°æ®åº“è·å–æœ‰PIDçš„ä»»åŠ¡ï¼Œç„¶åéªŒè¯è¿›ç¨‹æ˜¯å¦å­˜æ´»
        tasks_info = []

        # æŸ¥è¯¢æ•°æ®åº“ä¸­æœ‰process_pidçš„ä»»åŠ¡ï¼ˆå¯èƒ½æ­£åœ¨è¿è¡Œï¼‰
        if current_user.get('is_admin', False):
            # ç®¡ç†å‘˜å¯ä»¥çœ‹åˆ°æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
            potential_running_tasks = training_task_service.get_tasks_with_process_pid()
        else:
            # æ™®é€šç”¨æˆ·åªèƒ½çœ‹åˆ°è‡ªå·±çš„ä»»åŠ¡
            potential_running_tasks = training_task_service.get_tasks_with_process_pid(username=current_user['username'])

        for task_db in potential_running_tasks:
            # è·¨æœåŠ¡è¿›ç¨‹éªŒè¯ï¼šæ£€æŸ¥PIDæ˜¯å¦çœŸçš„åœ¨è¿è¡Œ
            if task_db.process_pid:
                is_actually_running = _check_process_running(task_db.process_pid)

                if is_actually_running:
                    # ä»ä»»åŠ¡ç®¡ç†å™¨è·å–ä»»åŠ¡è¯¦ç»†ä¿¡æ¯ï¼ˆå¦‚æœåœ¨å½“å‰æœåŠ¡ä¸­ï¼‰
                    from bubble_rag.training.model_sft.services.task_manager import task_manager
                    task = task_manager.get_task(task_db.task_id)

                    if task:
                        # å†…å­˜ä¸­æœ‰ä»»åŠ¡ä¿¡æ¯ï¼Œä½¿ç”¨æ··åˆæ•°æ®æº
                        task_info = convert_task_to_dict(task)
                        # ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“çš„è¿›åº¦å’ŒçŠ¶æ€ï¼ˆæ›´å¯é ï¼‰
                        task_info["progress"] = task_db.progress or 0
                        task_info["status"] = task_db.status
                    else:
                        # è·¨æœåŠ¡ä»»åŠ¡ï¼Œåªèƒ½ä»æ•°æ®åº“æ„å»ºä¿¡æ¯
                        task_info = {
                            "task_id": task_db.task_id,
                            "task_name": task_db.task_name,
                            "train_type": task_db.train_type,
                            "model_name_or_path": task_db.model_name_or_path,
                            "dataset_name_or_path": task_db.dataset_name_or_path,
                            "HF_subset": task_db.HF_subset,
                            "status": task_db.status,
                            "progress": task_db.progress or 0,
                            "username": task_db.username,
                            "created_at": task_db.created_at.isoformat() if task_db.created_at else None,
                            "started_at": task_db.started_at.isoformat() if task_db.started_at else None,
                            "completed_at": task_db.completed_at.isoformat() if task_db.completed_at else None,
                            "duration_seconds": _calculate_duration_seconds(task_db),
                            "error_message": task_db.error_message
                        }

                    # æ·»åŠ æ ¼å¼åŒ–æ—¶é•¿
                    task_info["duration_formatted"] = _format_duration(task_info.get("duration_seconds"))

                    # æ·»åŠ è·¨æœåŠ¡è¿›ç¨‹ä¿¡æ¯
                    task_info["process_info"] = {
                        "pid": task_db.process_pid,
                        "status": task_db.process_status,
                        "cross_service": task is None,  # æ ‡è¯†æ˜¯å¦ä¸ºè·¨æœåŠ¡ä»»åŠ¡
                        "verified_running": True
                    }

                    tasks_info.append(task_info)
                else:
                    # è¿›ç¨‹å·²æ­»ä½†æ•°æ®åº“æœªæ›´æ–°ï¼Œè®°å½•è­¦å‘Š
                    logger.warning(f"ğŸš¨ æ£€æµ‹åˆ°åƒµå°¸ä»»åŠ¡ï¼š{task_db.task_id} PID={task_db.process_pid} è¿›ç¨‹å·²ç»“æŸä½†æ•°æ®åº“æœªæ›´æ–°")

        return SrvResult(
            code=200,
            msg="è·å–è¿è¡Œä¸­ä»»åŠ¡åˆ—è¡¨æˆåŠŸ",
            data={
                "running_tasks": tasks_info,
                "total": len(tasks_info),
                "query_method": "cross_service_database_pid_verification"
            }
        )

    except Exception as e:
        logger.error(f"è·å–è¿è¡Œä¸­ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_running_tasks")
        return SrvResult(code=500, msg=f"è·å–è¿è¡Œä¸­ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}")

@router.get("/tasks")
def get_tasks(
    status: Optional[str] = Query(None, description="æŒ‰çŠ¶æ€è¿‡æ»¤: PENDING, RUNNING, SUCCEEDED, FAILED, STOPPED"),
    train_type: Optional[str] = Query(None, description="æŒ‰è®­ç»ƒç±»å‹è¿‡æ»¤: embedding, reranker"),
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰"),
    limit: Optional[int] = Query(None, description="é™åˆ¶è¿”å›æ•°é‡ï¼ˆä¸ä¼ åˆ™è¿”å›æ‰€æœ‰ï¼‰"),
    offset: Optional[int] = Query(0, description="åç§»é‡ï¼ˆé»˜è®¤0ï¼‰")
):
    """è·å–ä»»åŠ¡åˆ—è¡¨ï¼ˆæ”¯æŒè¿‡æ»¤ï¼‰- åŸºäºç”¨æˆ·æƒé™è¿”å›ä»»åŠ¡"""
    try:
        # ğŸ” éªŒè¯å¹¶è·å–ç”¨æˆ·ä¿¡æ¯
        from bubble_rag.utils.user_manager import validate_user
        current_user = validate_user(username)


        if current_user.get('is_admin', False) and username is None:
            # adminç”¨æˆ·é»˜è®¤æŸ¥çœ‹æ‰€æœ‰ä»»åŠ¡
            all_user_tasks = training_task_service.get_tasks_for_user_business(
                username=None,  # ä¸ä¼ usernameä»¥è·å–æ‰€æœ‰ä»»åŠ¡
                user_info=current_user
            )
        else:
            # å…¶ä»–æƒ…å†µï¼šæŸ¥çœ‹æŒ‡å®šç”¨æˆ·çš„ä»»åŠ¡
            target_username = username if username else current_user['username']
            all_user_tasks = training_task_service.get_tasks_for_user_business(
                username=target_username,
                user_info=current_user
            )
        
        # åº”ç”¨è¿‡æ»¤æ¡ä»¶ï¼ˆæ³¨æ„ï¼šall_user_tasks æ˜¯å­—å…¸åˆ—è¡¨ï¼‰
        filtered_tasks = all_user_tasks
        if status:
            filtered_tasks = [t for t in filtered_tasks if t.get("status") == status]
        if train_type:
            filtered_tasks = [t for t in filtered_tasks if t.get("train_type") == train_type]
        
        # è¿”å›æ‰€æœ‰è¿‡æ»¤åçš„ä»»åŠ¡
        total_count = len(filtered_tasks)
        tasks = filtered_tasks
        
        # ğŸŒ è·¨æœåŠ¡è·å–è¿è¡Œè¿›ç¨‹ä¿¡æ¯ï¼ˆç”¨äºis_runningçŠ¶æ€ï¼‰
        # æ„å»ºPIDæ˜ å°„ç”¨äºå¿«é€ŸæŸ¥æ‰¾è¿è¡ŒçŠ¶æ€
        running_task_pids = {}
        try:
            # æ ¹æ®ç”¨æˆ·æƒé™è·å–æœ‰PIDçš„ä»»åŠ¡
            if current_user.get('is_admin', False):
                pid_tasks = training_task_service.get_tasks_with_process_pid()
            else:
                pid_tasks = training_task_service.get_tasks_with_process_pid(username=current_user['username'])

            for pid_task in pid_tasks:
                if pid_task.process_pid and _check_process_running(pid_task.process_pid):
                    running_task_pids[pid_task.task_id] = True
        except Exception as e:
            logger.warning(f"è·å–è·¨æœåŠ¡è¿è¡ŒçŠ¶æ€å¤±è´¥: {e}")
            running_task_pids = {}

        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼ˆæ¦‚è§ˆä¿¡æ¯ï¼‰- æ³¨æ„ï¼štasks å·²ç»æ˜¯å­—å…¸åˆ—è¡¨
        tasks_data = []
        for task in tasks:
            task_id = task.get("task_id")
            is_running = task_id in running_task_pids

            # è·å–å®æ—¶è¿›åº¦ï¼ˆä¸ä»»åŠ¡è¿›åº¦æ¥å£ä¿æŒä¸€è‡´ - ä½¿ç”¨æ•°æ®åº“è¿›åº¦ä¼˜å…ˆï¼‰
            # ä½¿ç”¨æ•°æ®åº“è¿›åº¦ä½œä¸ºä¸»è¦æ•°æ®æºï¼Œç¡®ä¿ä¸è¿›åº¦æ¥å£ä¸€è‡´
            real_time_progress = task.get("progress", 0.0) or 0.0

            # ç¡®ä¿è¿›åº¦åœ¨åˆç†èŒƒå›´å†…
            real_time_progress = max(0.0, min(100.0, real_time_progress))

            # ğŸ”§ ä¿®å¤ï¼šçŠ¶æ€ä¸è¿›åº¦ä¸€è‡´æ€§æ£€æŸ¥
            task_status = task.get("status", "UNKNOWN")
            if task_status in ["SUCCEEDED", "FAILED", "STOPPED"] and real_time_progress == 0:
                # å¦‚æœä»»åŠ¡å·²ç»ç»“æŸä½†è¿›åº¦ä¸º0ï¼Œè®¾ç½®è¿›åº¦ä¸º100%ï¼ˆé’ˆå¯¹SUCCESSï¼‰æˆ–ä¿æŒ0ï¼ˆé’ˆå¯¹FAILED/STOPPEDï¼‰
                if task_status == "SUCCEEDED":
                    real_time_progress = 100.0
                    logger.info(f"ğŸ”§ ä¿®å¤ä»»åŠ¡è¿›åº¦: {task.get('task_id')} çŠ¶æ€={task_status}, è¿›åº¦ä»0%ä¿®æ­£ä¸º100%")

            # ğŸ“‹ ä»»åŠ¡æ¦‚è§ˆä¿¡æ¯ï¼ˆæ¯”è¯¦æƒ…è½»é‡ï¼‰
            task_overview = {
                "task_id": task_id,
                "task_name": task.get("task_name"),
                "train_type": task.get("train_type"),
                "model_name_or_path": task.get("model_name_or_path"),
                "dataset_name_or_path": task.get("dataset_name_or_path"),
                "HF_subset": task.get("HF_subset"),
                "status": task.get("status"),
                "progress": real_time_progress,  # ä½¿ç”¨å®æ—¶è¿›åº¦
                "is_running": is_running,
                "username": task.get("username"),  # æ·»åŠ ç”¨æˆ·åå­—æ®µ
                "created_at": task.get("created_at"),
                "started_at": task.get("started_at"),
                "completed_at": task.get("completed_at"),
                "error_message": task.get("error_message")
            }

            # æ·»åŠ è®­ç»ƒæ—¶é•¿å­—æ®µ - éœ€è¦åˆ›å»ºä¸€ä¸ªä¸´æ—¶å¯¹è±¡æ¥è®¡ç®—æ—¶é•¿
            duration_seconds = _calculate_duration_from_dict(task)
            task_overview["training_duration_seconds"] = duration_seconds
            task_overview["duration_formatted"] = _format_duration(duration_seconds)

            # æ·»åŠ é¢„ä¼°å®Œæˆæ—¶é—´å­—æ®µï¼ˆä½¿ç”¨å®æ—¶è¿›åº¦çš„task_overviewï¼‰
            estimated_completion_time = _calculate_estimated_completion_time_from_dict(task_overview)
            task_overview["estimated_completion_time"] = estimated_completion_time

            tasks_data.append(task_overview)
        
        # total_count å·²ç»åœ¨ä¸Šé¢è®¾ç½®å¥½äº†
        
        return SrvResult(
            code=200,
            msg="è·å–ä»»åŠ¡åˆ—è¡¨æˆåŠŸ",
            data={
                "tasks": tasks_data,
                "total": total_count
            }
        )
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_tasks")
        return SrvResult(code=500, msg=f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}")

# é…ç½®å’ŒéªŒè¯æ¥å£ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
@router.post("/config/validate")
def validate_config(request: UnifiedTrainingConfig):
    """éªŒè¯è®­ç»ƒé…ç½®"""
    try:
        is_valid, message = config_service.validate_training_config(request.model_dump())
        
        return SrvResult(
            code=200,
            msg="é…ç½®éªŒè¯å®Œæˆ",
            data={"valid": is_valid, "message": message}
        )
        
    except Exception as e:
        logger.error(f"éªŒè¯è®­ç»ƒé…ç½®å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "validate_config")
        return SrvResult(code=500, msg=f"éªŒè¯è®­ç»ƒé…ç½®å¤±è´¥: {str(e)}")

@router.get("/gpu/status")
def get_gpu_status():
    """è·å–GPUèµ„æºçŠ¶æ€ - å…¨å±€å¯è§ï¼ˆé¿å…èµ„æºå†²çªï¼‰"""
    try:
        # ğŸ” éªŒè¯ç”¨æˆ·èº«ä»½ï¼ˆä½†GPUçŠ¶æ€å¯¹æ‰€æœ‰ç”¨æˆ·å…¨å±€å¯è§ï¼‰
        from bubble_rag.utils.user_manager import validate_user
        current_user = validate_user()

        # ğŸŒ GPUæ˜¯ç‰©ç†èµ„æºï¼Œå¿…é¡»å…¨å±€å¯è§ä»¥é¿å…èµ„æºå†²çª
        gpu_status = gpu_resource_manager.get_resource_status()
        return SrvResult(code=200, msg="è·å–GPUçŠ¶æ€æˆåŠŸ", data=gpu_status)

    except Exception as e:
        logger.error(f"è·å–GPUçŠ¶æ€å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_gpu_status")
        return SrvResult(code=500, msg=f"è·å–GPUçŠ¶æ€å¤±è´¥: {str(e)}")

@router.post("/gpu/cleanup")
def cleanup_gpu_resources(
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """æ‰‹åŠ¨æ¸…ç†æ‰€æœ‰å·²å®Œæˆä»»åŠ¡çš„GPUèµ„æº - ä»…ç®¡ç†å‘˜"""
    try:
        # éªŒè¯ç®¡ç†å‘˜æƒé™ - GPUèµ„æºæ¸…ç†å±äºè¿ç»´æ“ä½œï¼Œåªå…è®¸ç®¡ç†å‘˜æ‰§è¡Œ
        from bubble_rag.utils.user_manager import validate_user
        current_user = validate_user(username)
        if not current_user.get('is_admin', False):
            return SrvResult(
                code=403,
                msg=f"æƒé™ä¸è¶³ï¼šGPUèµ„æºæ¸…ç†éœ€è¦ç®¡ç†å‘˜æƒé™ï¼Œå½“å‰ç”¨æˆ·: {current_user.get('username')}"
            )

        # å¼ºåˆ¶æ‰§è¡Œæ¸…ç†
        gpu_resource_manager._cleanup_failed_task_allocations()

        # è·å–æ¸…ç†åçš„çŠ¶æ€
        gpu_status = gpu_resource_manager.get_resource_status()

        return SrvResult(
            code=200,
            msg="GPUèµ„æºæ¸…ç†å®Œæˆ",
            data={
                "cleaned": True,
                "current_status": gpu_status
            }
        )

    except Exception as e:
        logger.error(f"æ¸…ç†GPUèµ„æºå¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "cleanup_gpu_resources")
        return SrvResult(code=500, msg=f"æ¸…ç†GPUèµ„æºå¤±è´¥: {str(e)}")

@router.post("/gpu/force_release/{task_id}")
def force_release_gpu_for_task(
    task_id: str,
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """å¼ºåˆ¶é‡Šæ”¾æŒ‡å®šä»»åŠ¡çš„GPUèµ„æº - åŸºäºç”¨æˆ·æƒé™"""
    try:
        # éªŒè¯ä»»åŠ¡è®¿é—®æƒé™ï¼ˆåŒ…å«ç”¨æˆ·æƒé™å’ŒæœåŠ¡å®ä¾‹æƒé™ï¼‰
        current_user, task_db = validate_task_access(task_id, username)

        success = gpu_resource_manager.force_cleanup_task(task_id)

        if success:
            gpu_status = gpu_resource_manager.get_resource_status()
            return SrvResult(
                code=200,
                msg=f"ä»»åŠ¡ {task_id} çš„GPUèµ„æºå·²å¼ºåˆ¶é‡Šæ”¾",
                data={
                    "task_id": task_id,
                    "released": True,
                    "current_status": gpu_status
                }
            )
        else:
            return SrvResult(
                code=404,
                msg=f"ä»»åŠ¡ {task_id} æ²¡æœ‰åˆ†é…GPUèµ„æºæˆ–é‡Šæ”¾å¤±è´¥"
            )

    except Exception as e:
        logger.error(f"å¼ºåˆ¶é‡Šæ”¾ä»»åŠ¡ {task_id} GPUèµ„æºå¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "force_release_gpu_for_task")
        return SrvResult(code=500, msg=f"å¼ºåˆ¶é‡Šæ”¾GPUèµ„æºå¤±è´¥: {str(e)}")

# æ•°æ®é›†ç›¸å…³æ¥å£ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
@router.get("/datasets/list")
def list_available_datasets(
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """åˆ—å‡ºå¯ç”¨çš„æ•°æ®é›† - éœ€è¦ç”¨æˆ·è®¤è¯"""
    try:
        # ğŸ” éªŒè¯ç”¨æˆ·èº«ä»½
        from bubble_rag.utils.user_manager import validate_user
        current_user = validate_user(username)

        datasets = dataset_service.list_available_datasets()
        return SrvResult(code=200, msg="è·å–æ•°æ®é›†åˆ—è¡¨æˆåŠŸ", data=datasets)
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_datasets")
        return SrvResult(code=500, msg=f"è·å–æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {str(e)}")

@router.post("/datasets/validate")
def validate_dataset(
    dataset_path: str = Query(..., description="æ•°æ®é›†è·¯å¾„"),
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """éªŒè¯æ•°æ®é›† - éœ€è¦ç”¨æˆ·è®¤è¯"""
    try:
        # ğŸ” éªŒè¯ç”¨æˆ·èº«ä»½
        from bubble_rag.utils.user_manager import validate_user
        current_user = validate_user(username)

        is_valid, message, sample_data = dataset_service.validate_dataset(dataset_path)
        
        return SrvResult(
            code=200,
            msg="æ•°æ®é›†éªŒè¯å®Œæˆ",
            data={
                "valid": is_valid,
                "message": message,
                "sample_data": sample_data
            }
        )
        
    except Exception as e:
        logger.error(f"éªŒè¯æ•°æ®é›†å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "validate_dataset")
        return SrvResult(code=500, msg=f"éªŒè¯æ•°æ®é›†å¤±è´¥: {str(e)}")

@router.post("/datasets/preview")
def preview_dataset(
    dataset_path: str = Query(..., description="æ•°æ®é›†è·¯å¾„"),
    max_samples: int = Query(5, description="é¢„è§ˆæ ·æœ¬æ•°é‡", ge=1, le=50),
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """é¢„è§ˆæ•°æ®é›† - éœ€è¦ç”¨æˆ·è®¤è¯"""
    try:
        # ğŸ” éªŒè¯ç”¨æˆ·èº«ä»½
        from bubble_rag.utils.user_manager import validate_user
        current_user = validate_user(username)

        preview_data = dataset_service.preview_dataset(dataset_path, max_samples)
        return SrvResult(code=200, msg="æ•°æ®é›†é¢„è§ˆæˆåŠŸ", data=preview_data)
        
    except Exception as e:
        logger.error(f"é¢„è§ˆæ•°æ®é›†å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "preview_dataset")
        return SrvResult(code=500, msg=f"é¢„è§ˆæ•°æ®é›†å¤±è´¥: {str(e)}")

@router.get("/service/health")
def get_service_health(
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """è·å–æœåŠ¡å¥åº·çŠ¶æ€å’Œå®ä¾‹ä¿¡æ¯ - éœ€è¦ç®¡ç†å‘˜æƒé™"""
    try:
        # éªŒè¯ç®¡ç†å‘˜æƒé™ - æœåŠ¡å¥åº·çŠ¶æ€å±äºè¿ç»´ä¿¡æ¯ï¼Œåªå…è®¸ç®¡ç†å‘˜æŸ¥çœ‹
        from bubble_rag.utils.user_manager import validate_user
        current_user = validate_user(username)
        if not current_user.get('is_admin', False):
            return SrvResult(
                code=403,
                msg=f"æƒé™ä¸è¶³ï¼šæŸ¥çœ‹æœåŠ¡å¥åº·çŠ¶æ€éœ€è¦ç®¡ç†å‘˜æƒé™ï¼Œå½“å‰ç”¨æˆ·: {current_user.get('username')}"
            )

        from bubble_rag.training.model_sft.utils.service_instance import service_instance_manager

        service_id = unified_training_service.service_instance_id
        health_status = "healthy" if service_id else "critical"
        
        health_data = {
            "service_instance_id": service_id,
            "health_status": health_status,
            "service_isolation": service_id is not None,
            "default_training_mode": unified_training_service.default_mode,
            "running_tasks_count": len(unified_training_service.get_running_processes()),
            "instance_info": service_instance_manager.get_instance_info() if service_id else None,
            "timestamp": datetime.now().isoformat()
        }
        
        if not service_id:
            health_data["warning"] = "âŒ æœåŠ¡å®ä¾‹IDä¸ºç©ºï¼ŒæœåŠ¡éš”ç¦»åŠŸèƒ½å¼‚å¸¸ï¼"
        
        return SrvResult(code=200, msg="æœåŠ¡å¥åº·çŠ¶æ€è·å–æˆåŠŸ", data=health_data)
        
    except Exception as e:
        logger.error(f"è·å–æœåŠ¡å¥åº·çŠ¶æ€å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "service_health")
        return SrvResult(code=500, msg=f"è·å–æœåŠ¡å¥åº·çŠ¶æ€å¤±è´¥: {str(e)}")

@router.get("/process/status/stats")
def get_process_status_statistics(
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """è·å–è¿›ç¨‹çŠ¶æ€ç»Ÿè®¡ä¿¡æ¯ - ä»…å½“å‰æœåŠ¡å®ä¾‹ï¼Œéœ€è¦ç®¡ç†å‘˜æƒé™"""
    try:
        # éªŒè¯ç®¡ç†å‘˜æƒé™ - è¿›ç¨‹çŠ¶æ€ç»Ÿè®¡å±äºè¿ç»´ä¿¡æ¯ï¼Œåªå…è®¸ç®¡ç†å‘˜æŸ¥çœ‹
        from bubble_rag.utils.user_manager import validate_user
        current_user = validate_user(username)
        if not current_user.get('is_admin', False):
            return SrvResult(
                code=403,
                msg=f"æƒé™ä¸è¶³ï¼šæŸ¥çœ‹è¿›ç¨‹çŠ¶æ€ç»Ÿè®¡éœ€è¦ç®¡ç†å‘˜æƒé™ï¼Œå½“å‰ç”¨æˆ·: {current_user.get('username')}"
            )

        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿åªè·å–å½“å‰æœåŠ¡å®ä¾‹çš„ç»Ÿè®¡
        current_service_id = unified_training_service.service_instance_id
        if not current_service_id:
            return SrvResult(
                code=500,
                msg="âŒ æœåŠ¡éš”ç¦»åŠŸèƒ½å¼‚å¸¸ï¼šæœåŠ¡å®ä¾‹IDä¸ºç©ºï¼Œæ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯ï¼"
            )
        
        # è·å–å½“å‰æœåŠ¡å®ä¾‹çš„ä»»åŠ¡ç»Ÿè®¡
        service_tasks = training_task_service.get_tasks_by_service_instance(current_service_id)
        stats = {}
        for task in service_tasks:
            process_status = task.process_status or ProcessStatus.UNKNOWN.value
            stats[process_status] = stats.get(process_status, 0) + 1
        
        # æ·»åŠ æ›´è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
        detailed_stats = {
            "total_tasks": sum(stats.values()),
            "status_breakdown": stats,
            "manageable_count": sum(stats.get(status, 0) for status in ProcessStatus.get_manageable_statuses()),
            "active_count": sum(stats.get(status, 0) for status in ProcessStatus.get_active_statuses()),
            "final_count": sum(stats.get(status, 0) for status in ProcessStatus.get_final_statuses()),
            "unknown_count": stats.get(ProcessStatus.UNKNOWN.value, 0),
            "timestamp": datetime.now().isoformat()
        }
        
        return SrvResult(
            code=200,
            msg="è·å–è¿›ç¨‹çŠ¶æ€ç»Ÿè®¡æˆåŠŸ",
            data=detailed_stats
        )
        
    except Exception as e:
        logger.error(f"è·å–è¿›ç¨‹çŠ¶æ€ç»Ÿè®¡å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_process_status_stats")
        return SrvResult(code=500, msg=f"è·å–è¿›ç¨‹çŠ¶æ€ç»Ÿè®¡å¤±è´¥: {str(e)}")

@router.get("/process/status/unknown")
def get_unknown_process_tasks(
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """è·å–UNKNOWNçŠ¶æ€çš„è¿›ç¨‹ä»»åŠ¡ - ä»…å½“å‰æœåŠ¡å®ä¾‹ï¼Œéœ€è¦ç®¡ç†å‘˜æƒé™"""
    try:
        # éªŒè¯ç®¡ç†å‘˜æƒé™ - UNKNOWNçŠ¶æ€ä»»åŠ¡å±äºæ•…éšœæ’æŸ¥ä¿¡æ¯ï¼Œåªå…è®¸ç®¡ç†å‘˜æŸ¥çœ‹
        from bubble_rag.utils.user_manager import validate_user
        current_user = validate_user(username)
        if not current_user.get('is_admin', False):
            return SrvResult(
                code=403,
                msg=f"æƒé™ä¸è¶³ï¼šæŸ¥çœ‹UNKNOWNçŠ¶æ€ä»»åŠ¡éœ€è¦ç®¡ç†å‘˜æƒé™ï¼Œå½“å‰ç”¨æˆ·: {current_user.get('username')}"
            )

        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿åªè·å–å½“å‰æœåŠ¡å®ä¾‹çš„ä»»åŠ¡
        current_service_id = unified_training_service.service_instance_id
        if not current_service_id:
            return SrvResult(
                code=500,
                msg="æœåŠ¡éš”ç¦»åŠŸèƒ½å¼‚å¸¸ï¼šæœåŠ¡å®ä¾‹IDä¸ºç©ºï¼Œæ— æ³•è·å–UNKNOWNä»»åŠ¡ï¼"
            )
        
        # è·å–å½“å‰æœåŠ¡å®ä¾‹çš„UNKNOWNçŠ¶æ€ä»»åŠ¡
        service_tasks = training_task_service.get_tasks_by_service_instance(current_service_id)
        unknown_tasks = [task for task in service_tasks if (task.process_status or ProcessStatus.UNKNOWN.value) == ProcessStatus.UNKNOWN.value]
        
        tasks_data = []
        for task in unknown_tasks:
            task_dict = training_task_service._task_db_to_dict(task)
            tasks_data.append({
                "task_id": task_dict["task_id"],
                "task_name": task_dict["task_name"],
                "process_pid": task_dict["process_pid"],
                "process_status": task_dict["process_status"],
                "status": task_dict["status"],
                "created_at": task_dict["created_at"],
                "updated_at": task_dict["updated_at"],
                "service_instance_id": task_dict["service_instance_id"]
            })
        
        return SrvResult(
            code=200,
            msg="è·å–UNKNOWNçŠ¶æ€ä»»åŠ¡æˆåŠŸ",
            data={
                "unknown_tasks": tasks_data,
                "count": len(tasks_data),
                "timestamp": datetime.now().isoformat()
            }
        )
        
    except Exception as e:
        logger.error(f"è·å–UNKNOWNçŠ¶æ€ä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_unknown_tasks")
        return SrvResult(code=500, msg=f"è·å–UNKNOWNçŠ¶æ€ä»»åŠ¡å¤±è´¥: {str(e)}")

@router.post("/process/status/recovery/unknown")
def recover_unknown_processes(
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """ä¸»åŠ¨è§¦å‘UNKNOWNçŠ¶æ€è¿›ç¨‹æ¢å¤ - ä»…å½“å‰æœåŠ¡å®ä¾‹ï¼Œéœ€è¦ç®¡ç†å‘˜æƒé™"""
    try:
        # éªŒè¯ç®¡ç†å‘˜æƒé™ - è¿›ç¨‹æ¢å¤æ“ä½œå±äºè¿ç»´æ“ä½œï¼Œåªå…è®¸ç®¡ç†å‘˜æ‰§è¡Œ
        from bubble_rag.utils.user_manager import validate_user
        current_user = validate_user(username)
        if not current_user.get('is_admin', False):
            return SrvResult(
                code=403,
                msg=f"æƒé™ä¸è¶³ï¼šæ‰§è¡Œè¿›ç¨‹æ¢å¤éœ€è¦ç®¡ç†å‘˜æƒé™ï¼Œå½“å‰ç”¨æˆ·: {current_user.get('username')}"
            )

        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿åªæ¢å¤å½“å‰æœåŠ¡å®ä¾‹çš„è¿›ç¨‹
        current_service_id = unified_training_service.service_instance_id
        if not current_service_id:
            return SrvResult(
                code=500,
                msg="âŒ æœåŠ¡éš”ç¦»åŠŸèƒ½å¼‚å¸¸ï¼šæœåŠ¡å®ä¾‹IDä¸ºç©ºï¼Œæ— æ³•æ¢å¤è¿›ç¨‹ï¼"
            )
        
        # é€šè¿‡ç»Ÿä¸€è®­ç»ƒæœåŠ¡è§¦å‘æ¢å¤ï¼ˆåªå¤„ç†å½“å‰æœåŠ¡å®ä¾‹çš„ä»»åŠ¡ï¼‰
        recovery_stats = unified_training_service.check_unknown_processes()
        
        return SrvResult(
            code=200,
            msg="UNKNOWNçŠ¶æ€è¿›ç¨‹æ¢å¤å®Œæˆ",
            data={
                "recovery_result": recovery_stats,
                "timestamp": datetime.now().isoformat()
            }
        )
        
    except Exception as e:
        logger.error(f"UNKNOWNçŠ¶æ€è¿›ç¨‹æ¢å¤å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "recover_unknown_processes")
        return SrvResult(code=500, msg=f"UNKNOWNçŠ¶æ€è¿›ç¨‹æ¢å¤å¤±è´¥: {str(e)}")

@router.get("/process/status/health")
def get_process_health_status(
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """è·å–è¿›ç¨‹å¥åº·çŠ¶æ€ç›‘æ§ä¿¡æ¯ - ä»…å½“å‰æœåŠ¡å®ä¾‹ï¼Œéœ€è¦ç®¡ç†å‘˜æƒé™"""
    try:
        # éªŒè¯ç®¡ç†å‘˜æƒé™ - è¿›ç¨‹å¥åº·çŠ¶æ€å±äºè¿ç»´ç›‘æ§ä¿¡æ¯ï¼Œåªå…è®¸ç®¡ç†å‘˜æŸ¥çœ‹
        from bubble_rag.utils.user_manager import validate_user
        current_user = validate_user(username)
        if not current_user.get('is_admin', False):
            return SrvResult(
                code=403,
                msg=f"æƒé™ä¸è¶³ï¼šæŸ¥çœ‹è¿›ç¨‹å¥åº·çŠ¶æ€éœ€è¦ç®¡ç†å‘˜æƒé™ï¼Œå½“å‰ç”¨æˆ·: {current_user.get('username')}"
            )

        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿åªè·å–å½“å‰æœåŠ¡å®ä¾‹çš„å¥åº·çŠ¶æ€
        current_service_id = unified_training_service.service_instance_id
        if not current_service_id:
            return SrvResult(
                code=500,
                msg="æœåŠ¡éš”ç¦»åŠŸèƒ½å¼‚å¸¸ï¼šæœåŠ¡å®ä¾‹IDä¸ºç©ºï¼Œæ— æ³•è·å–å¥åº·çŠ¶æ€ï¼"
            )
        
        # è·å–å½“å‰æœåŠ¡å®ä¾‹çš„ä»»åŠ¡ç»Ÿè®¡
        service_tasks = training_task_service.get_tasks_by_service_instance(current_service_id)
        stats = {}
        for task in service_tasks:
            process_status = task.process_status or ProcessStatus.UNKNOWN.value
            stats[process_status] = stats.get(process_status, 0) + 1
        
        # è®¡ç®—å¥åº·è¯„åˆ†
        total_tasks = sum(stats.values())
        if total_tasks == 0:
            health_score = 100
            health_level = "excellent"
        else:
            # å¥åº·è¯„åˆ†: (RUNNING + STOPPED) / total * 100
            healthy_count = stats.get(ProcessStatus.RUNNING.value, 0) + stats.get(ProcessStatus.STOPPED.value, 0)
            health_score = round((healthy_count / total_tasks) * 100, 2)
            
            if health_score >= 90:
                health_level = "excellent"
            elif health_score >= 75:
                health_level = "good"
            elif health_score >= 50:
                health_level = "warning"
            else:
                health_level = "critical"
        
        health_data = {
            "health_score": health_score,
            "health_level": health_level,
            "total_processes": total_tasks,
            "status_breakdown": stats,
            "alerts": [],
            "timestamp": datetime.now().isoformat()
        }
        
        # ç”Ÿæˆå‘Šè­¦ä¿¡æ¯
        unknown_count = stats.get(ProcessStatus.UNKNOWN.value, 0)
        if unknown_count > 0:
            health_data["alerts"].append(f"å‘ç° {unknown_count} ä¸ªUNKNOWNçŠ¶æ€è¿›ç¨‹ï¼Œå»ºè®®æ£€æŸ¥")
        
        orphaned_count = stats.get(ProcessStatus.ORPHANED.value, 0)
        if orphaned_count > 0:
            health_data["alerts"].append(f"å‘ç° {orphaned_count} ä¸ªORPHANEDçŠ¶æ€è¿›ç¨‹ï¼Œå·²è‡ªåŠ¨æ¸…ç†")
        
        terminated_count = stats.get(ProcessStatus.TERMINATED.value, 0)
        if terminated_count > total_tasks * 0.3 and total_tasks > 0:
            health_data["alerts"].append(f"TERMINATEDè¿›ç¨‹å æ¯”è¿‡é«˜ ({terminated_count}/{total_tasks})")
        
        return SrvResult(
            code=200,
            msg="è·å–è¿›ç¨‹å¥åº·çŠ¶æ€æˆåŠŸ",
            data=health_data
        )
        
    except Exception as e:
        logger.error(f"è·å–è¿›ç¨‹å¥åº·çŠ¶æ€å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_process_health")
        return SrvResult(code=500, msg=f"è·å–è¿›ç¨‹å¥åº·çŠ¶æ€å¤±è´¥: {str(e)}")

@router.delete("/tasks/{task_id}")
def delete_training_task(
    task_id: str,
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """
    åˆ é™¤è®­ç»ƒä»»åŠ¡

    åŠŸèƒ½ï¼š
    1. æ£€æŸ¥ä»»åŠ¡è®¿é—®æƒé™
    2. å¦‚æœä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œå…ˆåœæ­¢å¹¶æ€æ­»è¿›ç¨‹
    3. æ›´æ–°ä»»åŠ¡å’Œè¿›ç¨‹çŠ¶æ€
    4. ä»å†…å­˜å’Œæ•°æ®åº“ä¸­åˆ é™¤ä»»åŠ¡è®°å½•
    """
    try:
        # ğŸ” éªŒè¯ä»»åŠ¡è®¿é—®æƒé™
        current_user, task_db = validate_task_access(task_id, username)

        # ğŸ—‘ï¸ è°ƒç”¨ç»Ÿä¸€è®­ç»ƒæœåŠ¡åˆ é™¤ä»»åŠ¡
        success, message = unified_training_service.delete_task(task_id)
        
        if success:
            return SrvResult(
                code=200,
                msg="ä»»åŠ¡åˆ é™¤æˆåŠŸ",
                data={
                    "task_id": task_id,
                    "deleted": True,
                    "message": message
                }
            )
        else:
            return SrvResult(code=500, msg=f"åˆ é™¤ä»»åŠ¡å¤±è´¥: {message}")
            
    except Exception as e:
        logger.error(f"åˆ é™¤è®­ç»ƒä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "delete_task")
        return SrvResult(code=500, msg=f"åˆ é™¤ä»»åŠ¡å¤±è´¥: {str(e)}")


@router.delete("/tasks/service/all")
def delete_all_service_tasks(
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """
    åˆ é™¤å½“å‰æœåŠ¡å®ä¾‹çš„æ‰€æœ‰ä»»åŠ¡

    åŠŸèƒ½ï¼š
    1. è·å–å½“å‰æœåŠ¡å®ä¾‹çš„æ‰€æœ‰ä»»åŠ¡ï¼ˆåŒ…æ‹¬è¿è¡Œä¸­çš„ä»»åŠ¡ï¼‰
    2. é€ä¸€åœæ­¢æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡å¹¶æ€æ­»è¿›ç¨‹
    3. åˆ é™¤æ‰€æœ‰ä»»åŠ¡è®°å½•ï¼ˆå†…å­˜å’Œæ•°æ®åº“ï¼‰
    4. è¿”å›åˆ é™¤ç»“æœç»Ÿè®¡

    æ³¨æ„ï¼šæ­¤æ“ä½œä¸å¯é€†ï¼Œè¯·è°¨æ…ä½¿ç”¨ï¼Œä»…é™ç®¡ç†å‘˜ä½¿ç”¨
    """
    try:
        # éªŒè¯ç®¡ç†å‘˜æƒé™ - æ­¤æ“ä½œå½±å“æ‰€æœ‰ç”¨æˆ·ï¼Œåªå…è®¸ç®¡ç†å‘˜æ‰§è¡Œ
        from bubble_rag.utils.user_manager import validate_user
        current_user = validate_user(username)
        if not current_user.get('is_admin', False):
            return SrvResult(
                code=403,
                msg=f"æƒé™ä¸è¶³ï¼šåˆ é™¤æ‰€æœ‰ä»»åŠ¡éœ€è¦ç®¡ç†å‘˜æƒé™ï¼Œå½“å‰ç”¨æˆ·: {current_user.get('username')}"
            )

        current_service_id = unified_training_service.service_instance_id
        logger.info(f"ç®¡ç†å‘˜ {current_user.get('username')} å¼€å§‹åˆ é™¤æœåŠ¡å®ä¾‹ {current_service_id} çš„æ‰€æœ‰ä»»åŠ¡")

        # è·å–å½“å‰æœåŠ¡å®ä¾‹çš„æ‰€æœ‰ä»»åŠ¡
        all_tasks = training_task_service.get_tasks_by_service_instance(current_service_id)
        if not all_tasks:
            return SrvResult(
                code=200,
                msg="å½“å‰æœåŠ¡å®ä¾‹æ²¡æœ‰ä»»åŠ¡éœ€è¦åˆ é™¤",
                data={
                    "service_instance_id": current_service_id,
                    "deleted_count": 0,
                    "failed_count": 0,
                    "tasks": []
                }
            )

        logger.info(f"ğŸ“Š å‘ç° {len(all_tasks)} ä¸ªä»»åŠ¡éœ€è¦åˆ é™¤")

        # ç»Ÿè®¡ä¿¡æ¯
        deleted_count = 0
        failed_count = 0
        deletion_results = []

        # é€ä¸€åˆ é™¤ä»»åŠ¡
        for task in all_tasks:
            try:
                task_id = task.task_id
                logger.info(f"ğŸ—‘ï¸ åˆ é™¤ä»»åŠ¡: {task_id} (çŠ¶æ€: {task.status})")

                success, message = unified_training_service.delete_task(task_id)

                if success:
                    deleted_count += 1
                    deletion_results.append({
                        "task_id": task_id,
                        "status": task.status,
                        "deleted": True,
                        "message": message
                    })
                    logger.info(f"âœ… ä»»åŠ¡ {task_id} åˆ é™¤æˆåŠŸ")
                else:
                    failed_count += 1
                    deletion_results.append({
                        "task_id": task_id,
                        "status": task.status,
                        "deleted": False,
                        "message": message
                    })
                    logger.warning(f"âŒ ä»»åŠ¡ {task_id} åˆ é™¤å¤±è´¥: {message}")

            except Exception as task_error:
                failed_count += 1
                error_msg = str(task_error)
                deletion_results.append({
                    "task_id": task.task_id if hasattr(task, 'task_id') else 'unknown',
                    "status": getattr(task, 'status', 'unknown'),
                    "deleted": False,
                    "message": f"åˆ é™¤æ—¶å‘ç”Ÿå¼‚å¸¸: {error_msg}"
                })
                logger.error(f"âŒ åˆ é™¤ä»»åŠ¡ {getattr(task, 'task_id', 'unknown')} æ—¶å‘ç”Ÿå¼‚å¸¸: {error_msg}")

        # æ„å»ºå“åº”ç»“æœ
        total_tasks = len(all_tasks)
        success_rate = (deleted_count / total_tasks * 100) if total_tasks > 0 else 100

        result_message = f"æ‰¹é‡åˆ é™¤å®Œæˆ: æˆåŠŸ {deleted_count}/{total_tasks}, å¤±è´¥ {failed_count}/{total_tasks}, æˆåŠŸç‡ {success_rate:.1f}%"
        logger.info(f"ğŸ¯ {result_message}")

        return SrvResult(
            code=200 if failed_count == 0 else 207,  # 207 Multi-Status for partial success
            msg=result_message,
            data={
                "service_instance_id": current_service_id,
                "total_tasks": total_tasks,
                "deleted_count": deleted_count,
                "failed_count": failed_count,
                "success_rate": f"{success_rate:.1f}%",
                "tasks": deletion_results
            }
        )

    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ é™¤ä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "delete_all_service_tasks")
        return SrvResult(code=500, msg=f"æ‰¹é‡åˆ é™¤ä»»åŠ¡å¤±è´¥: {str(e)}")


@router.get("/tasks/{task_id}/loss_data")
def get_task_loss_data(
    task_id: str,
    limit: Optional[int] = Query(None, description="é™åˆ¶è¿”å›çš„è®°å½•æ•°é‡ï¼Œä¸æŒ‡å®šåˆ™è¿”å›æ‰€æœ‰è®°å½•"),
    loss_type: Optional[str] = Query("all", description="æŒ‡å®šlossç±»å‹: train, eval, all"),
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """
    è·å–è®­ç»ƒä»»åŠ¡çš„å®Œæ•´lossæ•°æ®
    
    åŠŸèƒ½ï¼š
    1. ä»LossManagerè·å–æŒ‡å®šä»»åŠ¡çš„æ‰€æœ‰losså†å²æ•°æ®
    2. æ”¯æŒè¿‡æ»¤lossç±»å‹ï¼ˆtrain_loss, eval_loss, æˆ–å…¨éƒ¨ï¼‰
    3. æ”¯æŒé™åˆ¶è¿”å›è®°å½•æ•°é‡
    4. è¿”å›å®Œæ•´çš„lossæ•°æ®ä¾›å‰ç«¯ç»˜åˆ¶æ›²çº¿å›¾
    
    Args:
        task_id: è®­ç»ƒä»»åŠ¡ID
        limit: å¯é€‰ï¼Œé™åˆ¶è¿”å›æœ€è¿‘çš„Næ¡è®°å½•
        loss_type: å¯é€‰ï¼ŒæŒ‡å®šè¿”å›çš„lossç±»å‹ï¼ˆtrain, eval, allï¼Œé»˜è®¤allï¼‰
        username: ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œç”¨äºæƒé™éªŒè¯ï¼‰

    Returns:
        åŒ…å«å®Œæ•´lossæ•°æ®çš„ç»“æœ
    """
    try:
        logger.info(f"è·å–ä»»åŠ¡lossæ•°æ®: {task_id}, limit={limit}, loss_type={loss_type}")

        # éªŒè¯ä»»åŠ¡è®¿é—®æƒé™ï¼ˆåŒ…å«ç”¨æˆ·æƒé™å’ŒæœåŠ¡å®ä¾‹æƒé™ï¼‰
        current_user, task_db = validate_task_access(task_id, username)
        
        # ğŸ“Š è·å–lossæ•°æ®
        from bubble_rag.training.model_sft.utils.loss_manager import get_loss_manager
        
        # æ„é€ output_dirè·¯å¾„ï¼ˆä»ä»»åŠ¡é…ç½®ä¸­è·å–ï¼‰
        output_dir = task_db.output_dir or "/tmp/training_output"
        
        # è·å–LossManagerå®ä¾‹
        loss_manager = get_loss_manager(output_dir, task_id)
        
        # è·å–æ‰€æœ‰losså†å²è®°å½•
        all_records = loss_manager.get_loss_history(limit=limit)
        
        # æ ¹æ®loss_typeè¿‡æ»¤æ•°æ®
        filtered_records = []
        for record in all_records:
            filtered_record = {
                "step": record.get("step"),
                "timestamp": record.get("timestamp"),
                "epoch": record.get("epoch")
            }
            
            # æ ¹æ®loss_typeæ·»åŠ ç›¸åº”çš„lossæ•°æ®
            if loss_type in ["all", "train"] and "train_loss" in record:
                filtered_record["train_loss"] = record["train_loss"]
            if loss_type in ["all", "eval"] and "eval_loss" in record:
                filtered_record["eval_loss"] = record["eval_loss"]
            if loss_type in ["all"] and "loss" in record:
                # å…¼å®¹æ€§ï¼šæœ‰äº›è®°å½•å¯èƒ½åªæœ‰'loss'å­—æ®µ
                filtered_record["loss"] = record["loss"]
            
            # åªä¿ç•™æœ‰lossæ•°æ®çš„è®°å½•
            has_loss_data = any(key in filtered_record for key in ["train_loss", "eval_loss", "loss"])
            if has_loss_data:
                filtered_records.append(filtered_record)
        
        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            "task_id": task_id,
            "total_records": len(filtered_records),
            "loss_type": loss_type,
            "loss_data": filtered_records,
            "task_status": task_db.status,
            "last_updated": filtered_records[-1]["timestamp"] if filtered_records else None
        }
        
        logger.info(f"âœ… æˆåŠŸè·å–ä»»åŠ¡ {task_id} çš„lossæ•°æ®: {len(filtered_records)} æ¡è®°å½•")
        
        return SrvResult(
            code=200,
            msg="è·å–lossæ•°æ®æˆåŠŸ",
            data=response_data
        )
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡lossæ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_task_loss_data")
        return SrvResult(code=500, msg=f"è·å–lossæ•°æ®å¤±è´¥: {str(e)}")


@router.get("/tasks/{task_id}/eval_results")
def get_task_eval_results(
    task_id: str,
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """
    è·å–è®­ç»ƒä»»åŠ¡çš„è¯„ä¼°ç»“æœ
    
    åŠŸèƒ½ï¼š
    1. è·å–æŒ‡å®šä»»åŠ¡çš„åŸºçº¿è¯„ä¼°ç»“æœï¼ˆbase_eval_resultsï¼‰å’Œæœ€ç»ˆè¯„ä¼°ç»“æœï¼ˆfinal_eval_resultsï¼‰
    2. åŒ…å«æ‰€æœ‰æ•°æ®é›†ï¼ˆtrain/eval/testï¼‰çš„è¯„ä¼°æ•°æ®
    3. è¿”å›è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€ä½³losså€¼ï¼ˆbest_train_loss, best_eval_lossï¼‰
    4. è¿”å›å®Œæ•´çš„è¯„ä¼°ç»“æœä¾›å‰ç«¯å±•ç¤º
    
    Args:
        task_id: è®­ç»ƒä»»åŠ¡ID
        username: ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œç”¨äºæƒé™éªŒè¯ï¼‰

    Returns:
        åŒ…å«åŸºçº¿å’Œæœ€ç»ˆè¯„ä¼°ç»“æœä»¥åŠæœ€ä½³losså€¼çš„æ•°æ®
    """
    try:
        logger.info(f"è·å–ä»»åŠ¡è¯„ä¼°ç»“æœ: {task_id}")

        # éªŒè¯ä»»åŠ¡è®¿é—®æƒé™ï¼ˆåŒ…å«ç”¨æˆ·æƒé™å’ŒæœåŠ¡å®ä¾‹æƒé™ï¼‰
        current_user, task_db = validate_task_access(task_id, username)
        
        # ğŸ“Š è·å–æ•°æ®é›†è¯„ä¼°ç»“æœ
        from bubble_rag.training.mysql_service.service.training_dataset_service import TrainingDatasetService
        
        datasets_with_results = TrainingDatasetService.get_datasets_with_eval_results_by_task(task_id)
        
        # ğŸ“ˆ è·å–è®­ç»ƒæŒ‡æ ‡ï¼ˆåŒ…å«best_train_losså’Œbest_eval_lossï¼‰
        training_metrics = None
        try:
            from bubble_rag.training.model_sft.utils.loss_manager import get_loss_manager
            output_dir = task_db.output_dir or "/tmp/training_output"
            loss_manager = get_loss_manager(output_dir, task_id)
            training_metrics = loss_manager.get_training_metrics()
        except Exception as e:
            logger.warning(f"è·å–è®­ç»ƒæŒ‡æ ‡å¤±è´¥: {e}")
        
        # æŒ‰æ•°æ®é›†ç±»å‹åˆ†ç»„æ•´ç†ç»“æœ
        eval_results = {
            "train": [],
            "eval": [],
            "test": []
        }
        
        total_base_results = 0
        total_final_results = 0
        
        for dataset in datasets_with_results:
            split_type = dataset["split_type"]
            dataset_result = {
                "dataset_id": dataset["id"],
                "dataset_name": dataset["dataset_name"],
                "data_source_id": dataset["data_source_id"],
                "base_eval_results": dataset["base_eval_results"],
                "final_eval_results": dataset["final_eval_results"],
                "evaluation_status": dataset["evaluation_status"],
                "configured_sample_size": dataset["configured_sample_size"],
                "last_updated": dataset["update_time"]
            }
            
            # ç»Ÿè®¡è¯„ä¼°ç»“æœæ•°é‡
            if dataset["base_eval_results"]:
                total_base_results += 1
            if dataset["final_eval_results"]:
                total_final_results += 1
            
            if split_type in eval_results:
                eval_results[split_type].append(dataset_result)
        
        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            "task_id": task_id,
            "task_status": task_db.status,
            "task_name": task_db.task_name,
            "train_type": task_db.train_type,
            "best_train_loss": training_metrics.get("best_train_loss") if training_metrics else None,
            "best_eval_loss": training_metrics.get("best_eval_loss") if training_metrics else None,
            "evaluation_summary": {
                "total_datasets": len(datasets_with_results),
                "datasets_with_base_results": total_base_results,
                "datasets_with_final_results": total_final_results,
                "train_datasets": len(eval_results["train"]),
                "eval_datasets": len(eval_results["eval"]),
                "test_datasets": len(eval_results["test"])
            },
            "eval_results": eval_results
        }
        
        logger.info(f"âœ… æˆåŠŸè·å–ä»»åŠ¡ {task_id} çš„è¯„ä¼°ç»“æœ: {len(datasets_with_results)} ä¸ªæ•°æ®é›†")
        
        return SrvResult(
            code=200,
            msg="è·å–è¯„ä¼°ç»“æœæˆåŠŸ",
            data=response_data
        )
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è¯„ä¼°ç»“æœå¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_task_eval_results")
        return SrvResult(code=500, msg=f"è·å–è¯„ä¼°ç»“æœå¤±è´¥: {str(e)}")


def _enhance_loss_data_with_metadata(task_id: str, raw_loss_data: List[Dict]) -> List[Dict]:
    """
    å°†lossæ•°æ®ä¸­çš„source_idæ˜ å°„å›æ•°æ®é›†åç§°ï¼Œå¹¶å¢å¼ºè¯„ä¼°å…ƒæ•°æ®

    Args:
        task_id: è®­ç»ƒä»»åŠ¡ID
        raw_loss_data: åŸå§‹lossæ•°æ®åˆ—è¡¨

    Returns:
        åº”ç”¨æ˜ å°„å’Œå…ƒæ•°æ®å¢å¼ºåçš„lossæ•°æ®åˆ—è¡¨
    """
    try:
        from bubble_rag.training.model_sft.utils.loss_manager import get_loss_manager

        # å…ˆå°è¯•ä»lossæ–‡ä»¶çš„å…ƒæ•°æ®ä¸­è·å–æ˜ å°„ï¼ˆæ€§èƒ½æ›´å¥½ï¼‰
        mapping = {}
        try:
            # è·å–taskçš„output_dir
            from bubble_rag.training.mysql_service.service.training_task_service import training_task_service
            task_db = training_task_service.get_training_task(task_id)
            if task_db and task_db.output_dir:
                loss_manager = get_loss_manager(task_db.output_dir, task_id)
                metadata = loss_manager.get_metadata()
                data_source_mapping = metadata.get("data_source_mapping", {})

                # è½¬æ¢æ˜ å°„æ–¹å‘ï¼š{dataset_name: source_id} â†’ {source_id: dataset_name}
                mapping = {v: k for k, v in data_source_mapping.items()}
                logger.debug(f"ä»lossæ–‡ä»¶è·å–source_idæ˜ å°„: {mapping}")
        except Exception as file_error:
            logger.debug(f"ä»lossæ–‡ä»¶è·å–æ˜ å°„å¤±è´¥: {file_error}")

        # å¦‚æœlossæ–‡ä»¶ä¸­æ²¡æœ‰æ˜ å°„ï¼Œå›é€€åˆ°æ•°æ®åº“æŸ¥è¯¢
        if not mapping:
            try:
                from bubble_rag.training.mysql_service.service.training_dataset_service import TrainingDatasetService
                mapping = TrainingDatasetService.get_source_id_to_dataset_mapping(task_id)
                logger.debug(f"ä»æ•°æ®åº“è·å–source_idæ˜ å°„: {mapping}")

                # ğŸ†• å°†æ•°æ®åº“æŸ¥è¯¢ç»“æœç¼“å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œä¸‹æ¬¡å°±ä¸ç”¨å†æŸ¥æ•°æ®åº“äº†
                if mapping and task_db and task_db.output_dir:
                    try:
                        loss_manager = get_loss_manager(task_db.output_dir, task_id)

                        # å°è¯•ä»æ•°æ®åº“è·å–æ›´å¤šä»»åŠ¡ä¿¡æ¯æ¥ä¸°å¯Œç¼“å­˜
                        cache_metadata = {
                            "data_source_mapping": {v: k for k, v in mapping.items()},  # è½¬æ¢å›åŸå§‹æ ¼å¼
                            "cached_from_database": True,
                            "cache_created_at": datetime.now().isoformat()
                        }

                        # å°è¯•ä»ä»»åŠ¡è®°å½•ä¸­è·å–æ›´å¤šä¿¡æ¯
                        try:
                            if hasattr(task_db, 'config') and task_db.config:
                                import json
                                task_config = json.loads(task_db.config) if isinstance(task_db.config, str) else task_db.config

                                # æ·»åŠ è®­ç»ƒç±»å‹å’Œé…ç½®ä¿¡æ¯
                                if 'train_type' in task_config:
                                    cache_metadata["train_type"] = task_config['train_type']
                                if 'model_config' in task_config:
                                    cache_metadata["model_config"] = task_config['model_config']
                                if 'data_config' in task_config:
                                    cache_metadata["data_config"] = task_config['data_config']

                                logger.debug(f"ä»ä»»åŠ¡é…ç½®ä¸­è·å–é¢å¤–ä¿¡æ¯: {list(cache_metadata.keys())}")
                        except Exception as config_error:
                            logger.debug(f"è·å–ä»»åŠ¡é…ç½®ä¿¡æ¯å¤±è´¥: {config_error}")

                        loss_manager.save_metadata(cache_metadata)
                        logger.info(f"âœ… å·²ç¼“å­˜æ•°æ®åº“æ˜ å°„åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œä»»åŠ¡: {task_id}")
                    except Exception as cache_error:
                        logger.warning(f"ç¼“å­˜æ˜ å°„åˆ°æœ¬åœ°å¤±è´¥: {cache_error}")

            except Exception as db_error:
                logger.warning(f"ä»æ•°æ®åº“è·å–æ˜ å°„ä¹Ÿå¤±è´¥: {db_error}")

        if not mapping:
            logger.warning(f"ä»»åŠ¡ {task_id} æ²¡æœ‰æ‰¾åˆ°source_idæ˜ å°„ï¼Œè¿”å›åŸå§‹æ•°æ®")
            return raw_loss_data

        # å¯¹æ¯æ¡è®°å½•åº”ç”¨æ˜ å°„
        mapped_data = []
        for record in raw_loss_data:
            mapped_record = record.copy()

            # æŸ¥æ‰¾éœ€è¦æ˜ å°„çš„evalæŒ‡æ ‡
            keys_to_update = {}
            for key, value in record.items():
                if key.startswith('eval_'):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯source_idæ ¼å¼: eval_1_loss, eval_2_pearsonç­‰
                    parts = key[5:].split('_')  # å»æ‰'eval_'å‰ç¼€
                    if len(parts) >= 2 and parts[0].isdigit():
                        source_id = parts[0]
                        metric_name = '_'.join(parts[1:])  # é‡æ–°ç»„åˆæŒ‡æ ‡å

                        # æŸ¥æ‰¾å¯¹åº”çš„æ•°æ®é›†åç§°
                        if source_id in mapping:
                            dataset_name = mapping[source_id]
                            new_key = f"eval_{dataset_name}_{metric_name}"
                            keys_to_update[key] = new_key

            # åº”ç”¨æ˜ å°„æ›´æ–°
            for old_key, new_key in keys_to_update.items():
                mapped_record[new_key] = mapped_record.pop(old_key)
                logger.debug(f"æ˜ å°„æŒ‡æ ‡å: {old_key} â†’ {new_key}")

            mapped_data.append(mapped_record)

        # ğŸ†• åº”ç”¨å…ƒæ•°æ®å¢å¼º
        try:
            from bubble_rag.training.model_sft.utils.evaluation_result import get_evaluation_result_processor
            processor = get_evaluation_result_processor()
            enhanced_data = processor.enhance_loss_data_with_metadata(mapped_data, mapping)
            logger.info(f"å®Œæˆlossæ•°æ®åå‘æ˜ å°„å’Œå…ƒæ•°æ®å¢å¼ºï¼Œå¤„ç†äº† {len(enhanced_data)} æ¡è®°å½•")
            return enhanced_data
        except Exception as enhance_error:
            logger.warning(f"å…ƒæ•°æ®å¢å¼ºå¤±è´¥: {enhance_error}ï¼Œè¿”å›æ˜ å°„åçš„æ•°æ®")
            return mapped_data

    except Exception as e:
        logger.warning(f"åº”ç”¨åå‘æ˜ å°„å’Œå…ƒæ•°æ®å¢å¼ºå¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹æ•°æ®")
        return raw_loss_data


# ==================== ğŸ”„ ä¸€é”®é‡å¯ä»»åŠ¡æ¥å£ ====================

@router.get("/tasks/{task_id}/restart_config")
def get_task_restart_config(
    task_id: str,
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """
    è·å–ä»»åŠ¡é‡å¯é…ç½®

    é‡å¯è®­ç»ƒçš„æ ‡å‡†æµç¨‹ï¼š
    1. ç”¨æˆ·ç‚¹å‡»"é‡å¯ä»»åŠ¡" -> è°ƒç”¨æ­¤æ¥å£è·å–åŸä»»åŠ¡é…ç½®
    2. å‰ç«¯è·³è½¬åˆ°è®­ç»ƒé…ç½®é¡µé¢ï¼Œç”¨è¿”å›çš„å‚æ•°é¢„å¡«å……è¡¨å•
    3. ç”¨æˆ·è°ƒæ•´å‚æ•°åç‚¹å‡»"å¼€å§‹è®­ç»ƒ" -> è°ƒç”¨ POST /start_training

    è¿™æ ·é¿å…äº†æå‰åˆ›å»ºä»»åŠ¡å…¥åº“ï¼Œåªæœ‰ç”¨æˆ·çœŸæ­£å¯åŠ¨æ—¶æ‰åˆ›å»ºä»»åŠ¡
    """
    try:
        # éªŒè¯ä»»åŠ¡è®¿é—®æƒé™ï¼ˆåŒ…å«ç”¨æˆ·æƒé™å’ŒæœåŠ¡å®ä¾‹æƒé™ï¼‰
        current_user, task = validate_task_access(task_id, username)

        # æ„å»ºé‡å¯é…ç½®
        restart_config = {
            "base_task_id": task_id,
            "original_task_name": task.task_name,
            "original_status": task.status,
            "original_created_at": task.created_at.isoformat() if task.created_at else None,

            # æ ¸å¿ƒè®­ç»ƒé…ç½®
            "train_type": task.train_type,
            "model_name_or_path": task.model_name_or_path,
            "dataset_name_or_path": task.dataset_name_or_path,
            "HF_subset": task.HF_subset,
            "device": task.device,

            # è®­ç»ƒå‚æ•°
            "training_params": json.loads(task.training_params) if task.training_params else {},

            # å»ºè®®çš„æ–°ä»»åŠ¡åç§°ï¼ˆåŸºäºé‡å¯è®¡æ•°ï¼‰
            "suggested_task_name": f"{task.task_name}_restart_{getattr(task, 'restart_count', 0) + 1}" if task.task_name else f"restart_{getattr(task, 'restart_count', 0) + 1}"
        }

        # å¦‚æœæ˜¯å¤±è´¥ä»»åŠ¡ï¼Œæä¾›å¤±è´¥ä¿¡æ¯ç”¨äºå‚è€ƒ
        if task.status == TrainingStatus.FAILED.value and task.error_message:
            restart_config["failure_info"] = {
                "error_message": task.error_message,
                "failed_at": task.completed_at.isoformat() if task.completed_at else None
            }

        return SrvResult(
            code=200,
            msg="è·å–é‡å¯é…ç½®æˆåŠŸ",
            data=restart_config
        )

    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡é‡å¯é…ç½®å¤±è´¥: {str(e)}", exc_info=True)
        return SrvResult(code=500, msg=f"è·å–é‡å¯é…ç½®å¤±è´¥: {str(e)}")


@router.get("/tasks/{task_id}/restart_history")
def get_task_restart_history(
    task_id: str,
    username: Optional[str] = Query(None, description="ç”¨æˆ·åï¼ˆå¯é€‰ï¼Œä¸ä¼ åˆ™é»˜è®¤ä¸ºadminç”¨æˆ·ï¼‰")
):
    """
    è·å–ä»»åŠ¡é‡å¯å†å²
    æŸ¥æ‰¾åŸºäºæ­¤ä»»åŠ¡é‡å¯çš„æ‰€æœ‰ä»»åŠ¡
    """
    try:
        # éªŒè¯ä»»åŠ¡è®¿é—®æƒé™ï¼ˆåŒ…å«ç”¨æˆ·æƒé™å’ŒæœåŠ¡å®ä¾‹æƒé™ï¼‰
        current_user, base_task = validate_task_access(task_id, username)

        # ä½¿ç”¨æ•°æ®åº“å­—æ®µç›´æ¥æŸ¥è¯¢é‡å¯ä»»åŠ¡
        restart_tasks_db = training_task_service.get_restart_tasks_by_base_id(task_id)
        restart_tasks = []

        for task in restart_tasks_db:
            task_dict = convert_task_to_dict(task)
            restart_info = {
                "task_id": task_dict["task_id"],
                "task_name": task_dict["task_name"],
                "status": task_dict["status"],
                "progress": task_dict["progress"],
                "created_at": task_dict["created_at"],
                "started_at": task_dict["started_at"],
                "completed_at": task_dict["completed_at"],
                "base_task_id": getattr(task, 'base_task_id', None)
            }
            restart_tasks.append(restart_info)

        # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        restart_tasks.sort(key=lambda x: x["created_at"], reverse=True)

        return SrvResult(
            code=200,
            msg="è·å–é‡å¯å†å²æˆåŠŸ",
            data={
                "base_task_id": task_id,
                "base_task_name": base_task.task_name,
                "restart_count_from_db": getattr(base_task, 'restart_count', 0),
                "restart_count_found": len(restart_tasks),
                "restart_tasks": restart_tasks
            }
        )

    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡é‡å¯å†å²å¤±è´¥: {str(e)}", exc_info=True)
        return SrvResult(code=500, msg=f"è·å–é‡å¯å†å²å¤±è´¥: {str(e)}")



