"""
ç»Ÿä¸€è®­ç»ƒAPIè·¯ç”±
æ”¯æŒä¸²è¡Œ(serial)å’Œå¹¶è¡Œ(parallel)è®­ç»ƒæ¨¡å¼çš„ç»Ÿä¸€æ¥å£
"""
from fastapi import APIRouter, Query
from pydantic import BaseModel, Field, ValidationError
from typing import Optional, Dict, Any, List
from datetime import datetime

from bubble_rag.entity.query.response_model import SrvResult
from bubble_rag.training.model_sft.services.unified_training_service import unified_training_service
from bubble_rag.training.model_sft.services.dataset_service import dataset_service
from bubble_rag.training.model_sft.services.model_service import model_service
from bubble_rag.training.model_sft.services.config_service import config_service
from bubble_rag.training.model_sft.models.training_task import TrainingTaskCreateRequest
from bubble_rag.training.model_sft.models.unified_config import UnifiedTrainingConfig
from bubble_rag.training.mysql_service.service.training_task_service import training_task_service
from bubble_rag.training.mysql_service.service.training_dataset_service import TrainingDatasetService
from bubble_rag.training.model_sft.enums import TrainingStatus
from bubble_rag.training.model_sft.enums.training_task_enums import ProcessStatus
from bubble_rag.training.model_sft.utils.error_handler import handle_api_error
from bubble_rag.training.model_sft.utils.gpu_resource_manager import gpu_resource_manager
import logging

logger = logging.getLogger(__name__)

router = APIRouter()

class StartTrainingRequest(UnifiedTrainingConfig):
    """å¯åŠ¨è®­ç»ƒè¯·æ±‚æ¨¡å‹ï¼ˆç»§æ‰¿ç»Ÿä¸€é…ç½®ï¼‰"""
    training_mode: Optional[str] = Field(default="parallel", description="è®­ç»ƒæ¨¡å¼: serial(ä¸²è¡Œ) æˆ– parallel(å¹¶è¡Œ)")

# å…¬å…±æ•°æ®è½¬æ¢å‡½æ•°
def convert_task_to_dict(task) -> Dict[str, Any]:
    """å°†è®­ç»ƒä»»åŠ¡å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
    task_data = {
        "task_id": task.task_id,
        "task_name": task.task_name,
        "description": task.description,
        "train_type": task.train_type,
        "dataset_name_or_path": task.dataset_name_or_path,
        "output_dir": task.output_dir,
        "device": task.device,
        "status": task.status,
        "progress": task.progress,
        "created_at": task.created_at.isoformat() if task.created_at else None,
        "started_at": task.started_at.isoformat() if task.started_at else None,
        "completed_at": task.completed_at.isoformat() if task.completed_at else None,
        "duration_seconds": task.duration_seconds,
        "final_model_path": task.final_model_path,
        "error_message": task.error_message,
        "training_params": task.training_params or {}
    }
    return task_data

@router.post("/start_training")
def start_training(request: StartTrainingRequest):
    """
    å¯åŠ¨è®­ç»ƒä»»åŠ¡ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
    
    æ”¯æŒä¸¤ç§è®­ç»ƒæ¨¡å¼ï¼š
    - serial: ä¸²è¡Œè®­ç»ƒï¼Œä¸€æ¬¡åªèƒ½è¿è¡Œä¸€ä¸ªä»»åŠ¡
    - parallel: å¹¶è¡Œè®­ç»ƒï¼Œå¯åŒæ—¶è¿è¡Œå¤šä¸ªä»»åŠ¡
    """
    try:
        # æ£€æŸ¥æœåŠ¡å®ä¾‹ID - ç¡®ä¿æœåŠ¡éš”ç¦»åŠŸèƒ½æ­£å¸¸
        if not unified_training_service.service_instance_id:
            return SrvResult(
                code=500,
                msg="âŒ æœåŠ¡éš”ç¦»åŠŸèƒ½å¼‚å¸¸ï¼šæœåŠ¡å®ä¾‹IDä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºè®­ç»ƒä»»åŠ¡ï¼"
            )
        # æå–è®­ç»ƒæ¨¡å¼
        training_mode = request.training_mode or "parallel"
        
        # åˆ†ç¦»è®­ç»ƒæ¨¡å¼å’Œå…¶ä»–å‚æ•°
        request_data = request.model_dump(exclude={"training_mode"}, exclude_unset=True)
        
        # åˆ†ç¦»æ ¸å¿ƒä»»åŠ¡å‚æ•°å’Œè®­ç»ƒå‚æ•°
        core_task_fields = {
            "task_name", "description", "train_type", "model_name_or_path", 
            "dataset_name_or_path", "HF_subset", "output_dir", "device"
        }
        
        core_params = {k: v for k, v in request_data.items() if k in core_task_fields}
        training_params = {k: v for k, v in request_data.items() if k not in core_task_fields}
        
        # åˆå¹¶ç”¨æˆ·æä¾›çš„training_params
        if request.training_params:
            training_params.update(request.training_params)
        
        # ç›´æ¥ä½¿ç”¨Pydanticçš„TrainingParametersè¿›è¡ŒéªŒè¯
        try:
            from bubble_rag.training.model_sft.models.training_parameters import TrainingParameters
            validated_training_params = TrainingParameters(**training_params)
            training_params_dict = validated_training_params.model_dump(exclude_unset=True)
            
            logger.info(f"è®­ç»ƒå‚æ•°éªŒè¯æˆåŠŸï¼Œå…± {len(training_params_dict)} ä¸ªå‚æ•°")
            
        except Exception as e:
            logger.error(f"è®­ç»ƒå‚æ•°éªŒè¯å¤±è´¥: {e}")
            # å¦‚æœæ˜¯ Pydantic éªŒè¯é”™è¯¯ï¼Œæä¾›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
            if hasattr(e, 'errors'):
                error_details = []
                for error in e.errors():
                    field = '.'.join(str(loc) for loc in error['loc'])
                    error_details.append(f"{field}: {error['msg']}")
                error_msg = "å‚æ•°éªŒè¯å¤±è´¥: " + "; ".join(error_details)
            else:
                error_msg = f"è®­ç»ƒå‚æ•°éªŒè¯å¤±è´¥: {e}"
            
            return SrvResult(code=400, msg=error_msg)
        
        # åˆ›å»ºè®­ç»ƒä»»åŠ¡è¯·æ±‚
        training_request = TrainingTaskCreateRequest(
            task_name=core_params.get("task_name"),
            description=core_params.get("description"),
            train_type=core_params["train_type"],
            model_name_or_path=core_params["model_name_or_path"],
            dataset_name_or_path=core_params["dataset_name_or_path"],
            HF_subset=core_params.get("HF_subset"),
            output_dir=core_params.get("output_dir"),
            device=core_params.get("device", "auto"),
            training_params=training_params_dict
        )
        
        logger.info(f"å¯åŠ¨è®­ç»ƒä»»åŠ¡è¯·æ±‚ï¼Œæ¨¡å¼: {training_mode}")
        logger.info(f"è®­ç»ƒå‚æ•°: {list(training_request.training_params.keys())}")
        
        # ä½¿ç”¨ç»Ÿä¸€è®­ç»ƒæœåŠ¡å¯åŠ¨ä»»åŠ¡
        task = unified_training_service.start_training(training_request, training_mode=training_mode)
        
        # æ„å»ºå“åº”æ•°æ®
        response_data = convert_task_to_dict(task)
        response_data["training_mode"] = training_mode
        
        return SrvResult(
            code=200,
            msg=f"è®­ç»ƒä»»åŠ¡å¯åŠ¨æˆåŠŸ (æ¨¡å¼: {training_mode})",
            data=response_data
        )
        
    except ValidationError as ve:
        logger.error(f"è¯·æ±‚å‚æ•°éªŒè¯å¤±è´¥: {ve}")
        error_details = []
        for error in ve.errors():
            field = '.'.join(str(loc) for loc in error['loc']) 
            error_details.append(f"{field}: {error['msg']}")
        return SrvResult(
            code=422,
            msg=f"è¯·æ±‚å‚æ•°éªŒè¯å¤±è´¥: {'; '.join(error_details)}"
        )
    except Exception as e:
        logger.error(f"å¯åŠ¨è®­ç»ƒä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "start_training")  # è®°å½•é”™è¯¯åˆ°æ—¥å¿—
        return SrvResult(
            code=500,
            msg=f"å¯åŠ¨è®­ç»ƒä»»åŠ¡å¤±è´¥: {str(e)}"
        )

@router.post("/stop_training")
def stop_training(task_id: str = Query(..., description="ä»»åŠ¡ID")):
    """åœæ­¢è®­ç»ƒä»»åŠ¡"""
    try:
        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šéªŒè¯ä»»åŠ¡æ˜¯å¦å±äºå½“å‰æœåŠ¡å®ä¾‹
        task_db = training_task_service.get_training_task(task_id)
        if not task_db:
            return SrvResult(code=404, msg=f"æœªæ‰¾åˆ°ä»»åŠ¡: {task_id}")
        
        # æ£€æŸ¥æœåŠ¡å®ä¾‹å½’å±æƒé™
        current_service_id = unified_training_service.service_instance_id
        if task_db.service_instance_id != current_service_id:
            logger.warning(f"ğŸš« è·¨æœåŠ¡åœæ­¢ä»»åŠ¡è¢«æ‹’ç»: ä»»åŠ¡ {task_id} å±äºæœåŠ¡ {task_db.service_instance_id}ï¼Œå½“å‰æœåŠ¡ {current_service_id}")
            return SrvResult(
                code=403, 
                msg=f"æƒé™ä¸è¶³ï¼šä»»åŠ¡å±äºå…¶ä»–æœåŠ¡å®ä¾‹ï¼Œæ— æ³•åœæ­¢"
            )
        
        success = unified_training_service.stop_training(task_id)
        
        if success:
            return SrvResult(
                code=200,
                msg="è®­ç»ƒä»»åŠ¡å·²åœæ­¢",
                data={"task_id": task_id, "stopped": True}
            )
        else:
            return SrvResult(code=500, msg=f"åœæ­¢è®­ç»ƒä»»åŠ¡å¤±è´¥: {task_id}")
            
    except Exception as e:
        logger.error(f"åœæ­¢è®­ç»ƒä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "stop_training")
        return SrvResult(code=500, msg=f"åœæ­¢è®­ç»ƒä»»åŠ¡å¤±è´¥: {str(e)}")

@router.get("/tasks/{task_id}")
def get_task_detail(task_id: str):
    """è·å–ä»»åŠ¡è¯¦æƒ…ï¼ˆä»æ•°æ®åº“è·å–å®Œæ•´ä¿¡æ¯ï¼ŒåŒ…å«æŒä¹…åŒ–çš„è¿›åº¦çŠ¶æ€ï¼‰"""
    try:
        # ğŸ”§ ä»æ•°æ®åº“è·å–å®Œæ•´ä»»åŠ¡ä¿¡æ¯ï¼ˆæŒä¹…åŒ–æ•°æ®ï¼‰
        task_db = training_task_service.get_training_task(task_id)
        
        if not task_db:
            return SrvResult(code=404, msg=f"æœªæ‰¾åˆ°ä»»åŠ¡: {task_id}")
        
        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šéªŒè¯ä»»åŠ¡æ˜¯å¦å±äºå½“å‰æœåŠ¡å®ä¾‹
        current_service_id = unified_training_service.service_instance_id
        if task_db.service_instance_id != current_service_id:
            logger.warning(f"ğŸš« è·¨æœåŠ¡æŸ¥è¯¢ä»»åŠ¡è¢«æ‹’ç»: ä»»åŠ¡ {task_id} å±äºæœåŠ¡ {task_db.service_instance_id}ï¼Œå½“å‰æœåŠ¡ {current_service_id}")
            return SrvResult(
                code=403,
                msg=f"æƒé™ä¸è¶³ï¼šä»»åŠ¡å±äºå…¶ä»–æœåŠ¡å®ä¾‹ï¼Œæ— æ³•æŸ¥çœ‹è¯¦æƒ…"
            )
        
        # è·å–è¿è¡Œè¿›ç¨‹ä¿¡æ¯ï¼ˆå®æ—¶çŠ¶æ€ï¼‰
        running_processes = unified_training_service.get_running_processes()
        is_running = task_id in running_processes
        process_info = running_processes.get(task_id, {}).get('process_info', {}) if is_running else None
        
        # ğŸ”§ ä¿®å¤è¿›åº¦æ˜¾ç¤ºé—®é¢˜ï¼šä½¿ç”¨ä¸progressæ¥å£ç›¸åŒçš„æ··åˆæ•°æ®æºç­–ç•¥
        # ç¡®ä¿ä¸¤ä¸ªæ¥å£è¿”å›ä¸€è‡´çš„è¿›åº¦æ•°æ®
        real_time_progress = task_db.progress or 0.0  # ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“è¿›åº¦ï¼ˆå¯é ï¼‰
        memory_progress = 0.0
        sync_status = "unknown"
        
        try:
            from bubble_rag.training.model_sft.services.task_manager import task_manager
            memory_task = task_manager.get_task(task_id)
            if memory_task:
                memory_progress = memory_task.progress
                # è®¡ç®—åŒæ­¥çŠ¶æ€
                sync_diff = abs(real_time_progress - memory_progress)
                sync_status = "synced" if sync_diff < 1 else "out_of_sync"
                logger.debug(f"ä»»åŠ¡è¯¦æƒ…API: æ•°æ®åº“è¿›åº¦={real_time_progress}%, å†…å­˜è¿›åº¦={memory_progress}%, åŒæ­¥çŠ¶æ€={sync_status}")
            else:
                logger.debug(f"å†…å­˜ä¸­æœªæ‰¾åˆ°ä»»åŠ¡ï¼Œä½¿ç”¨æ•°æ®åº“è¿›åº¦: {real_time_progress}%")
                sync_status = "memory_not_found"
        except Exception as e:
            logger.warning(f"è·å–å†…å­˜ä»»åŠ¡å¤±è´¥ï¼Œä½¿ç”¨æ•°æ®åº“è¿›åº¦: {e}")
            sync_status = "memory_error"
            
        # ç¡®ä¿è¿›åº¦åœ¨åˆç†èŒƒå›´å†…
        real_time_progress = max(0.0, min(100.0, real_time_progress))
        
        # ğŸ”§ è°ƒè¯•æ—¥å¿—ï¼šè¾“å‡ºæœ€ç»ˆä½¿ç”¨çš„è¿›åº¦å€¼
        logger.info(f"ğŸ” ä»»åŠ¡è¯¦æƒ…APIè°ƒè¯•: task_id={task_id}, æ•°æ®åº“åŸå§‹è¿›åº¦={task_db.progress}, æœ€ç»ˆè¿›åº¦={real_time_progress}, åŒæ­¥çŠ¶æ€={sync_status}")
        
        # è®¡ç®—é¢„ä¼°å‰©ä½™æ—¶é—´ï¼ˆä½¿ç”¨å®æ—¶è¿›åº¦ï¼‰
        estimated_time = None
        if task_db.started_at and real_time_progress > 0 and real_time_progress < 100:
            elapsed = (datetime.now() - task_db.started_at).total_seconds()
            estimated_total = elapsed / (real_time_progress / 100)
            estimated_time = max(0, estimated_total - elapsed)
        
        # ğŸ“Š è¯¦ç»†ä»»åŠ¡ä¿¡æ¯
        task_detail = {
            # åŸºç¡€ä¿¡æ¯
            "task_id": task_db.task_id,
            "task_name": task_db.task_name,
            "description": task_db.description,
            "train_type": task_db.train_type,
            "model_name_or_path": task_db.model_name_or_path,
            "dataset_name_or_path": task_db.dataset_name_or_path,
            "HF_subset": getattr(task_db, 'HF_subset', None),  # æ–°å¢å­—æ®µ
            "output_dir": task_db.output_dir,
            "device": task_db.device,
            
            # çŠ¶æ€ä¿¡æ¯
            "status": task_db.status,
            "progress": real_time_progress,
            "is_running": is_running,
            
            # æ—¶é—´ä¿¡æ¯
            "created_at": task_db.created_at.isoformat() if task_db.created_at else None,
            "started_at": task_db.started_at.isoformat() if task_db.started_at else None,
            "completed_at": task_db.completed_at.isoformat() if task_db.completed_at else None,
            "estimated_time_remaining": estimated_time,
            
            # ç»“æœä¿¡æ¯
            "final_model_path": task_db.final_model_path,
            "error_message": task_db.error_message,
            
            # è¿›ç¨‹ä¿¡æ¯ï¼ˆå¦‚æœæ­£åœ¨è¿è¡Œï¼‰
            "process_info": process_info,
            
            # è®­ç»ƒå‚æ•°
            "training_params": task_db.training_params
        }
        
        return SrvResult(code=200, msg="è·å–ä»»åŠ¡è¯¦æƒ…æˆåŠŸ", data=task_detail)
            
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_task_detail")
        return SrvResult(code=500, msg=f"è·å–ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {str(e)}")

@router.get("/tasks/{task_id}/datasets")
def get_task_datasets(task_id: str):
    """è·å–ä»»åŠ¡çš„è®­ç»ƒæ•°æ®é›†ä¿¡æ¯ - ä»…å½“å‰æœåŠ¡å®ä¾‹"""
    try:
        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šå…ˆéªŒè¯ä»»åŠ¡æ˜¯å¦å±äºå½“å‰æœåŠ¡å®ä¾‹
        task_db = training_task_service.get_training_task(task_id)
        if not task_db:
            return SrvResult(code=404, msg=f"æœªæ‰¾åˆ°ä»»åŠ¡: {task_id}")
        
        current_service_id = unified_training_service.service_instance_id
        if not current_service_id:
            return SrvResult(
                code=500,
                msg="âŒ æœåŠ¡éš”ç¦»åŠŸèƒ½å¼‚å¸¸ï¼šæœåŠ¡å®ä¾‹IDä¸ºç©ºï¼Œæ— æ³•è·å–æ•°æ®é›†ä¿¡æ¯ï¼"
            )
        
        if task_db.service_instance_id != current_service_id:
            logger.warning(f"ğŸš« è·¨æœåŠ¡æŸ¥è¯¢æ•°æ®é›†è¢«æ‹’ç»: ä»»åŠ¡ {task_id} å±äºæœåŠ¡ {task_db.service_instance_id}ï¼Œå½“å‰æœåŠ¡ {current_service_id}")
            return SrvResult(
                code=403,
                msg=f"æƒé™ä¸è¶³ï¼šä»»åŠ¡å±äºå…¶ä»–æœåŠ¡å®ä¾‹ï¼Œæ— æ³•æŸ¥çœ‹æ•°æ®é›†ä¿¡æ¯"
            )
        
        # ğŸ“Š è·å–ä»»åŠ¡çš„æ•°æ®é›†ä¿¡æ¯
        try:
            # è·å–æ‰€æœ‰æ•°æ®æº
            data_sources = TrainingDatasetService.get_data_sources_by_task(task_id)
            
            if not data_sources:
                return SrvResult(
                    code=200,
                    msg="ä»»åŠ¡æš‚æ— æ•°æ®é›†ä¿¡æ¯",
                    data={
                        "task_id": task_id,
                        "data_sources": [],
                        "summary": {
                            "total_sources": 0,
                            "total_samples": 0,
                            "actual_total_samples": 0,
                            "available_splits": []
                        }
                    }
                )
            
            # è·å–è¯¦ç»†æ•°æ®é›†ä¿¡æ¯
            datasets_info = []
            total_samples = 0
            actual_total_samples = 0
            all_splits = set()
            
            for source_id in data_sources:
                source_info = TrainingDatasetService.get_splits_by_source(task_id, source_id)
                datasets_info.append({
                    "data_source_id": source_id,
                    "dataset_base_name": source_info["base_name"],
                    "dataset_path": source_info["path"],
                    "splits": source_info["splits"]
                })
                
                # ç»Ÿè®¡ä¿¡æ¯
                for split_info in source_info["splits"].values():
                    total_samples += split_info.get("samples", 0)
                    actual_total_samples += split_info.get("actual_samples", 0)
                    all_splits.add(split_info.get("split_type", "unknown"))
            
            # è·å–æ€§èƒ½æ‘˜è¦
            performance_summary = TrainingDatasetService.get_source_performance_summary(task_id)
            
            datasets_data = {
                "task_id": task_id,
                "data_sources": datasets_info,
                "performance_summary": performance_summary,
                "summary": {
                    "total_sources": len(data_sources),
                    "total_samples": total_samples,  # åŸå§‹æ•°æ®é›†æ€»æ ·æœ¬æ•°
                    "actual_total_samples": actual_total_samples,  # å®é™…è®­ç»ƒä½¿ç”¨æ€»æ ·æœ¬æ•°
                    "available_splits": list(all_splits),
                    "timestamp": datetime.now().isoformat()
                }
            }
            
            return SrvResult(
                code=200,
                msg="è·å–ä»»åŠ¡æ•°æ®é›†ä¿¡æ¯æˆåŠŸ",
                data=datasets_data
            )
            
        except Exception as dataset_error:
            logger.error(f"è·å–æ•°æ®é›†ä¿¡æ¯å¤±è´¥: {dataset_error}")
            return SrvResult(
                code=500,
                msg=f"è·å–æ•°æ®é›†ä¿¡æ¯å¤±è´¥: {str(dataset_error)}"
            )
            
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡æ•°æ®é›†å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_task_datasets")
        return SrvResult(code=500, msg=f"è·å–ä»»åŠ¡æ•°æ®é›†å¤±è´¥: {str(e)}")

@router.get("/tasks/{task_id}/training_metrics")
def get_task_training_metrics(task_id: str, limit: Optional[int] = Query(None, ge=1, le=10000, description="é™åˆ¶è¿”å›çš„lossè®°å½•æ•°é‡")):
    """è·å–ä»»åŠ¡çš„è®­ç»ƒæŒ‡æ ‡å’Œlosså†å² - ä»…å½“å‰æœåŠ¡å®ä¾‹"""
    try:
        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šå…ˆéªŒè¯ä»»åŠ¡æ˜¯å¦å±äºå½“å‰æœåŠ¡å®ä¾‹
        task_db = training_task_service.get_training_task(task_id)
        if not task_db:
            return SrvResult(code=404, msg=f"æœªæ‰¾åˆ°ä»»åŠ¡: {task_id}")
        
        current_service_id = unified_training_service.service_instance_id
        if not current_service_id:
            return SrvResult(
                code=500,
                msg="âŒ æœåŠ¡éš”ç¦»åŠŸèƒ½å¼‚å¸¸ï¼šæœåŠ¡å®ä¾‹IDä¸ºç©ºï¼Œæ— æ³•è·å–è®­ç»ƒæŒ‡æ ‡ï¼"
            )
        
        if task_db.service_instance_id != current_service_id:
            logger.warning(f"ğŸš« è·¨æœåŠ¡æŸ¥è¯¢è®­ç»ƒæŒ‡æ ‡è¢«æ‹’ç»: ä»»åŠ¡ {task_id} å±äºæœåŠ¡ {task_db.service_instance_id}ï¼Œå½“å‰æœåŠ¡ {current_service_id}")
            return SrvResult(
                code=403,
                msg=f"æƒé™ä¸è¶³ï¼šä»»åŠ¡å±äºå…¶ä»–æœåŠ¡å®ä¾‹ï¼Œæ— æ³•æŸ¥çœ‹è®­ç»ƒæŒ‡æ ‡"
            )
        
        # ğŸ“Š è·å–è®­ç»ƒæŒ‡æ ‡å’Œlosså†å²
        try:
            # æ£€æŸ¥æœ¬åœ°lossæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            output_dir = task_db.output_dir
            if not output_dir:
                return SrvResult(
                    code=404,
                    msg="ä»»åŠ¡è¾“å‡ºç›®å½•æœªé…ç½®ï¼Œæ— æ³•è·å–è®­ç»ƒæŒ‡æ ‡"
                )
            
            from bubble_rag.training.model_sft.utils.loss_manager import get_loss_manager
            
            # åˆ›å»ºæˆ–è·å–lossç®¡ç†å™¨
            loss_manager = get_loss_manager(output_dir, task_id)
            
            # è·å–è®­ç»ƒæŒ‡æ ‡æ±‡æ€»
            training_metrics = loss_manager.get_training_metrics()
            
            # è·å–losså†å²è®°å½•
            loss_history = loss_manager.get_loss_history(limit=limit)
            
            # æ„å»ºå“åº”æ•°æ®
            metrics_data = {
                "task_id": task_id,
                "training_metrics": training_metrics,
                "loss_history": loss_history,
                "loss_history_count": len(loss_history),
                "total_loss_records": training_metrics.get("loss_records_count", 0),
                "files_info": {
                    "loss_history_file": str(loss_manager.loss_history_file),
                    "training_metrics_file": str(loss_manager.training_metrics_file),
                    "loss_history_exists": loss_manager.loss_history_file.exists(),
                    "training_metrics_exists": loss_manager.training_metrics_file.exists()
                }
            }
            
            return SrvResult(
                code=200,
                msg="è·å–è®­ç»ƒæŒ‡æ ‡æˆåŠŸ",
                data=metrics_data
            )
            
        except Exception as metrics_error:
            logger.error(f"è·å–è®­ç»ƒæŒ‡æ ‡å¤±è´¥: {metrics_error}")
            return SrvResult(
                code=500,
                msg=f"è·å–è®­ç»ƒæŒ‡æ ‡å¤±è´¥: {str(metrics_error)}"
            )
            
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è®­ç»ƒæŒ‡æ ‡å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_task_training_metrics")
        return SrvResult(code=500, msg=f"è·å–ä»»åŠ¡è®­ç»ƒæŒ‡æ ‡å¤±è´¥: {str(e)}")

@router.get("/tasks/{task_id}/progress")
def get_task_progress(task_id: str):
    """è·å–ä»»åŠ¡å®æ—¶è¿›åº¦ï¼ˆä»å†…å­˜è·å–ï¼Œé«˜é¢‘è½®è¯¢ä¼˜åŒ–ï¼‰"""
    try:
        # ğŸš€ ç›´æ¥ä»å†…å­˜è·å–å®æ—¶è¿›åº¦ï¼ˆé¿å…æ•°æ®åº“æŸ¥è¯¢å»¶è¿Ÿï¼‰
        from bubble_rag.training.model_sft.services.task_manager import task_manager
        task = task_manager.get_task(task_id)
        
        if not task:
            # å¦‚æœå†…å­˜ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»æ•°æ®åº“éªŒè¯ä»»åŠ¡æ˜¯å¦å­˜åœ¨
            task_db = training_task_service.get_training_task(task_id)
            if not task_db:
                return SrvResult(code=404, msg=f"æœªæ‰¾åˆ°ä»»åŠ¡: {task_id}")
            
            # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šéªŒè¯ä»»åŠ¡å½’å±
            current_service_id = unified_training_service.service_instance_id
            if task_db.service_instance_id != current_service_id:
                logger.warning(f"ğŸš« è·¨æœåŠ¡æŸ¥è¯¢è¿›åº¦è¢«æ‹’ç»: ä»»åŠ¡ {task_id} å±äºæœåŠ¡ {task_db.service_instance_id}ï¼Œå½“å‰æœåŠ¡ {current_service_id}")
                return SrvResult(
                    code=403,
                    msg=f"æƒé™ä¸è¶³ï¼šä»»åŠ¡å±äºå…¶ä»–æœåŠ¡å®ä¾‹ï¼Œæ— æ³•æŸ¥çœ‹è¿›åº¦"
                )
            
            # ä»»åŠ¡å­˜åœ¨ä½†ä¸åœ¨å†…å­˜ä¸­ï¼ˆå¯èƒ½å·²å®Œæˆï¼‰ï¼Œè¿”å›æ•°æ®åº“ä¸­çš„çŠ¶æ€
            progress_data = {
                "task_id": task_db.task_id,
                "status": task_db.status,
                "progress": task_db.progress,
                "is_running": False,
                "timestamp": datetime.now().isoformat(),
                "source": "database"  # æ ‡è¯†æ•°æ®æ¥æº
            }
            return SrvResult(code=200, msg="è·å–ä»»åŠ¡è¿›åº¦æˆåŠŸï¼ˆæ¥æºï¼šæ•°æ®åº“ï¼‰", data=progress_data)
        
        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šéªŒè¯ä»»åŠ¡å½’å±ï¼ˆåªåœ¨æ‰¾åˆ°å†…å­˜ä»»åŠ¡æ—¶æ£€æŸ¥ï¼‰
        try:
            task_db = training_task_service.get_training_task(task_id)
            if task_db:
                current_service_id = unified_training_service.service_instance_id
                if task_db.service_instance_id != current_service_id:
                    logger.warning(f"ğŸš« è·¨æœåŠ¡æŸ¥è¯¢è¿›åº¦è¢«æ‹’ç»: ä»»åŠ¡ {task_id} å±äºæœåŠ¡ {task_db.service_instance_id}ï¼Œå½“å‰æœåŠ¡ {current_service_id}")
                    return SrvResult(
                        code=403,
                        msg=f"æƒé™ä¸è¶³ï¼šä»»åŠ¡å±äºå…¶ä»–æœåŠ¡å®ä¾‹ï¼Œæ— æ³•æŸ¥çœ‹è¿›åº¦"
                    )
        except Exception as e:
            logger.warning(f"å®‰å…¨æ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­è¿”å›è¿›åº¦: {e}")
        
        # è·å–è¿è¡Œè¿›ç¨‹ä¿¡æ¯  
        running_processes = unified_training_service.get_running_processes()
        is_running = task_id in running_processes
        
        # ğŸš€ å®æ—¶è¿›åº¦ä¿¡æ¯ï¼ˆä»å†…å­˜è·å–ï¼Œé«˜é¢‘è½®è¯¢ä¼˜åŒ–ï¼‰
        # ğŸ”§ ä¿®å¤è¿›åº¦åŒæ­¥é—®é¢˜ï¼šä¼˜å…ˆä»æ•°æ®åº“è·å–æœ€æ–°è¿›åº¦ï¼Œç¡®ä¿å‡†ç¡®æ€§
        try:
            task_db = training_task_service.get_training_task(task_id)
            if task_db and task_db.service_instance_id == unified_training_service.service_instance_id:
                # ä½¿ç”¨æ•°æ®åº“ä¸­çš„æœ€æ–°è¿›åº¦æ•°æ®ï¼Œå› ä¸ºå®ƒæ›´å¯é 
                db_progress = task_db.progress or 0
                db_status = task_db.status
                
                # ä½†æ˜¯ä½¿ç”¨å†…å­˜ä¸­çš„is_runningçŠ¶æ€ï¼ˆæ›´å®æ—¶ï¼‰
                memory_is_running = is_running
                
                progress_data = {
                    "task_id": task.task_id,
                    "status": db_status,  # ä½¿ç”¨æ•°æ®åº“çŠ¶æ€
                    "progress": db_progress,  # ä½¿ç”¨æ•°æ®åº“è¿›åº¦
                    "progress_percentage": db_progress,  # å…¼å®¹å­—æ®µ
                    "is_running": memory_is_running,  # ä½¿ç”¨å†…å­˜è¿è¡ŒçŠ¶æ€
                    "timestamp": datetime.now().isoformat(),
                    "source": "hybrid",  # æ··åˆæ•°æ®æº
                    # æ·»åŠ è®­ç»ƒè¯¦æƒ…
                    "training_details": {
                        "memory_progress": task.progress,
                        "database_progress": db_progress,
                        "sync_status": "synced" if abs(task.progress - db_progress) < 1 else "out_of_sync",
                        "task_started_at": task.started_at.isoformat() if task.started_at else None,
                        "process_running": memory_is_running
                    }
                }
                
                logger.info(f"è¿›åº¦APIæ··åˆæ¨¡å¼: ä»»åŠ¡{task_id} DBè¿›åº¦={db_progress}%, å†…å­˜è¿›åº¦={task.progress}%, è¿è¡ŒçŠ¶æ€={memory_is_running}")
            else:
                # å›é€€åˆ°çº¯å†…å­˜æ¨¡å¼
                progress_data = {
                    "task_id": task.task_id,
                    "status": task.status,
                    "progress": task.progress,
                    "progress_percentage": task.progress,
                    "is_running": is_running,
                    "timestamp": datetime.now().isoformat(),
                    "source": "memory_fallback"
                }
        except Exception as e:
            logger.warning(f"è·å–æ•°æ®åº“è¿›åº¦å¤±è´¥ï¼Œä½¿ç”¨å†…å­˜æ•°æ®: {e}")
            # å¦‚æœæ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œä½¿ç”¨åŸæœ‰å†…å­˜æ•°æ®
            progress_data = {
                "task_id": task.task_id,
                "status": task.status,
                "progress": task.progress,
                "progress_percentage": task.progress,
                "is_running": is_running,
                "timestamp": datetime.now().isoformat(),
                "source": "memory_emergency"
            }
        
        return SrvResult(code=200, msg="è·å–ä»»åŠ¡è¿›åº¦æˆåŠŸï¼ˆæ¥æºï¼šå†…å­˜ï¼‰", data=progress_data)
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è¿›åº¦å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_task_progress")
        return SrvResult(code=500, msg=f"è·å–ä»»åŠ¡è¿›åº¦å¤±è´¥: {str(e)}")

@router.get("/training_logs")
def get_training_logs(
    task_id: str = Query(..., description="ä»»åŠ¡ID"),
    lines: int = Query(50, description="è·å–æ—¥å¿—è¡Œæ•°", ge=1, le=1000)
):
    """
    è·å–è®­ç»ƒæ—¥å¿—å’Œlossæ•°æ®
    
    åŠŸèƒ½ï¼š
    1. è·å–è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ–‡æœ¬æ—¥å¿—ä¿¡æ¯
    2. è·å–å®Œæ•´çš„losså†å²æ•°æ®ï¼ˆtrain_loss, eval_lossç­‰ï¼‰
    3. æä¾›ç»Ÿä¸€çš„è®­ç»ƒç›‘æ§æ•°æ®æ¥å£
    
    Args:
        task_id: è®­ç»ƒä»»åŠ¡ID
        lines: è·å–çš„æ—¥å¿—è¡Œæ•°ï¼ˆ1-1000ï¼‰
        
    Returns:
        åŒ…å«æ—¥å¿—å’Œlossæ•°æ®çš„ç»¼åˆä¿¡æ¯
    """
    try:
        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šå…ˆéªŒè¯ä»»åŠ¡æ˜¯å¦å±äºå½“å‰æœåŠ¡å®ä¾‹
        task_db = training_task_service.get_training_task(task_id)
        if not task_db:
            return SrvResult(code=404, msg=f"æœªæ‰¾åˆ°ä»»åŠ¡: {task_id}")
        
        current_service_id = unified_training_service.service_instance_id
        if task_db.service_instance_id != current_service_id:
            logger.warning(f"ğŸš« è·¨æœåŠ¡æŸ¥è¯¢æ—¥å¿—è¢«æ‹’ç»: ä»»åŠ¡ {task_id} å±äºæœåŠ¡ {task_db.service_instance_id}ï¼Œå½“å‰æœåŠ¡ {current_service_id}")
            return SrvResult(
                code=403,
                msg=f"æƒé™ä¸è¶³ï¼šä»»åŠ¡å±äºå…¶ä»–æœåŠ¡å®ä¾‹ï¼Œæ— æ³•æŸ¥çœ‹æ—¥å¿—"
            )
        
        # ä»ä»»åŠ¡ç®¡ç†å™¨è·å–ä»»åŠ¡ä¿¡æ¯
        from bubble_rag.training.model_sft.services.task_manager import task_manager
        task = task_manager.get_task(task_id)
        
        if not task:
            return SrvResult(code=404, msg=f"æœªæ‰¾åˆ°ä»»åŠ¡: {task_id}")
        
        # è·å–æœ€è¿‘çš„æ—¥å¿—
        recent_logs = task.logs[-lines:] if task.logs else []
        
        # ğŸ†• åŒæ—¶è·å–lossæ•°æ®
        loss_data = []
        try:
            from bubble_rag.training.model_sft.utils.loss_manager import get_loss_manager
            output_dir = task_db.output_dir or "/tmp/training_output"
            loss_manager = get_loss_manager(output_dir, task_id)
            loss_data = loss_manager.get_loss_history()
        except Exception as e:
            logger.warning(f"è·å–lossæ•°æ®å¤±è´¥: {e}")
        
        return SrvResult(
            code=200,
            msg="è·å–è®­ç»ƒæ—¥å¿—æˆåŠŸ",
            data={
                "task_id": task.task_id,
                "logs": recent_logs,
                "total_logs": len(task.logs) if task.logs else 0,
                "requested_lines": lines,
                "loss_data": loss_data,
                "total_loss_records": len(loss_data)
            }
        )
        
    except Exception as e:
        logger.error(f"è·å–è®­ç»ƒæ—¥å¿—å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_training_logs")
        return SrvResult(code=500, msg=f"è·å–è®­ç»ƒæ—¥å¿—å¤±è´¥: {str(e)}")

@router.get("/running_tasks")
def get_running_tasks():
    """è·å–æ­£åœ¨è¿è¡Œçš„è®­ç»ƒä»»åŠ¡åˆ—è¡¨ - ä»…è¿”å›å½“å‰æœåŠ¡å®ä¾‹çš„ä»»åŠ¡"""
    try:
        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿åªè·å–å½“å‰æœåŠ¡å®ä¾‹çš„ä»»åŠ¡
        current_service_id = unified_training_service.service_instance_id
        if not current_service_id:
            return SrvResult(
                code=500,
                msg="âŒ æœåŠ¡éš”ç¦»åŠŸèƒ½å¼‚å¸¸ï¼šæœåŠ¡å®ä¾‹IDä¸ºç©ºï¼Œæ— æ³•è·å–è¿è¡Œä»»åŠ¡åˆ—è¡¨ï¼"
            )
        
        running_processes = unified_training_service.get_running_processes()
        
        tasks_info = []
        for task_id, process_info in running_processes.items():
            # ğŸ” éªŒè¯ä»»åŠ¡å½’å±æƒé™
            task_db = training_task_service.get_training_task(task_id)
            if not task_db or task_db.service_instance_id != current_service_id:
                logger.warning(f"ğŸš« è·¨æœåŠ¡è·å–è¿è¡Œä»»åŠ¡è¢«è¿‡æ»¤: ä»»åŠ¡ {task_id}")
                continue
            
            # ä»ä»»åŠ¡ç®¡ç†å™¨è·å–ä»»åŠ¡è¯¦ç»†ä¿¡æ¯
            from bubble_rag.training.model_sft.services.task_manager import task_manager
            task = task_manager.get_task(task_id)
            
            if task:
                # ğŸ”§ ä½¿ç”¨æ··åˆæ•°æ®æºè·å–æœ€å‡†ç¡®çš„è¿›åº¦ï¼ˆç±»ä¼¼progressæ¥å£ï¼‰
                try:
                    # è·å–æ•°æ®åº“ä¸­çš„æœ€æ–°è¿›åº¦
                    actual_progress = task.progress  # é»˜è®¤ä½¿ç”¨å†…å­˜è¿›åº¦
                    actual_status = task.status
                    
                    if task_db:
                        db_progress = task_db.progress or 0
                        db_status = task_db.status
                        
                        # ä¼˜å…ˆä½¿ç”¨æ•°æ®åº“è¿›åº¦ï¼ˆæ›´å¯é ï¼‰
                        actual_progress = db_progress
                        actual_status = db_status
                        
                        logger.debug(f"è¿è¡Œä»»åŠ¡ {task_id}: DBè¿›åº¦={db_progress}%, å†…å­˜è¿›åº¦={task.progress}%")
                except Exception as e:
                    logger.warning(f"è·å–ä»»åŠ¡ {task_id} æœ€æ–°è¿›åº¦å¤±è´¥: {e}")
                    actual_progress = task.progress
                    actual_status = task.status
                
                task_info = convert_task_to_dict(task)
                # ä½¿ç”¨æœ€å‡†ç¡®çš„è¿›åº¦å’ŒçŠ¶æ€
                task_info["progress"] = actual_progress
                task_info["status"] = actual_status
                
                task_info["process_info"] = {
                    "pid": process_info["pid"],
                    "mode": process_info["process_info"].get("mode", "unknown"),
                    "started_at": process_info["process_info"].get("started_at").isoformat() if process_info["process_info"].get("started_at") and hasattr(process_info["process_info"].get("started_at"), "isoformat") else None
                }
                tasks_info.append(task_info)
        
        return SrvResult(
            code=200,
            msg="è·å–è¿è¡Œä¸­ä»»åŠ¡åˆ—è¡¨æˆåŠŸ",
            data={
                "running_tasks": tasks_info,
                "total": len(tasks_info)
            }
        )
        
    except Exception as e:
        logger.error(f"è·å–è¿è¡Œä¸­ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_running_tasks")
        return SrvResult(code=500, msg=f"è·å–è¿è¡Œä¸­ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}")

@router.get("/tasks")
def get_tasks(
    limit: int = Query(20, description="è¿”å›è®°å½•æ•°é™åˆ¶", ge=1, le=100),
    offset: int = Query(0, description="åç§»é‡", ge=0),
    status: Optional[str] = Query(None, description="æŒ‰çŠ¶æ€è¿‡æ»¤: PENDING, RUNNING, SUCCEEDED, FAILED, STOPPED"),
    train_type: Optional[str] = Query(None, description="æŒ‰è®­ç»ƒç±»å‹è¿‡æ»¤: embedding, reranker")
):
    """è·å–ä»»åŠ¡åˆ—è¡¨ï¼ˆæ”¯æŒåˆ†é¡µå’Œè¿‡æ»¤ï¼‰- ä»…è¿”å›å½“å‰æœåŠ¡å®ä¾‹çš„ä»»åŠ¡"""
    try:
        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿åªè·å–å½“å‰æœåŠ¡å®ä¾‹çš„ä»»åŠ¡
        current_service_id = unified_training_service.service_instance_id
        if not current_service_id:
            return SrvResult(
                code=500,
                msg="âŒ æœåŠ¡éš”ç¦»åŠŸèƒ½å¼‚å¸¸ï¼šæœåŠ¡å®ä¾‹IDä¸ºç©ºï¼Œæ— æ³•è·å–ä»»åŠ¡åˆ—è¡¨ï¼"
            )
        
        # ğŸ”§ ä»æ•°æ®åº“è·å–å½“å‰æœåŠ¡å®ä¾‹çš„ä»»åŠ¡åˆ—è¡¨ï¼ˆæ”¯æŒè¿‡æ»¤ï¼‰
        all_service_tasks = training_task_service.get_tasks_by_service_instance(current_service_id)
        
        # åº”ç”¨è¿‡æ»¤æ¡ä»¶
        filtered_tasks = all_service_tasks
        if status:
            filtered_tasks = [t for t in filtered_tasks if t.status == status]
        if train_type:
            filtered_tasks = [t for t in filtered_tasks if t.train_type == train_type]
        
        # è®¡ç®—æ€»æ•°å¹¶åº”ç”¨åˆ†é¡µ
        total_count = len(filtered_tasks)
        tasks = filtered_tasks[offset:offset + limit]
        
        # è·å–è¿è¡Œè¿›ç¨‹ä¿¡æ¯ï¼ˆç”¨äºis_runningçŠ¶æ€ï¼‰
        running_processes = unified_training_service.get_running_processes()
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼ˆæ¦‚è§ˆä¿¡æ¯ï¼‰
        tasks_data = []
        for task in tasks:
            is_running = task.task_id in running_processes
            
            # ğŸ“‹ ä»»åŠ¡æ¦‚è§ˆä¿¡æ¯ï¼ˆæ¯”è¯¦æƒ…è½»é‡ï¼‰
            task_overview = {
                "task_id": task.task_id,
                "task_name": task.task_name,
                "train_type": task.train_type,
                "model_name_or_path": task.model_name_or_path,
                "dataset_name_or_path": task.dataset_name_or_path[:50] + "..." if len(task.dataset_name_or_path or "") > 50 else task.dataset_name_or_path,  # æˆªæ–­é•¿è·¯å¾„
                "HF_subset": getattr(task, 'HF_subset', None),
                "status": task.status,
                "progress": task.progress,
                "is_running": is_running,
                "created_at": task.created_at.isoformat() if task.created_at else None,
                "started_at": task.started_at.isoformat() if task.started_at else None,
                "completed_at": task.completed_at.isoformat() if task.completed_at else None,
                "error_message": task.error_message[:100] + "..." if task.error_message and len(task.error_message) > 100 else task.error_message  # æˆªæ–­é•¿é”™è¯¯ä¿¡æ¯
            }
            tasks_data.append(task_overview)
        
        # total_count å·²ç»åœ¨ä¸Šé¢è®¾ç½®å¥½äº†
        
        return SrvResult(
            code=200,
            msg="è·å–ä»»åŠ¡åˆ—è¡¨æˆåŠŸ",
            data={
                "tasks": tasks_data,
                "total": total_count,
                "limit": limit,
                "offset": offset,
                "has_more": offset + limit < total_count
            }
        )
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_tasks")
        return SrvResult(code=500, msg=f"è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}")

# é…ç½®å’ŒéªŒè¯æ¥å£ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
@router.post("/config/validate")
def validate_config(request: UnifiedTrainingConfig):
    """éªŒè¯è®­ç»ƒé…ç½®"""
    try:
        is_valid, message = config_service.validate_training_config(request.model_dump())
        
        return SrvResult(
            code=200,
            msg="é…ç½®éªŒè¯å®Œæˆ",
            data={"valid": is_valid, "message": message}
        )
        
    except Exception as e:
        logger.error(f"éªŒè¯è®­ç»ƒé…ç½®å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "validate_config")
        return SrvResult(code=500, msg=f"éªŒè¯è®­ç»ƒé…ç½®å¤±è´¥: {str(e)}")

@router.get("/gpu/status")
def get_gpu_status():
    """è·å–GPUèµ„æºçŠ¶æ€"""
    try:
        gpu_status = gpu_resource_manager.get_resource_status()
        return SrvResult(code=200, msg="è·å–GPUçŠ¶æ€æˆåŠŸ", data=gpu_status)
        
    except Exception as e:
        logger.error(f"è·å–GPUçŠ¶æ€å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_gpu_status")
        return SrvResult(code=500, msg=f"è·å–GPUçŠ¶æ€å¤±è´¥: {str(e)}")

# æ•°æ®é›†ç›¸å…³æ¥å£ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
@router.get("/datasets/list")
def list_available_datasets():
    """åˆ—å‡ºå¯ç”¨çš„æ•°æ®é›†"""
    try:
        datasets = dataset_service.list_available_datasets()
        return SrvResult(code=200, msg="è·å–æ•°æ®é›†åˆ—è¡¨æˆåŠŸ", data=datasets)
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_datasets")
        return SrvResult(code=500, msg=f"è·å–æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {str(e)}")

@router.post("/datasets/validate")
def validate_dataset(dataset_path: str = Query(..., description="æ•°æ®é›†è·¯å¾„")):
    """éªŒè¯æ•°æ®é›†"""
    try:
        is_valid, message, sample_data = dataset_service.validate_dataset(dataset_path)
        
        return SrvResult(
            code=200,
            msg="æ•°æ®é›†éªŒè¯å®Œæˆ",
            data={
                "valid": is_valid,
                "message": message,
                "sample_data": sample_data
            }
        )
        
    except Exception as e:
        logger.error(f"éªŒè¯æ•°æ®é›†å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "validate_dataset")
        return SrvResult(code=500, msg=f"éªŒè¯æ•°æ®é›†å¤±è´¥: {str(e)}")

@router.post("/datasets/preview")
def preview_dataset(
    dataset_path: str = Query(..., description="æ•°æ®é›†è·¯å¾„"),
    max_samples: int = Query(5, description="é¢„è§ˆæ ·æœ¬æ•°é‡", ge=1, le=50)
):
    """é¢„è§ˆæ•°æ®é›†"""
    try:
        preview_data = dataset_service.preview_dataset(dataset_path, max_samples)
        return SrvResult(code=200, msg="æ•°æ®é›†é¢„è§ˆæˆåŠŸ", data=preview_data)
        
    except Exception as e:
        logger.error(f"é¢„è§ˆæ•°æ®é›†å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "preview_dataset")
        return SrvResult(code=500, msg=f"é¢„è§ˆæ•°æ®é›†å¤±è´¥: {str(e)}")

@router.get("/service/health")
def get_service_health():
    """è·å–æœåŠ¡å¥åº·çŠ¶æ€å’Œå®ä¾‹ä¿¡æ¯"""
    try:
        from bubble_rag.training.model_sft.utils.service_instance import service_instance_manager
        
        service_id = unified_training_service.service_instance_id
        health_status = "healthy" if service_id else "critical"
        
        health_data = {
            "service_instance_id": service_id,
            "health_status": health_status,
            "service_isolation": service_id is not None,
            "default_training_mode": unified_training_service.default_mode,
            "running_tasks_count": len(unified_training_service.get_running_processes()),
            "instance_info": service_instance_manager.get_instance_info() if service_id else None,
            "timestamp": datetime.now().isoformat()
        }
        
        if not service_id:
            health_data["warning"] = "âŒ æœåŠ¡å®ä¾‹IDä¸ºç©ºï¼ŒæœåŠ¡éš”ç¦»åŠŸèƒ½å¼‚å¸¸ï¼"
        
        return SrvResult(code=200, msg="æœåŠ¡å¥åº·çŠ¶æ€è·å–æˆåŠŸ", data=health_data)
        
    except Exception as e:
        logger.error(f"è·å–æœåŠ¡å¥åº·çŠ¶æ€å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "service_health")
        return SrvResult(code=500, msg=f"è·å–æœåŠ¡å¥åº·çŠ¶æ€å¤±è´¥: {str(e)}")

@router.get("/process/status/stats")
def get_process_status_statistics():
    """è·å–è¿›ç¨‹çŠ¶æ€ç»Ÿè®¡ä¿¡æ¯ - ä»…å½“å‰æœåŠ¡å®ä¾‹"""
    try:
        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿åªè·å–å½“å‰æœåŠ¡å®ä¾‹çš„ç»Ÿè®¡
        current_service_id = unified_training_service.service_instance_id
        if not current_service_id:
            return SrvResult(
                code=500,
                msg="âŒ æœåŠ¡éš”ç¦»åŠŸèƒ½å¼‚å¸¸ï¼šæœåŠ¡å®ä¾‹IDä¸ºç©ºï¼Œæ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯ï¼"
            )
        
        # è·å–å½“å‰æœåŠ¡å®ä¾‹çš„ä»»åŠ¡ç»Ÿè®¡
        service_tasks = training_task_service.get_tasks_by_service_instance(current_service_id)
        stats = {}
        for task in service_tasks:
            process_status = task.process_status or ProcessStatus.UNKNOWN.value
            stats[process_status] = stats.get(process_status, 0) + 1
        
        # æ·»åŠ æ›´è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
        detailed_stats = {
            "total_tasks": sum(stats.values()),
            "status_breakdown": stats,
            "manageable_count": sum(stats.get(status, 0) for status in ProcessStatus.get_manageable_statuses()),
            "active_count": sum(stats.get(status, 0) for status in ProcessStatus.get_active_statuses()),
            "final_count": sum(stats.get(status, 0) for status in ProcessStatus.get_final_statuses()),
            "unknown_count": stats.get(ProcessStatus.UNKNOWN.value, 0),
            "timestamp": datetime.now().isoformat()
        }
        
        return SrvResult(
            code=200,
            msg="è·å–è¿›ç¨‹çŠ¶æ€ç»Ÿè®¡æˆåŠŸ",
            data=detailed_stats
        )
        
    except Exception as e:
        logger.error(f"è·å–è¿›ç¨‹çŠ¶æ€ç»Ÿè®¡å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_process_status_stats")
        return SrvResult(code=500, msg=f"è·å–è¿›ç¨‹çŠ¶æ€ç»Ÿè®¡å¤±è´¥: {str(e)}")

@router.get("/process/status/unknown")
def get_unknown_process_tasks():
    """è·å–UNKNOWNçŠ¶æ€çš„è¿›ç¨‹ä»»åŠ¡ - ä»…å½“å‰æœåŠ¡å®ä¾‹"""
    try:
        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿åªè·å–å½“å‰æœåŠ¡å®ä¾‹çš„ä»»åŠ¡
        current_service_id = unified_training_service.service_instance_id
        if not current_service_id:
            return SrvResult(
                code=500,
                msg="âŒ æœåŠ¡éš”ç¦»åŠŸèƒ½å¼‚å¸¸ï¼šæœåŠ¡å®ä¾‹IDä¸ºç©ºï¼Œæ— æ³•è·å–UNKNOWNä»»åŠ¡ï¼"
            )
        
        # è·å–å½“å‰æœåŠ¡å®ä¾‹çš„UNKNOWNçŠ¶æ€ä»»åŠ¡
        service_tasks = training_task_service.get_tasks_by_service_instance(current_service_id)
        unknown_tasks = [task for task in service_tasks if (task.process_status or ProcessStatus.UNKNOWN.value) == ProcessStatus.UNKNOWN.value]
        
        tasks_data = []
        for task in unknown_tasks:
            task_dict = training_task_service._task_db_to_dict(task)
            tasks_data.append({
                "task_id": task_dict["task_id"],
                "task_name": task_dict["task_name"],
                "process_pid": task_dict["process_pid"],
                "process_status": task_dict["process_status"],
                "status": task_dict["status"],
                "created_at": task_dict["created_at"],
                "updated_at": task_dict["updated_at"],
                "service_instance_id": task_dict["service_instance_id"]
            })
        
        return SrvResult(
            code=200,
            msg="è·å–UNKNOWNçŠ¶æ€ä»»åŠ¡æˆåŠŸ",
            data={
                "unknown_tasks": tasks_data,
                "count": len(tasks_data),
                "timestamp": datetime.now().isoformat()
            }
        )
        
    except Exception as e:
        logger.error(f"è·å–UNKNOWNçŠ¶æ€ä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_unknown_tasks")
        return SrvResult(code=500, msg=f"è·å–UNKNOWNçŠ¶æ€ä»»åŠ¡å¤±è´¥: {str(e)}")

@router.post("/process/status/recovery/unknown")
def recover_unknown_processes():
    """ä¸»åŠ¨è§¦å‘UNKNOWNçŠ¶æ€è¿›ç¨‹æ¢å¤ - ä»…å½“å‰æœåŠ¡å®ä¾‹"""
    try:
        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿åªæ¢å¤å½“å‰æœåŠ¡å®ä¾‹çš„è¿›ç¨‹
        current_service_id = unified_training_service.service_instance_id
        if not current_service_id:
            return SrvResult(
                code=500,
                msg="âŒ æœåŠ¡éš”ç¦»åŠŸèƒ½å¼‚å¸¸ï¼šæœåŠ¡å®ä¾‹IDä¸ºç©ºï¼Œæ— æ³•æ¢å¤è¿›ç¨‹ï¼"
            )
        
        # é€šè¿‡ç»Ÿä¸€è®­ç»ƒæœåŠ¡è§¦å‘æ¢å¤ï¼ˆåªå¤„ç†å½“å‰æœåŠ¡å®ä¾‹çš„ä»»åŠ¡ï¼‰
        recovery_stats = unified_training_service.check_unknown_processes()
        
        return SrvResult(
            code=200,
            msg="UNKNOWNçŠ¶æ€è¿›ç¨‹æ¢å¤å®Œæˆ",
            data={
                "recovery_result": recovery_stats,
                "timestamp": datetime.now().isoformat()
            }
        )
        
    except Exception as e:
        logger.error(f"UNKNOWNçŠ¶æ€è¿›ç¨‹æ¢å¤å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "recover_unknown_processes")
        return SrvResult(code=500, msg=f"UNKNOWNçŠ¶æ€è¿›ç¨‹æ¢å¤å¤±è´¥: {str(e)}")

@router.get("/process/status/health")
def get_process_health_status():
    """è·å–è¿›ç¨‹å¥åº·çŠ¶æ€ç›‘æ§ä¿¡æ¯ - ä»…å½“å‰æœåŠ¡å®ä¾‹"""
    try:
        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿åªè·å–å½“å‰æœåŠ¡å®ä¾‹çš„å¥åº·çŠ¶æ€
        current_service_id = unified_training_service.service_instance_id
        if not current_service_id:
            return SrvResult(
                code=500,
                msg="âŒ æœåŠ¡éš”ç¦»åŠŸèƒ½å¼‚å¸¸ï¼šæœåŠ¡å®ä¾‹IDä¸ºç©ºï¼Œæ— æ³•è·å–å¥åº·çŠ¶æ€ï¼"
            )
        
        # è·å–å½“å‰æœåŠ¡å®ä¾‹çš„ä»»åŠ¡ç»Ÿè®¡
        service_tasks = training_task_service.get_tasks_by_service_instance(current_service_id)
        stats = {}
        for task in service_tasks:
            process_status = task.process_status or ProcessStatus.UNKNOWN.value
            stats[process_status] = stats.get(process_status, 0) + 1
        
        # è®¡ç®—å¥åº·è¯„åˆ†
        total_tasks = sum(stats.values())
        if total_tasks == 0:
            health_score = 100
            health_level = "excellent"
        else:
            # å¥åº·è¯„åˆ†: (RUNNING + STOPPED) / total * 100
            healthy_count = stats.get(ProcessStatus.RUNNING.value, 0) + stats.get(ProcessStatus.STOPPED.value, 0)
            health_score = round((healthy_count / total_tasks) * 100, 2)
            
            if health_score >= 90:
                health_level = "excellent"
            elif health_score >= 75:
                health_level = "good"
            elif health_score >= 50:
                health_level = "warning"
            else:
                health_level = "critical"
        
        health_data = {
            "health_score": health_score,
            "health_level": health_level,
            "total_processes": total_tasks,
            "status_breakdown": stats,
            "alerts": [],
            "timestamp": datetime.now().isoformat()
        }
        
        # ç”Ÿæˆå‘Šè­¦ä¿¡æ¯
        unknown_count = stats.get(ProcessStatus.UNKNOWN.value, 0)
        if unknown_count > 0:
            health_data["alerts"].append(f"å‘ç° {unknown_count} ä¸ªUNKNOWNçŠ¶æ€è¿›ç¨‹ï¼Œå»ºè®®æ£€æŸ¥")
        
        orphaned_count = stats.get(ProcessStatus.ORPHANED.value, 0)
        if orphaned_count > 0:
            health_data["alerts"].append(f"å‘ç° {orphaned_count} ä¸ªORPHANEDçŠ¶æ€è¿›ç¨‹ï¼Œå·²è‡ªåŠ¨æ¸…ç†")
        
        terminated_count = stats.get(ProcessStatus.TERMINATED.value, 0)
        if terminated_count > total_tasks * 0.3 and total_tasks > 0:
            health_data["alerts"].append(f"TERMINATEDè¿›ç¨‹å æ¯”è¿‡é«˜ ({terminated_count}/{total_tasks})")
        
        return SrvResult(
            code=200,
            msg="è·å–è¿›ç¨‹å¥åº·çŠ¶æ€æˆåŠŸ",
            data=health_data
        )
        
    except Exception as e:
        logger.error(f"è·å–è¿›ç¨‹å¥åº·çŠ¶æ€å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_process_health")
        return SrvResult(code=500, msg=f"è·å–è¿›ç¨‹å¥åº·çŠ¶æ€å¤±è´¥: {str(e)}")

@router.delete("/tasks/{task_id}")
def delete_training_task(task_id: str):
    """
    åˆ é™¤è®­ç»ƒä»»åŠ¡
    
    åŠŸèƒ½ï¼š
    1. æ£€æŸ¥ä»»åŠ¡å½’å±æƒé™
    2. å¦‚æœä»»åŠ¡æ­£åœ¨è¿è¡Œï¼Œå…ˆåœæ­¢å¹¶æ€æ­»è¿›ç¨‹
    3. æ›´æ–°ä»»åŠ¡å’Œè¿›ç¨‹çŠ¶æ€
    4. ä»å†…å­˜å’Œæ•°æ®åº“ä¸­åˆ é™¤ä»»åŠ¡è®°å½•
    """
    try:
        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šå…ˆéªŒè¯ä»»åŠ¡æ˜¯å¦å±äºå½“å‰æœåŠ¡å®ä¾‹
        task_db = training_task_service.get_training_task(task_id)
        if not task_db:
            return SrvResult(code=404, msg=f"æœªæ‰¾åˆ°ä»»åŠ¡: {task_id}")
        
        current_service_id = unified_training_service.service_instance_id
        if task_db.service_instance_id != current_service_id:
            logger.warning(f"ğŸš« è·¨æœåŠ¡åˆ é™¤ä»»åŠ¡è¢«æ‹’ç»: ä»»åŠ¡ {task_id} å±äºæœåŠ¡ {task_db.service_instance_id}ï¼Œå½“å‰æœåŠ¡ {current_service_id}")
            return SrvResult(
                code=403,
                msg=f"æƒé™ä¸è¶³ï¼šä»»åŠ¡å±äºå…¶ä»–æœåŠ¡å®ä¾‹ï¼Œæ— æ³•åˆ é™¤"
            )
        
        # ğŸ—‘ï¸ è°ƒç”¨ç»Ÿä¸€è®­ç»ƒæœåŠ¡åˆ é™¤ä»»åŠ¡
        success, message = unified_training_service.delete_task(task_id)
        
        if success:
            return SrvResult(
                code=200,
                msg="ä»»åŠ¡åˆ é™¤æˆåŠŸ",
                data={
                    "task_id": task_id,
                    "deleted": True,
                    "message": message
                }
            )
        else:
            return SrvResult(code=500, msg=f"åˆ é™¤ä»»åŠ¡å¤±è´¥: {message}")
            
    except Exception as e:
        logger.error(f"åˆ é™¤è®­ç»ƒä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "delete_task")
        return SrvResult(code=500, msg=f"åˆ é™¤ä»»åŠ¡å¤±è´¥: {str(e)}")


@router.get("/tasks/{task_id}/loss_data")
def get_task_loss_data(
    task_id: str,
    limit: Optional[int] = Query(None, description="é™åˆ¶è¿”å›çš„è®°å½•æ•°é‡ï¼Œä¸æŒ‡å®šåˆ™è¿”å›æ‰€æœ‰è®°å½•"),
    loss_type: Optional[str] = Query("all", description="æŒ‡å®šlossç±»å‹: train, eval, all")
):
    """
    è·å–è®­ç»ƒä»»åŠ¡çš„å®Œæ•´lossæ•°æ®
    
    åŠŸèƒ½ï¼š
    1. ä»LossManagerè·å–æŒ‡å®šä»»åŠ¡çš„æ‰€æœ‰losså†å²æ•°æ®
    2. æ”¯æŒè¿‡æ»¤lossç±»å‹ï¼ˆtrain_loss, eval_loss, æˆ–å…¨éƒ¨ï¼‰
    3. æ”¯æŒé™åˆ¶è¿”å›è®°å½•æ•°é‡
    4. è¿”å›å®Œæ•´çš„lossæ•°æ®ä¾›å‰ç«¯ç»˜åˆ¶æ›²çº¿å›¾
    
    Args:
        task_id: è®­ç»ƒä»»åŠ¡ID
        limit: å¯é€‰ï¼Œé™åˆ¶è¿”å›æœ€è¿‘çš„Næ¡è®°å½•
        loss_type: å¯é€‰ï¼ŒæŒ‡å®šè¿”å›çš„lossç±»å‹ï¼ˆtrain, eval, allï¼Œé»˜è®¤allï¼‰
    
    Returns:
        åŒ…å«å®Œæ•´lossæ•°æ®çš„ç»“æœ
    """
    try:
        logger.info(f"è·å–ä»»åŠ¡lossæ•°æ®: {task_id}, limit={limit}, loss_type={loss_type}")
        
        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šéªŒè¯ä»»åŠ¡æ˜¯å¦å­˜åœ¨ä¸”å±äºå½“å‰æœåŠ¡å®ä¾‹
        task_db = training_task_service.get_training_task(task_id)
        if not task_db:
            return SrvResult(code=404, msg=f"æœªæ‰¾åˆ°ä»»åŠ¡: {task_id}")
        
        current_service_id = unified_training_service.service_instance_id
        if task_db.service_instance_id != current_service_id:
            logger.warning(f"ğŸš« è·¨æœåŠ¡è·å–lossæ•°æ®è¢«æ‹’ç»: ä»»åŠ¡ {task_id} å±äºæœåŠ¡ {task_db.service_instance_id}ï¼Œå½“å‰æœåŠ¡ {current_service_id}")
            return SrvResult(
                code=403,
                msg=f"æƒé™ä¸è¶³ï¼šä»»åŠ¡å±äºå…¶ä»–æœåŠ¡å®ä¾‹ï¼Œæ— æ³•è·å–lossæ•°æ®"
            )
        
        # ğŸ“Š è·å–lossæ•°æ®
        from bubble_rag.training.model_sft.utils.loss_manager import get_loss_manager
        
        # æ„é€ output_dirè·¯å¾„ï¼ˆä»ä»»åŠ¡é…ç½®ä¸­è·å–ï¼‰
        output_dir = task_db.output_dir or "/tmp/training_output"
        
        # è·å–LossManagerå®ä¾‹
        loss_manager = get_loss_manager(output_dir, task_id)
        
        # è·å–æ‰€æœ‰losså†å²è®°å½•
        all_records = loss_manager.get_loss_history(limit=limit)
        
        # æ ¹æ®loss_typeè¿‡æ»¤æ•°æ®
        filtered_records = []
        for record in all_records:
            filtered_record = {
                "step": record.get("step"),
                "timestamp": record.get("timestamp"),
                "epoch": record.get("epoch")
            }
            
            # æ ¹æ®loss_typeæ·»åŠ ç›¸åº”çš„lossæ•°æ®
            if loss_type in ["all", "train"] and "train_loss" in record:
                filtered_record["train_loss"] = record["train_loss"]
            if loss_type in ["all", "eval"] and "eval_loss" in record:
                filtered_record["eval_loss"] = record["eval_loss"]
            if loss_type in ["all"] and "loss" in record:
                # å…¼å®¹æ€§ï¼šæœ‰äº›è®°å½•å¯èƒ½åªæœ‰'loss'å­—æ®µ
                filtered_record["loss"] = record["loss"]
            
            # åªä¿ç•™æœ‰lossæ•°æ®çš„è®°å½•
            has_loss_data = any(key in filtered_record for key in ["train_loss", "eval_loss", "loss"])
            if has_loss_data:
                filtered_records.append(filtered_record)
        
        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            "task_id": task_id,
            "total_records": len(filtered_records),
            "loss_type": loss_type,
            "loss_data": filtered_records,
            "task_status": task_db.status,
            "last_updated": filtered_records[-1]["timestamp"] if filtered_records else None
        }
        
        logger.info(f"âœ… æˆåŠŸè·å–ä»»åŠ¡ {task_id} çš„lossæ•°æ®: {len(filtered_records)} æ¡è®°å½•")
        
        return SrvResult(
            code=200,
            msg="è·å–lossæ•°æ®æˆåŠŸ",
            data=response_data
        )
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡lossæ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_task_loss_data")
        return SrvResult(code=500, msg=f"è·å–lossæ•°æ®å¤±è´¥: {str(e)}")


@router.get("/tasks/{task_id}/eval_results")
def get_task_eval_results(task_id: str):
    """
    è·å–è®­ç»ƒä»»åŠ¡çš„è¯„ä¼°ç»“æœ
    
    åŠŸèƒ½ï¼š
    1. è·å–æŒ‡å®šä»»åŠ¡çš„åŸºçº¿è¯„ä¼°ç»“æœï¼ˆbase_eval_resultsï¼‰å’Œæœ€ç»ˆè¯„ä¼°ç»“æœï¼ˆfinal_eval_resultsï¼‰
    2. åŒ…å«æ‰€æœ‰æ•°æ®é›†ï¼ˆtrain/eval/testï¼‰çš„è¯„ä¼°æ•°æ®
    3. è¿”å›è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€ä½³losså€¼ï¼ˆbest_train_loss, best_eval_lossï¼‰
    4. è¿”å›å®Œæ•´çš„è¯„ä¼°ç»“æœä¾›å‰ç«¯å±•ç¤º
    
    Args:
        task_id: è®­ç»ƒä»»åŠ¡ID
    
    Returns:
        åŒ…å«åŸºçº¿å’Œæœ€ç»ˆè¯„ä¼°ç»“æœä»¥åŠæœ€ä½³losså€¼çš„æ•°æ®
    """
    try:
        logger.info(f"è·å–ä»»åŠ¡è¯„ä¼°ç»“æœ: {task_id}")
        
        # ğŸ” å®‰å…¨æ£€æŸ¥ï¼šéªŒè¯ä»»åŠ¡æ˜¯å¦å­˜åœ¨ä¸”å±äºå½“å‰æœåŠ¡å®ä¾‹
        task_db = training_task_service.get_training_task(task_id)
        if not task_db:
            return SrvResult(code=404, msg=f"æœªæ‰¾åˆ°ä»»åŠ¡: {task_id}")
        
        current_service_id = unified_training_service.service_instance_id
        if task_db.service_instance_id != current_service_id:
            logger.warning(f"ğŸš« è·¨æœåŠ¡è·å–è¯„ä¼°ç»“æœè¢«æ‹’ç»: ä»»åŠ¡ {task_id} å±äºæœåŠ¡ {task_db.service_instance_id}ï¼Œå½“å‰æœåŠ¡ {current_service_id}")
            return SrvResult(
                code=403,
                msg=f"æƒé™ä¸è¶³ï¼šä»»åŠ¡å±äºå…¶ä»–æœåŠ¡å®ä¾‹ï¼Œæ— æ³•è·å–è¯„ä¼°ç»“æœ"
            )
        
        # ğŸ“Š è·å–æ•°æ®é›†è¯„ä¼°ç»“æœ
        from bubble_rag.training.mysql_service.service.training_dataset_service import TrainingDatasetService
        
        datasets_with_results = TrainingDatasetService.get_datasets_with_eval_results_by_task(task_id)
        
        # ğŸ“ˆ è·å–è®­ç»ƒæŒ‡æ ‡ï¼ˆåŒ…å«best_train_losså’Œbest_eval_lossï¼‰
        training_metrics = None
        try:
            from bubble_rag.training.model_sft.utils.loss_manager import get_loss_manager
            output_dir = task_db.output_dir or "/tmp/training_output"
            loss_manager = get_loss_manager(output_dir, task_id)
            training_metrics = loss_manager.get_training_metrics()
        except Exception as e:
            logger.warning(f"è·å–è®­ç»ƒæŒ‡æ ‡å¤±è´¥: {e}")
        
        # æŒ‰æ•°æ®é›†ç±»å‹åˆ†ç»„æ•´ç†ç»“æœ
        eval_results = {
            "train": [],
            "eval": [],
            "test": []
        }
        
        total_base_results = 0
        total_final_results = 0
        
        for dataset in datasets_with_results:
            split_type = dataset["split_type"]
            dataset_result = {
                "dataset_id": dataset["id"],
                "dataset_name": dataset["dataset_name"],
                "data_source_id": dataset["data_source_id"],
                "base_eval_results": dataset["base_eval_results"],
                "final_eval_results": dataset["final_eval_results"],
                "evaluation_status": dataset["evaluation_status"],
                "configured_sample_size": dataset["configured_sample_size"],
                "last_updated": dataset["update_time"]
            }
            
            # ç»Ÿè®¡è¯„ä¼°ç»“æœæ•°é‡
            if dataset["base_eval_results"]:
                total_base_results += 1
            if dataset["final_eval_results"]:
                total_final_results += 1
            
            if split_type in eval_results:
                eval_results[split_type].append(dataset_result)
        
        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            "task_id": task_id,
            "task_status": task_db.status,
            "task_name": task_db.task_name,
            "train_type": task_db.train_type,
            "best_train_loss": training_metrics.get("best_train_loss") if training_metrics else None,
            "best_eval_loss": training_metrics.get("best_eval_loss") if training_metrics else None,
            "evaluation_summary": {
                "total_datasets": len(datasets_with_results),
                "datasets_with_base_results": total_base_results,
                "datasets_with_final_results": total_final_results,
                "train_datasets": len(eval_results["train"]),
                "eval_datasets": len(eval_results["eval"]),
                "test_datasets": len(eval_results["test"])
            },
            "eval_results": eval_results
        }
        
        logger.info(f"âœ… æˆåŠŸè·å–ä»»åŠ¡ {task_id} çš„è¯„ä¼°ç»“æœ: {len(datasets_with_results)} ä¸ªæ•°æ®é›†")
        
        return SrvResult(
            code=200,
            msg="è·å–è¯„ä¼°ç»“æœæˆåŠŸ",
            data=response_data
        )
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è¯„ä¼°ç»“æœå¤±è´¥: {str(e)}", exc_info=True)
        handle_api_error(e, "get_task_eval_results")
        return SrvResult(code=500, msg=f"è·å–è¯„ä¼°ç»“æœå¤±è´¥: {str(e)}")